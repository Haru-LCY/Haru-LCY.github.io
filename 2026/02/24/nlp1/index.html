

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/kazuki.jpg">
  <link rel="icon" href="/img/kazuki.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="瑾瑜當年">
  <meta name="keywords" content="">
  
    <meta name="description" content="Context-free GrammarBasic ConceptsS: start symbol T: non-terminals x,y,1,2,3….: terminals $\epsilon$ empty string Derivations: A sequence ofsteps where non-terminals arereplaced by the right-hand side">
<meta property="og:type" content="article">
<meta property="og:title" content="老派自然语言处理之必要">
<meta property="og:url" content="http://example.com/2026/02/24/nlp1/index.html">
<meta property="og:site_name" content="港湾">
<meta property="og:description" content="Context-free GrammarBasic ConceptsS: start symbol T: non-terminals x,y,1,2,3….: terminals $\epsilon$ empty string Derivations: A sequence ofsteps where non-terminals arereplaced by the right-hand side">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/nlp1/nlp=hmm.png">
<meta property="og:image" content="http://example.com/img/nlp1/nlp=topo.png">
<meta property="og:image" content="http://example.com/img/nlp1/nlp=dgm.png">
<meta property="og:image" content="http://example.com/img/nlp1/nlp=w.png">
<meta property="article:published_time" content="2026-02-24T14:37:35.000Z">
<meta property="article:modified_time" content="2026-02-24T14:48:41.723Z">
<meta property="article:author" content="瑾瑜當年">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/nlp1/nlp=hmm.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>老派自然语言处理之必要 - 港湾</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>觀瀾</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/sea.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="老派自然语言处理之必要"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2026-02-24 22:37" pubdate>
          2026年2月24日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          29 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">老派自然语言处理之必要</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    本文最后更新于 2026-02-24T22:48:41+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="Context-free-Grammar"><a href="#Context-free-Grammar" class="headerlink" title="Context-free Grammar"></a>Context-free Grammar</h1><h2 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h2><p>S: start symbol</p>
<p>T: non-terminals</p>
<p>x,y,1,2,3….: terminals</p>
<p>$\epsilon$ empty string</p>
<p>Derivations: A sequence of<br>steps where non-terminals are<br>replaced by the right-hand side<br>of a production rule in the CFG.</p>
<h2 id="CFL-L-G"><a href="#CFL-L-G" class="headerlink" title="CFL L(G)"></a>CFL L(G)</h2><p>Every regular language is context-free.</p>
<h3 id="Regex-Examples"><a href="#Regex-Examples" class="headerlink" title="Regex Examples:"></a>Regex Examples:</h3><p>b* : X-&gt; $\epsilon$ | Xb</p>
<p>[cde] : Y-&gt; c | d | e</p>
<p>S -&gt; aSb | $\epsilon$ cannot be expressed by regex, because regex cannot express a string with a string with same amount of a and b.</p>
<h3 id="L0"><a href="#L0" class="headerlink" title="L0"></a>L0</h3><p>CFG covers more than we really want, but it is already useful for parsing.</p>
<p>eg. VP -&gt; Verb NP  “write a flight” do not make sense.</p>
<h3 id="Non-CFL-Example"><a href="#Non-CFL-Example" class="headerlink" title="Non-CFL Example"></a>Non-CFL Example</h3><p>$L={a^nb^nc^n|n&gt;0}$</p>
<h3 id="CNF-Chomsky-normal-form"><a href="#CNF-Chomsky-normal-form" class="headerlink" title="CNF (Chomsky normal form)"></a>CNF (Chomsky normal form)</h3><p>Definition: A CFG is CNF if it is $\epsilon$-free.</p>
<p>Explanation: Either A -&gt; BC or A-&gt;a</p>
<p>Theorem: Any CFG can be converted into CNF. (eg.b*)</p>
<script type="math/tex; mode=display">\begin{array}{l}
S \to X \mid \epsilon \\
X \to XB \mid b\\
B \to b 
\end{array}</script><h3 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h3><p>Definition: syntatic parsing means mapping from a scentence to its parse tree.</p>
<p><strong>CKY Parsing</strong></p>
<p>First, convert grammar to CNF.</p>
<p>Then, dynamic programming.</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-tag">function</span> <span class="hljs-selector-tag">CKY-PARSE</span>(words, grammar) <span class="hljs-selector-tag">returns</span> <span class="hljs-selector-tag">table</span><br>  <span class="hljs-selector-tag">for</span> <span class="hljs-selector-tag">j</span> ← <span class="hljs-selector-tag">from</span> <span class="hljs-number">1</span> <span class="hljs-selector-tag">to</span> <span class="hljs-selector-tag">LENGTH</span>(words) <span class="hljs-selector-tag">do</span><br>    <span class="hljs-comment">// 阶段 1: 处理长度 L=1 的子串</span><br>    <span class="hljs-selector-tag">for</span> <span class="hljs-keyword">all</span> &#123;<span class="hljs-selector-tag">A</span> | <span class="hljs-selector-tag">A</span> → <span class="hljs-selector-tag">words</span><span class="hljs-selector-attr">[j]</span> ∈ <span class="hljs-selector-tag">grammar</span>&#125;<br>      <span class="hljs-selector-tag">table</span><span class="hljs-selector-attr">[j-1, j]</span> ← <span class="hljs-selector-tag">table</span><span class="hljs-selector-attr">[j-1, j]</span> ∪ <span class="hljs-selector-tag">A</span><br>    <br>    <span class="hljs-comment">// 阶段 2: 处理长度 L &gt;= 2 的子串</span><br>    <span class="hljs-selector-tag">for</span> <span class="hljs-selector-tag">i</span> ← <span class="hljs-selector-tag">from</span> <span class="hljs-selector-tag">j-2</span> <span class="hljs-selector-tag">down</span> <span class="hljs-selector-tag">to</span> <span class="hljs-number">0</span> <span class="hljs-selector-tag">do</span><br>      <span class="hljs-selector-tag">for</span> <span class="hljs-selector-tag">k</span> ← <span class="hljs-selector-tag">i</span>+<span class="hljs-number">1</span> <span class="hljs-selector-tag">to</span> <span class="hljs-selector-tag">j-1</span> <span class="hljs-selector-tag">do</span><br>        <span class="hljs-selector-tag">for</span> <span class="hljs-keyword">all</span> &#123;<span class="hljs-selector-tag">A</span> | <span class="hljs-selector-tag">A</span> → <span class="hljs-selector-tag">BC</span> ∈ <span class="hljs-selector-tag">grammar</span> <span class="hljs-selector-tag">and</span> <span class="hljs-selector-tag">B</span> ∈ <span class="hljs-selector-tag">table</span><span class="hljs-selector-attr">[i, k]</span> <span class="hljs-selector-tag">and</span> <span class="hljs-selector-tag">C</span> ∈ <span class="hljs-selector-tag">table</span><span class="hljs-selector-attr">[k, j]</span>&#125;<br>          <span class="hljs-selector-tag">table</span><span class="hljs-selector-attr">[i, j]</span> ← <span class="hljs-selector-tag">table</span><span class="hljs-selector-attr">[i, j]</span> ∪ <span class="hljs-selector-tag">A</span><br></code></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Non-terminal</th>
<th style="text-align:left">Production Rules</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">S</td>
<td style="text-align:left">$\to$ NP VP</td>
</tr>
<tr>
<td style="text-align:left">NP</td>
<td style="text-align:left">$\to$ Det Nom \</td>
<td>PropN</td>
</tr>
<tr>
<td style="text-align:left">Nom</td>
<td style="text-align:left">$\to$ Noun \</td>
<td>Noun Nom</td>
</tr>
<tr>
<td style="text-align:left">VP</td>
<td style="text-align:left">$\to$ Verb NP \</td>
<td>Verb</td>
</tr>
<tr>
<td style="text-align:left">Det</td>
<td style="text-align:left">$\to$ ‘the’ \</td>
<td>‘a’</td>
</tr>
<tr>
<td style="text-align:left">Noun</td>
<td style="text-align:left">$\to$ ‘book’ \</td>
<td>‘flight’</td>
</tr>
<tr>
<td style="text-align:left">PropN</td>
<td style="text-align:left">$\to$ ‘Houston’</td>
</tr>
<tr>
<td style="text-align:left">Verb</td>
<td style="text-align:left">$\to$ ‘booked’</td>
</tr>
</tbody>
</table>
</div>
<p>Convert to CNF:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Non-terminal</th>
<th style="text-align:left">Production Rules</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">S</td>
<td style="text-align:left">$\to$ NP VP</td>
</tr>
<tr>
<td style="text-align:left">NP</td>
<td style="text-align:left">$\to$ Det Nom \</td>
<td>‘Houston’</td>
</tr>
<tr>
<td style="text-align:left">Nom</td>
<td style="text-align:left">$\to$ ‘book’ \</td>
<td>‘flight’  \</td>
<td>Noun Nom</td>
</tr>
<tr>
<td style="text-align:left">VP</td>
<td style="text-align:left">$\to$ Verb NP \</td>
<td>‘booked’</td>
</tr>
<tr>
<td style="text-align:left">Det</td>
<td style="text-align:left">$\to$ ‘the’ \</td>
<td>‘a’</td>
</tr>
<tr>
<td style="text-align:left">Noun</td>
<td style="text-align:left">$\to$ ‘book’ \</td>
<td>‘flight’</td>
</tr>
<tr>
<td style="text-align:left">PropN</td>
<td style="text-align:left">$\to$ ‘Houston’</td>
</tr>
<tr>
<td style="text-align:left">Verb</td>
<td style="text-align:left">$\to$ ‘booked’</td>
</tr>
</tbody>
</table>
</div>
<p>the book booked</p>
<p>Apply CKY:</p>
<p>(0,1): Det</p>
<p>(1,2): Nom/Noun </p>
<p>(0,2): (0,1)+(1,2) NP -&gt; Det Nom </p>
<p>(2,3): VP/Verb </p>
<p>(1,3): (1,2)+(2,3) 空</p>
<p>(0,3): (0,2)+(2，3) S -&gt; NP VP</p>
<h1 id="Latent-Semantic-Analysis"><a href="#Latent-Semantic-Analysis" class="headerlink" title="Latent Semantic Analysis"></a>Latent Semantic Analysis</h1><p>TD Matrix</p>
<p>document number |D|</p>
<p>vocab  number |V|</p>
<p>|V|$\times$ |D|</p>
<p>每一个entry代表这个词在文件里出现的次数</p>
<script type="math/tex; mode=display">
\mathbf{W}_{td} =
\begin{array}{c}
\text{cat} \\
\text{dog} \\
\text{the}
\end{array}
\left[
\begin{array}{ccccccc}
d_1 & d_2 & d_3 & d_4 & d_5 & d_6 & d_7 \\
1 & 1 & 0 & 1 & 0 & 1 & 0 \\
0 & 2 & 0 & 1 & 1 & 1 & 0 \\
20 & 13 & 18 & 22 & 15 & 4 & 20
\end{array}
\right]</script><p>Good: related words do appear together!</p>
<p>Bad: Related words “sometimes” appear together， the co-occurrence statistics might be sparse（矩阵中0很多）</p>
<h2 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h2><ul>
<li><strong>目标：</strong> 不直接使用高维、稀疏的词-文档矩阵 ($W_{td}$)，而是为每个词汇 ($w$) 和文档 ($d$) 推断出一个低维的<strong>潜在向量 (Latent Vector, lv)</strong> 表示。</li>
<li><strong>数学近似：</strong> 词 $i$ 与文档 $j$ 的关系 $W_{td}(i, j)$ 被近似为它们潜在向量的点积（并确保非负）：<script type="math/tex; mode=display">W_{td}(i, j) \approx \max(lv(w_i)^T lv(d_j), 0)</script></li>
<li><strong>$W_{td}$ 矩阵：</strong> 行是词汇，列是文档，单元格是词频或TF-IDF值。</li>
</ul>
<script type="math/tex; mode=display">W_{td} = U \Sigma V^T</script><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">矩阵</th>
<th style="text-align:left">作用</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>$U$</strong></td>
<td style="text-align:left"><strong>词表示矩阵</strong></td>
<td style="text-align:left">包含词汇的潜在向量表示（行）。</td>
</tr>
<tr>
<td style="text-align:left"><strong>$\Sigma$</strong></td>
<td style="text-align:left"><strong>奇异值矩阵</strong></td>
<td style="text-align:left">对角矩阵，包含奇异值，用于确定降维时保留多少维度（即潜在语义的数量）。</td>
</tr>
<tr>
<td style="text-align:left"><strong>$V^T$</strong></td>
<td style="text-align:left"><strong>文档表示矩阵</strong></td>
<td style="text-align:left">包含文档的潜在向量表示（列）。</td>
</tr>
</tbody>
</table>
</div>
<p>保留最重要的k个奇异值——保留U的前K列，V的前K行</p>
<ul>
<li><strong>问题：</strong> SVD 产生的 $U$ 和 $V$ 矩阵是<strong>正交</strong>的，并且其元素可以包含<strong>负值</strong>。这与原始的 $W_{td}$ 矩阵是<strong>非负</strong>的特性不符，有时会降低模型结果的<strong>可解释性</strong>。SVD would pay too much attention to the high-freq words!</li>
</ul>
<h2 id="TF-IDF-normalization"><a href="#TF-IDF-normalization" class="headerlink" title="TF-IDF normalization"></a>TF-IDF normalization</h2><script type="math/tex; mode=display">count'(i, j) = \text{tf}(i, j) \cdot \text{idf}(i)</script><script type="math/tex; mode=display">\text{tf}(i, j) = \frac{\text{词 } i \text{ 在文档 } j \text{ 中出现的次数}}{\text{文档 } j \text{ 中的词总数}}</script><p>Smoothed版本，用于避免除以零和给予常见词语非零的得分：</p>
<script type="math/tex; mode=display">\text{idf}(i) = \log\left(\frac{\text{文档总数} + 1}{\text{包含词 } i \text{ 的文档数} + 1}\right) + 1</script><p><strong>逆文档频率</strong> $\text{idf}(i)$ 衡量一个词 $i$ 在<strong>整个语料库中</strong>的普遍性或稀有性。它的核心思想是：<strong>一个词在越少的文档中出现，它就越具有区分度，其权重就应该越高。</strong></p>
<h1 id="EM-Algorithm"><a href="#EM-Algorithm" class="headerlink" title="EM Algorithm"></a>EM Algorithm</h1><h2 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h2><p>mixture of gaussian distributions</p>
<h3 id="隐变量-z-（Latent-Assignment）"><a href="#隐变量-z-（Latent-Assignment）" class="headerlink" title="隐变量 $z$（Latent Assignment）"></a>隐变量 $z$（Latent Assignment）</h3><ul>
<li><strong>定义：</strong> $z$ 是一个离散的隐变量，代表观测到的数据点 $x$ 是<strong>由哪个高斯分量 $c$ 生成的</strong>。</li>
<li><strong>特点：</strong> 我们<strong>观察到</strong>数据 $x$，但<strong>不知道</strong> $x$ 属于哪一个分量（即 $z$ 是<strong>隐藏</strong>的）。</li>
</ul>
<h3 id="生成一个数据点-x"><a href="#生成一个数据点-x" class="headerlink" title="生成一个数据点 $x$"></a>生成一个数据点 $x$</h3><ol>
<li><p><strong>选择混合分量：</strong></p>
<script type="math/tex; mode=display">p(z=c) = \pi_c</script><ul>
<li>首先，以概率 $\pi_c$ <strong>选择</strong>一个高斯分量 $c$。$\pi_c$ 就是该分量的混合系数。</li>
</ul>
</li>
<li><p><strong>从分量中采样：</strong></p>
<script type="math/tex; mode=display">p(x|z=c) = \mathcal{N}(x; \mu_c, \sigma_c)</script><ul>
<li>一旦选择了分量 $c$，就从这个分量对应的<strong>高斯分布</strong> $\mathcal{N}$ 中<strong>采样</strong>生成数据点 $x$。</li>
</ul>
</li>
</ol>
<h3 id="求和"><a href="#求和" class="headerlink" title="求和"></a>求和</h3><p>由于隐变量 $z$ 是我们不知道的，为了得到观测数据 $x$ 的总概率 $p(x)$，我们需要对 $z$ 的所有可能取值（即所有的分量 $c$）进行<strong>求和（边缘化）</strong>：</p>
<script type="math/tex; mode=display">\begin{aligned} p(x) &= \sum_c p(x, z=c) \\ &= \sum_c p(x|z=c) p(z=c) \\ &= \sum_c \mathcal{N}(x; \mu_c, \sigma_c) \cdot \pi_c \end{aligned}</script><h2 id="EM-algo"><a href="#EM-algo" class="headerlink" title="EM algo"></a>EM algo</h2><p>GOAL: find $\theta$ to maximize log likehood of observed data. </p>
<script type="math/tex; mode=display">\log p(X|\theta) = \log \sum_Z p(X, Z|\theta)</script><h3 id="1-随机初始化-Random-Initialization"><a href="#1-随机初始化-Random-Initialization" class="headerlink" title="1. 随机初始化 (Random Initialization)"></a>1. 随机初始化 (Random Initialization)</h3><p>随机设定模型的初始参数 $\theta_0$。例如，在 GMM 中，就是随机设置每个高斯分量的初始 $(\pi_c, \mu_c, \sigma_c)$。</p>
<ul>
<li><strong>注意：</strong> E-M 算法保证收敛到<strong>局部最优解</strong>，因此初始化的选择会影响最终的结果。</li>
</ul>
<h3 id="2-迭代直到收敛-Iterate-until-convergence"><a href="#2-迭代直到收敛-Iterate-until-convergence" class="headerlink" title="2. 迭代直到收敛 (Iterate until convergence)"></a>2. 迭代直到收敛 (Iterate until convergence)</h3><p>不断重复 E 步和 M 步</p>
<h3 id="E-步-Expectation-Step-猜测隐变量（-Z-）的隶属关系"><a href="#E-步-Expectation-Step-猜测隐变量（-Z-）的隶属关系" class="headerlink" title="E 步 (Expectation Step): 猜测隐变量（$Z$）的隶属关系"></a>E 步 (Expectation Step): 猜测隐变量（$Z$）的隶属关系</h3><ul>
<li><strong>公式：</strong> 计算 $q(Z) := p(Z|X, \theta_k)$。</li>
</ul>
<p>$X$ 是观测数据，假设 $\theta_k$是对的，得到关于$Z$ 的最佳猜测分布 $q(Z)$<br>具体表现为每个数据点 $x_i$ <strong>来自每个分量 $c$ 的概率</strong>：</p>
<pre><code class="hljs">$$\gamma_&#123;ic&#125; = p(z_i = c | x_i, \theta_k)$$
 $\gamma_&#123;ic&#125;$（Responsibility）
</code></pre><h3 id="M-步-Maximization-Step-精修模型参数（-theta-）"><a href="#M-步-Maximization-Step-精修模型参数（-theta-）" class="headerlink" title="M 步 (Maximization Step): 精修模型参数（$\theta$）"></a>M 步 (Maximization Step): 精修模型参数（$\theta$）</h3><p>假设现在$q(Z)$是真实的, 更新 $\theta$ 以最大化 <script type="math/tex">\sum_Z q(Z) \log p(X, Z|\theta)=\mathbb{E}_q[\log p(X, Z|\theta)]</script><br> <strong>关键：</strong> 经过 E 步的转换，这个期望函数的形式通常简化为具有<strong>解析解（Closed-Form Solution）</strong>，允许我们直接计算出最优的 $\theta$。</p>
<h1 id="Hidden-Markov-Model-HMM"><a href="#Hidden-Markov-Model-HMM" class="headerlink" title="Hidden Markov Model (HMM)"></a>Hidden Markov Model (HMM)</h1><p>Motivation task: position tagging, named entinity recognition.</p>
<p><img src="/img/nlp1/nlp=hmm.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>inital probability:  $\pi$</p>
<p>hidden states transfer:<br> $p(q<em>2 \mid q_1) = a</em>{q_1, q_2}$ </p>
<p>emission probability: $b_i(o_k)$</p>
<p><img src="/img/nlp1/nlp=topo.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>topology order: 先生成o在生成下一个q</p>
<p>marginal probability of an observation: $p(\mathbf{O})=\sum_{\mathbf{Q}} p(\mathbf{O}, \mathbf{Q})$.</p>
<p>N: number of hidden state Q</p>
<p>T：sequence length</p>
<p>路径总数 $(num tags)^{seqlen}=N^T$</p>
<p>每条路径计算成本 $O(T)$</p>
<p>总成本$O(TN^T)$</p>
<h2 id="forward-algorithm"><a href="#forward-algorithm" class="headerlink" title="forward algorithm"></a>forward algorithm</h2><script type="math/tex; mode=display">p(o_{1:t}, q_t = j)\equiv\alpha(t, j) = b_j(o_t) \sum_{i} \alpha(t-1, i) a_{ij}\quad O(N)</script><script type="math/tex; mode=display">\alpha(1, j) = \pi_j b_j(o_1)</script><script type="math/tex; mode=display">p(\mathbf{O}) = \sum_{j} \alpha(T, j)</script><p>j一共有N个，算到T步需要算T次，一次N复杂度，所以是$O(N^2T)$</p>
<h2 id="backward-algorithm"><a href="#backward-algorithm" class="headerlink" title="backward algorithm"></a>backward algorithm</h2><script type="math/tex; mode=display">p(O_{t+1:T} \mid q_t = i) = \sum_{j} [p(O_{t+2:T} \mid q_{t+1} = j) \underbrace{p(q_{t+1} = j \mid q_t = i)}_{A} \underbrace{p(o_{t+1} \mid q_{t+1} = j)}_{B}]</script><script type="math/tex; mode=display">\beta(t, i) = \sum_{j} a_{ij} b_j(o_{t+1}) \beta(t + 1, j)</script><script type="math/tex; mode=display">\alpha(t, i) \beta(t, i) = p(\mathbf{O}, q_t = i)</script><h2 id="Vertibi——find-the-most-probable-q"><a href="#Vertibi——find-the-most-probable-q" class="headerlink" title="Vertibi——find the most probable q"></a>Vertibi——find the most probable q</h2><script type="math/tex; mode=display">\mathbf{Q}^* = \underset{\mathbf{Q}}{\operatorname{argmax}} \, P(\mathbf{Q} \mid \mathbf{O})</script><script type="math/tex; mode=display">\underset{\mathbf{Q}}{\operatorname{argmax}} \, P(\mathbf{Q} \mid \mathbf{O}) = \underset{\mathbf{Q}}{\operatorname{argmax}} \, \frac{P(\mathbf{Q}, \mathbf{O})}{P(\mathbf{O})}</script><p>因为P(O)是观测到的常数</p>
<script type="math/tex; mode=display">\underset{\mathbf{Q}}{\operatorname{argmax}} \, P(\mathbf{Q} \mid \mathbf{O}) = \underset{\mathbf{Q}}{\operatorname{argmax}} \, P(\mathbf{Q}, \mathbf{O})</script><p>表示在观测到序列的前 $t$ 个元素 $(\mathbf{O}_{1:t})$ 的条件下，<strong>所有</strong>以隐藏状态 $q_t = j$ 结尾的路径中，<strong>最大概率</strong>的那条路径的概率值：</p>
<script type="math/tex; mode=display">\delta(t, j)=\max_{Q_{1:t-1}} p(\mathbf{O}_{1:t}, \mathbf{Q}_{1:t-1}, q_t = j)</script><script type="math/tex; mode=display">\delta(t, j) = b_j(o_t) \max_{i} \delta(t - 1, i) a_{ij}</script><script type="math/tex; mode=display">\delta(1, j) = \pi_j b_j(o_1)</script><h2 id="Where-do-pi-A-B-come-from"><a href="#Where-do-pi-A-B-come-from" class="headerlink" title="Where do $\pi$ A ,B come from"></a>Where do $\pi$ A ,B come from</h2><p>Supervised learning: labeled data.</p>
<p>Unsupervised learning:</p>
<script type="math/tex; mode=display">\log p(O|\theta) = \log \sum_Q p(O, Q|\theta)</script><h1 id="DGM-Direct-Graphical-Model"><a href="#DGM-Direct-Graphical-Model" class="headerlink" title="DGM(Direct Graphical Model)"></a>DGM(Direct Graphical Model)</h1><h2 id="normalized-property"><a href="#normalized-property" class="headerlink" title="normalized property"></a>normalized property</h2><p>For DAG, as long as each of the (separate) conditional distribution is a valid (normalized), then the joint distribution is normalized.</p>
<script type="math/tex; mode=display">P(O, S, H, C) = P(O) \cdot P(S) \cdot P(H \mid O, S) \cdot P(C \mid S)</script><p>（这里 $O$=overweight, $S$=smoking, $H$=heart disease, $C$=cough）</p>
<p>我们对所有变量求和：</p>
<script type="math/tex; mode=display">\sum_{O, S, H, C} P(O, S, H, C) = \sum_{O, S, H} \left[ P(O) P(S) P(H \mid O, S) \cdot \sum_{C} P(C \mid S) \right]</script><ol>
<li><p><strong>对 $C$ 求和：</strong> 因为 $P(C \mid S)$ 是归一化的，所以 $\sum_{C} P(C \mid S) = 1$。</p>
<script type="math/tex; mode=display">\sum_{O, S, H} P(O) P(S) P(H \mid O, S) \times 1 = \sum_{O, S, H} P(O) P(S) P(H \mid O, S)</script></li>
<li><p><strong>对 $H$ 求和：</strong> 因为 $P(H \mid O, S)$ 是归一化的，所以 $\sum_{H} P(H \mid O, S) = 1$。</p>
<script type="math/tex; mode=display">\sum_{O, S} P(O) P(S) \times 1 = \sum_{O, S} P(O) P(S)</script></li>
<li><p><strong>对 $O$ 和 $S$ 求和：</strong> 因为 $P(O)$ 和 $P(S)$ 是<strong>独立</strong>的边缘分布，我们可以分别求和：</p>
<script type="math/tex; mode=display">\sum_{O} P(O) \cdot \sum_{S} P(S)</script><p>由于边缘分布本身是归一化的：$\sum<em>{O} P(O) = 1$ 且 $\sum</em>{S} P(S) = 1$。</p>
<script type="math/tex; mode=display">1 \times 1 = 1</script></li>
</ol>
<h2 id="acyclic"><a href="#acyclic" class="headerlink" title="acyclic"></a>acyclic</h2><p>A&lt;-&gt;B</p>
<p>If I “set” P(A=0|B=0)=1, P(A=1|B=1)=1, P(B=1|A=0)=1, P(B=0|A=1)=1…</p>
<p> Basically B is 100% sure that A=B, while A is 100% sure that A!=B…</p>
<p><img src="/img/nlp1/nlp=dgm.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>第一个的意思是：在已知 $B$ 和 $C$ 的条件下，$A$ 是否独立于 $D$ 和 $E$ 的联合集合？</p>
<p>答案：是是否</p>
<h2 id="汇聚节点"><a href="#汇聚节点" class="headerlink" title="汇聚节点"></a>汇聚节点</h2><p> A-&gt;B&lt;-C 观测到B,AC不独立</p>
<p>D-连接 (d-connected) 的定义</p>
<p>(1) 汇聚节点条件 (Collider Rule):<strong> 路径 $U$ 上的</strong>每一个汇聚节点<strong>（Collider，即 $\rightarrow Z \leftarrow$ 结构中的 $Z$）都</strong>有后代位于观测集 $W$ 中**（包括 $Z$ 本身位于 $W$ 中）。</p>
<pre><code class="hljs">* **含义：** 汇聚节点上的信息流必须被观测到（或者其结果被观测到）才能被**打开**。
</code></pre><p><strong>(2) 非汇聚节点条件 (Non-Collider Rule):</strong> 路径 $U$ 上的<strong>其他任何节点</strong>（非汇聚节点，即串联 $\rightarrow Z \rightarrow$ 或发散 $\leftarrow Z \rightarrow$ 结构中的 $Z$）<strong>都不在观测集 $W$ 中</strong>。</p>
<h1 id="N-gram-LM"><a href="#N-gram-LM" class="headerlink" title="N-gram LM"></a>N-gram LM</h1><h2 id="Various-applications-of-LM"><a href="#Various-applications-of-LM" class="headerlink" title="Various applications of LM"></a>Various applications of LM</h2><p>Generation </p>
<p>• Auto-complete</p>
<p>• Speech-to-text</p>
<p>• Question-answering / chatbots</p>
<p>• Machine translation</p>
<p>• Summarization</p>
<p>Classification </p>
<p>• Authorship attribution</p>
<p>• Detecting spam vs not spam</p>
<p>• Grammar Correction</p>
<h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><p>Unigram LM</p>
<p>认为每一个词都是独立的，容易导致错误的句子和正确的句子概率一样<script type="math/tex">P_{unigram}(w_1 ... w_T) = P(w_1)P(w_2) ... P(w_T)</script></p>
<p>Bigram LM 每两个词来考虑</p>
<p>Ngram LM</p>
<p>真实的概率 <script type="math/tex">P(w_{1:T}) = \prod_{t=1}^{T} P(w_t | w_{1:t-1})</script></p>
<p>N-gram 模型假设一个词 $w<em>t$ 的出现<strong>只依赖于它前面紧邻的 $N-1$ 个词</strong>，而不是全部的历史 $w</em>{1:t-1}$。</p>
<script type="math/tex; mode=display">P(w_t | w_{1:t-1}) \approx P_{ngram}(w_t | w_{t-N+1:t-1})</script><h2 id="special-tokens"><a href="#special-tokens" class="headerlink" title="special tokens"></a>special tokens</h2><p>\<eos\> end of scentence</p>
<p>\<unk\> out-of-vocabulary token</p>
<p>set word appear at least twice as vocabulary and others as \<unk\> </p>
<h2 id="How-to-get-P？"><a href="#How-to-get-P？" class="headerlink" title="How to get P？"></a>How to get P？</h2><h3 id="MLE"><a href="#MLE" class="headerlink" title="MLE:"></a>MLE:</h3><script type="math/tex; mode=display">P_{tri}(\text{NLP} | \text{we study}) = \frac{count(\text{we study NLP})}{count(\text{we study } *)}</script><p>Problem: Sparsity, if we study NLP does not apper in the training corpus, then the probability will be 0.</p>
<h3 id="add-k-smoothing"><a href="#add-k-smoothing" class="headerlink" title="add-k smoothing"></a>add-k smoothing</h3><script type="math/tex; mode=display">P_{tri}(w_t | w_{t-2}w_{t-1}) = \frac{count(w_{t-2}w_{t-1}w_t) + k}{count(w_{t-2}w_{t-1}) + k|V|}</script><p>因为 Add-k 对<strong>所有</strong> N-gram 都一视同仁地增加了 $k$，这可能会对高频 N-gram 的概率造成不必要的扭曲</p>
<h3 id="Interpolation-or-backoff"><a href="#Interpolation-or-backoff" class="headerlink" title="Interpolation or backoff"></a>Interpolation or backoff</h3><p><strong>线性插值公式：</strong> 将不同阶的 N-gram 模型的概率进行<strong>加权平均</strong>：</p>
<pre><code class="hljs">$$P_&#123;tri&#125;(w_t | w_&#123;t-2&#125;w_&#123;t-1&#125;) = \lambda_1 P_&#123;tri&#125;(w_t | w_&#123;t-2&#125;w_&#123;t-1&#125;) + \lambda_2 P_&#123;bi&#125;(w_t | w_&#123;t-1&#125;) + \lambda_3 P_&#123;uni&#125;(w_t)$$
</code></pre><p>回退： 如果 Trigram 计数为 0，则<strong>回退</strong>到 Bigram 模型 $P_{bi}$。</p>
<pre><code class="hljs">$$P_&#123;tri-BO&#125;(w_t | w_&#123;t-2&#125;w_&#123;t-1&#125;) = \begin&#123;cases&#125; P^*_&#123;tri&#125;(w_t | w_&#123;t-2&#125;w_&#123;t-1&#125;) &amp; \text&#123;if &#125; count(w_&#123;t-2&#125;w_&#123;t-1&#125;w_t) &gt; 0 \\ \alpha(w_&#123;t-2&#125;w_&#123;t-1&#125;)P_&#123;bi&#125;(w_t | w_&#123;t-1&#125;) &amp; \text&#123;otherwise&#125; \end&#123;cases&#125;$$
</code></pre><h2 id="LM-evaluation"><a href="#LM-evaluation" class="headerlink" title="LM evaluation"></a>LM evaluation</h2><p>PPL(Perplexity) 越低越好</p>
<script type="math/tex; mode=display">PPL(W) = 2^{-l}, \quad \text{where } l = \frac{\log_2(P(W))}{token\_len(W)}</script><ul>
<li>$P(W)$ 是模型计算出的整个测试集句子的概率。</li>
<li>$token_len(W)$ 是测试集中的词/token总数。</li>
</ul>
<p>PPL does not involve actual generation. (衡量模型分配给已知测试数据的概率高低)</p>
<p>PPL cares more diversity than quality. </p>
<p>Example: If LM’s probability concentrate on a small set of very good<br>sentences, then the model’s PPL on a held-out set will be very bad.</p>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p><img src="/img/nlp1/nlp=w.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>parameters: input embedding W for context, output embedding U for prediction.</p>
<p>every row is a vector for a word.</p>
<h2 id="Skipgram"><a href="#Skipgram" class="headerlink" title="Skipgram"></a>Skipgram</h2><p>objective:</p>
<script type="math/tex; mode=display">p_{\theta}(\text{out} | \text{input}) = \frac{\exp(u_{\text{out}} \cdot w_{\text{input}})}{\sum_{v \in V} \exp(u_v \cdot w_{\text{input}})}</script><p>computation bottleneck: 分母takes $O(V)$ to compute.</p>
<h3 id="solution：negative-sampling"><a href="#solution：negative-sampling" class="headerlink" title="solution：negative sampling"></a>solution：negative sampling</h3><ul>
<li>对于一个给定的中心词 $x$ 和一个上下文词 $y$（这是一个<strong>正例</strong>），我们希望模型预测它们是一对的概率高。<ul>
<li>我们随机抽取 $k$ 个<strong>负例</strong> $y’$（非上下文词），希望模型预测 $x$ 和这些 $y’$ 不是一对的概率高。</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\log \sigma(\mathbf{u}_y \cdot \mathbf{w}_x) + \sum_{i} \mathbb{E}_{y' \sim P_n} [\log \sigma(-\mathbf{u}_{y'} \cdot \mathbf{w}_x)]</script><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">符号</th>
<th style="text-align:left">含义</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$\log \sigma(\mathbf{u}_y \cdot \mathbf{w}_x)$</td>
<td style="text-align:left"><strong>正例项</strong></td>
<td style="text-align:left">希望最大化 $x$ 和真实上下文 $y$ 是一对的概率（点积越大，$\sigma$ 越大，$\log \sigma$ 越接近 0）。</td>
</tr>
<tr>
<td style="text-align:left">$\log \sigma(-\mathbf{u}_{y’} \cdot \mathbf{w}_x)$</td>
<td style="text-align:left"><strong>负例项</strong></td>
<td style="text-align:left">引入负号 $(-\dots)$，目的是希望最大化 $\sigma(-\mathbf{u}<em>{y’} \cdot \mathbf{w}_x)$，这等价于最小化 $\sigma(\mathbf{u}</em>{y’} \cdot \mathbf{w}_x)$。这迫使模型预测 $x$ 和非上下文 $y’$ 不是一对的概率高。</td>
</tr>
<tr>
<td style="text-align:left">$P_n$</td>
<td style="text-align:left"><strong>负采样分布</strong></td>
<td style="text-align:left">“$\mathbf{P}_n$ can be a unigram model.” 通常使用词汇的<strong>一元模型</strong>（Unigram Model）作为采样分布，即词 $v$ 被采样的概率与其在语料库中的频率相关。</td>
</tr>
</tbody>
</table>
</div>
<p>Q1: 在 SGD 训练过程中，所有向量都会收敛到同一个值吗？为什么？</p>
<p><strong>不会</strong>收敛到同一个值。</p>
<ol>
<li><strong>目标函数的分母 (Softmax)：</strong> 分母 $\sum<em>{v \in V} \exp(u_v \cdot w</em>{\text{input}})$ 会计算输入词向量 $w_{\text{input}}$ 与<strong>词汇表中所有词</strong>的输出向量 $u_v$ 的相似度。这意味着，为了最大化上下文词的概率，模型必须学会在<strong>高维空间</strong>中将词语<strong>区分开来</strong>。</li>
<li><strong>上下文关系：</strong> 不同的词有不同的上下文。例如，“猫”的上下文通常是“抓”、“毛茸茸”，而“车”的上下文通常是“驾驶”、“轮胎”。模型会根据这些<strong>不同的统计关系</strong>，将具有相似上下文的词汇拉近，将不相似的词汇推远。</li>
</ol>
<p>Q2: 拥有一个不同的输出嵌入矩阵 $U$ (Output Embedding) 是否有用？</p>
<p> <strong>角色分离 (Separation of Roles):</strong></p>
<ul>
<li><strong>$W$ (输入嵌入/中心词):</strong> 学习如何<strong>表示</strong>一个词，以便它能被用于预测其上下文。</li>
<li><strong>$U$ (输出嵌入/上下文词):</strong> 学习如何作为一个<strong>分类器</strong>或<strong>预测层</strong>的权重，来识别哪些词是中心词的上下文。</li>
</ul>
<h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><script type="math/tex; mode=display">p_{cbow}(x_t | x_{t-s} \dots x_{t+s}) = \frac{\exp(\mathbf{u}_{x_t} \cdot \frac{1}{2S}\sum_j \mathbf{w}_{t+j})}{Z}</script><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">符号</th>
<th style="text-align:left">含义</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$\mathbf{u}_{x_t}$</td>
<td style="text-align:left"><strong>输出词向量</strong></td>
<td style="text-align:left">词 $x_t$ 作为<strong>输出词</strong>时的向量表示。</td>
</tr>
<tr>
<td style="text-align:left">$\mathbf{w}_{t+j}$</td>
<td style="text-align:left"><strong>输入词向量</strong></td>
<td style="text-align:left">上下文词 $w_{t+j}$ 作为<strong>输入词</strong>时的向量表示。</td>
</tr>
<tr>
<td style="text-align:left">$\frac{1}{2S}\sum<em>j \mathbf{w}</em>{t+j}$</td>
<td style="text-align:left"><strong>上下文平均向量</strong></td>
<td style="text-align:left">所有上下文词的<strong>输入词向量的平均值</strong>（对应图中的 PROJECTION/SUM 结果）。 $2S$ 是上下文窗口中的词数。</td>
</tr>
<tr>
<td style="text-align:left">$Z$</td>
<td style="text-align:left"><strong>归一化项（Softmax分母）</strong></td>
<td style="text-align:left">这是一个归一化因子，也称为 <strong>Softmax 分母</strong>。它是词汇表中<strong>所有词</strong>与上下文平均向量点积的指数之和，确保所有预测概率加起来等于 1。</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">\mathbf{w}_x \leftarrow \mathbf{w}_x + \eta (\mathbf{u}_y - \mathbb{E}_{p_{\theta}(v|x)} [\mathbf{u}_v])</script><p>$\mathbf{u}_y$: output embedding for correct label</p>
<p>${E}<em>{p</em>{\theta}(v|x)} [\mathbf{u}_v]$: what the model think is correct.</p>
<p>$\frac{\partial (-\log p(y|x))}{\partial \mathbf{w}_x}$ 这就是梯度</p>
<h2 id="properties-of-word-vector"><a href="#properties-of-word-vector" class="headerlink" title="properties of word vector"></a>properties of word vector</h2><p>Linear Word Analogies: wapple-wapples $\approx$ wcar-wcars</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/NLP/" class="print-no-link">#NLP</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>老派自然语言处理之必要</div>
      <div>http://example.com/2026/02/24/nlp1/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>瑾瑜當年</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2026年2月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2026/02/24/post-2/" title="Diffusion LM初探：LLaDA">
                        <span class="hidden-mobile">Diffusion LM初探：LLaDA</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="twikoo"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/twikoo/1.6.39/twikoo.all.min.js', function() {
        var options = Object.assign(
          {"envId":"https://twikoo-haru.netlify.app/.netlify/functions/twikoo","region":"ap-shanghai","path":"window.location.pathname","enable":true},
          {
            el: '#twikoo',
            path: 'window.location.pathname',
            onCommentLoaded: function() {
              Fluid.utils.listenDOMLoaded(function() {
                var imgSelector = '#twikoo .tk-content img:not(.tk-owo-emotion)';
                Fluid.plugins.imageCaption(imgSelector);
                Fluid.plugins.fancyBox(imgSelector);
              });
            }
          }
        )
        twikoo.init(options)
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start --><script data-pjax src="https://unpkg.com/oh-my-live2d"></script><script>const oml2d = OML2D.loadOml2d({dockedPosition:"left",mobileDisplay:true,models:[{"path":"https://model.hacxy.cn/HK416-1-normal/model.json","scale":0.1,"motionPreloadStrategy":"ALL","position":[-10,60],"stageStyle":{"height":520},"mobileScale":0.06,"mobilePosition":[10,30],"mobileStageStyle":{"height":250}}],parentElement:document.body,primaryColor:"#336699",sayHello:false,tips:{style: {"width":230,"height":120,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":180,"height":80,"left":"calc(50% - 30px)","top":"-100px"},idleTips:{interval:15000,message:function(){
  return axios.get('https://v1.hitokoto.cn?c=i')
    .then(function (response) {
      return response.data.hitokoto ;
    })
    .catch(function (error) {
      console.error(error);
    });
}
}}});</script><!-- hexo injector body_end end --></body>
</html>
