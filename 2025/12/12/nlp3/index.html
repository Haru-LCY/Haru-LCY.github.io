

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/kazuki.jpg">
  <link rel="icon" href="/img/kazuki.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="ç‘¾ç‘œç•¶å¹´">
  <meta name="keywords" content="">
  
    <meta name="description" content="Transformer decoder and GPT Apply attention mask to forbid the attention from future timesteps to train AR-LM. A transformer model for AR-LM is also referred to as a transformer decoder. 12345678910de">
<meta property="og:type" content="article">
<meta property="og:title" content="ç ´æ™“ä¹‹åˆ»ï¼šTransformerçš„è¯ç”Ÿä¸è‡ªç„¶è¯­è¨€å¤„ç†å‰æ²¿">
<meta property="og:url" content="http://example.com/2025/12/12/nlp3/index.html">
<meta property="og:site_name" content="æ¸¯æ¹¾">
<meta property="og:description" content="Transformer decoder and GPT Apply attention mask to forbid the attention from future timesteps to train AR-LM. A transformer model for AR-LM is also referred to as a transformer decoder. 12345678910de">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/nlp3/nlp=rl.png">
<meta property="og:image" content="http://example.com/img/nlp3/nlp=kld.png">
<meta property="og:image" content="http://example.com/img/nlp3/nlp=dec.png">
<meta property="og:image" content="http://example.com/img/nlp3/nlp=encdec.png">
<meta property="og:image" content="http://example.com/img/nlp3/nlp=cot.png">
<meta property="og:image" content="http://example.com/img/nlp3/nlp=comp.png">
<meta property="article:published_time" content="2025-12-12T14:54:44.000Z">
<meta property="article:modified_time" content="2026-02-24T15:12:56.763Z">
<meta property="article:author" content="ç‘¾ç‘œç•¶å¹´">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/nlp3/nlp=rl.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>ç ´æ™“ä¹‹åˆ»ï¼šTransformerçš„è¯ç”Ÿä¸è‡ªç„¶è¯­è¨€å¤„ç†å‰æ²¿ - æ¸¯æ¹¾</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>è§€ç€¾</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>é¦–é¡µ</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>å½’æ¡£</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>æ ‡ç­¾</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>å…³äº</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>å‹é“¾</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/sea.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="ç ´æ™“ä¹‹åˆ»ï¼šTransformerçš„è¯ç”Ÿä¸è‡ªç„¶è¯­è¨€å¤„ç†å‰æ²¿"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-12-12 22:54" pubdate>
          2025å¹´12æœˆ12æ—¥ æ™šä¸Š
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.7k å­—
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          48 åˆ†é’Ÿ
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> æ¬¡
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">ç ´æ™“ä¹‹åˆ»ï¼šTransformerçš„è¯ç”Ÿä¸è‡ªç„¶è¯­è¨€å¤„ç†å‰æ²¿</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    æœ¬æ–‡æœ€åæ›´æ–°äº 2026-02-24T23:12:56+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="transformer-decoder-and-gpt">Transformer decoder and GPT</h1>
<p>Apply attention mask to forbid the attention from future timesteps to
train AR-LM.</p>
<p>A transformer model for AR-LM is also referred to as a transformer
decoder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">attention</span>(<span class="hljs-params">query, key, value, mask=<span class="hljs-literal">None</span>, dropout=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span><br>     d_k = query.size(-<span class="hljs-number">1</span>)<br>     scores = torch.matmul(query, key.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / math.sqrt(d_k)<br>     <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, -<span class="hljs-number">1e9</span>) <span class="hljs-comment">#åœ¨maskä¸º0çš„åœ°æ–¹åŠ ä¸€ä¸ªå¾ˆå°çš„è´Ÿæ•°ï¼Œä¿è¯expä¹‹åè¶‹äº0ï¼Œå½¢çŠ¶ä¸º(L_Q,L_k)</span><br>     p_attn = scores.softmax(dim=-<span class="hljs-number">1</span>)<br>     <span class="hljs-comment"># å¯¹keyç»´åº¦åšsoftmax</span><br>    output = torch.matmul(p_attn, value)<br>    <span class="hljs-keyword">return</span> output, p_attn <br></code></pre></td></tr></table></figure>
<h2 id="gpt2">GPT2</h2>
<p>apply the learned model zero-shot to some downstream language
generation task (translation, summarization, QA, etc.).</p>
<p>zero-shotï¼šå®Œå…¨ä¸å¾®è°ƒ</p>
<p>è®¾è®¡ä¸€äº›å¾ˆå¥½çš„promptæ¥å®ç°ï¼Œæ¯”å¦‚ï¼›</p>
<p>â€œTranslate the following text to French. Text: [ENG TEXT] French:â€
â€œGiven the document, answer the question. Document: [DOC] Question: [Q]
Answer:â€</p>
<p>open-ended generation: tasks that has big freedom and diversity, like
story or news generation.The model needs to rely its own (memory,
consistency or creativity)</p>
<h2 id="topk-sampling">topk sampling</h2>
<p>We will represent <span class="math inline">\(P(\cdot |
W_{1..i})\)</span> by <span class="math inline">\(p = (p_1, p_2, \dots,
p_{|V|})\)</span> (where the elements are sorted so that <span
class="math inline">\(p_1 \geq p_2 \geq p_3 \dots \geq
p_{|V|}\)</span>).</p>
<p>Top-K sampling transforms <span class="math inline">\(p\)</span> to
<span class="math inline">\(\hat{p}\)</span> by: <span
class="math display">\[\hat{p}_i = \frac{p_i \cdot \mathbb{1}\{i \leq
K\}}{Z}\]</span> <span class="math inline">\(Z\)</span> æ˜¯å½’ä¸€åŒ–å¸¸æ•°
(normalization constant)ã€‚ <span class="math display">\[Z =
\sum_{i=1}^{|V|} p_i \cdot \mathbb{1}\{i \leq K\} = \sum_{i=1}^{K}
p_i\]</span>
ä¿ç•™æ¦‚ç‡æœ€å¤§çš„kä¸ªï¼Œå¹¶é‡æ–°å½’ä¸€åŒ–æ€»æ¦‚ç‡ä¸º1ã€‚å¦‚æœk=1ï¼Œé‚£å°±å˜æˆäº†greedy
decodingäº†ã€‚</p>
<p>quality-diversity trade-off</p>
<h1 id="rethink-mleæœ€å¤§ä¼¼ç„¶ä¼°è®¡">Rethink MLEï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰</h1>
<p>MLE ç›®æ ‡å‡½æ•° (The MLE objective): <span class="math display">\[\log
P(W) = \sum \log P(W_i | W_{1:i-1})\]</span> * <span
class="math inline">\(P(W_i | W_{1:i-1})\)</span> æ˜¯åœ¨ç»™å®šå†å²ä¸Šä¸‹æ–‡
<span class="math inline">\(W_{1:i-1}\)</span> çš„æƒ…å†µä¸‹ï¼Œä¸‹ä¸€ä¸ªè¯æ˜¯
<span class="math inline">\(W_i\)</span> çš„æ¦‚ç‡ã€‚</p>
<ul>
<li>â€œResearchers thinks the teacher forcing in MLE training is to
blame.â€ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå½“æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ <span
class="math inline">\(W_i\)</span> æ—¶ï¼Œå®ƒä½¿ç”¨çš„å†å²ä¸Šä¸‹æ–‡ <span
class="math inline">\(W_{1:i-1}\)</span>
æ€»æ˜¯æ¥è‡ªäºçœŸå®çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸æ˜¯æ¨¡å‹è‡ªå·±ä¹‹å‰é¢„æµ‹çš„è¯ã€‚</li>
</ul>
<p>The exposure bias hypothesis: Due to the exposure to ground-truth
prefix, the model is biased to only perform well during training, but
not generation. ï¼ˆè®­ç»ƒçš„æ—¶å€™å‰æ–‡æ˜¯å®Œç¾çš„æ‰€ä»¥è¡¨ç°å¥½ï¼‰</p>
<p>Importantly, the error is assumed to accumulate during generation,
and the generation will be incrementally distortedï¼ˆæ‰­æ›²ï¼‰.
è‡ªå›å½’ä¸­é”™è¯¯ç´¯åŠ </p>
<h2 id="we-cannot-directly-use-gan">we cannot directly use GAN</h2>
<p>GAN çš„ç›®æ ‡æ˜¯è®©ç”Ÿæˆå™¨ <span class="math inline">\(G\)</span> å’Œåˆ¤åˆ«å™¨
<span class="math inline">\(D\)</span> äº’ç›¸ç«äº‰ï¼š <span
class="math display">\[\min_{G} \max_{D} V(D, G) =
\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log
D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim
p_{\mathbf{z}}(\mathbf{z})}[\log(1 - D(G(\mathbf{z})))]\]</span> *
åˆ¤åˆ«å™¨ <span class="math inline">\(D\)</span> çš„ç›®æ ‡ (æœ€å¤§åŒ– <span
class="math inline">\(\max_{D}\)</span>): * æœ€å¤§åŒ– <span
class="math inline">\(\log D(\mathbf{x})\)</span> (çœŸå®æ•°æ® <span
class="math inline">\(\mathbf{x}\)</span> è¢«åˆ¤åˆ«ä¸ºçœŸçš„æ¦‚ç‡)ã€‚ * æœ€å¤§åŒ–
<span class="math inline">\(\log(1 - D(G(\mathbf{z})))\)</span>
(ç”Ÿæˆæ•°æ® <span class="math inline">\(G(\mathbf{z})\)</span>
è¢«åˆ¤åˆ«ä¸ºå‡çš„æ¦‚ç‡)ã€‚ * ç”Ÿæˆå™¨ <span class="math inline">\(G\)</span>
çš„ç›®æ ‡ (æœ€å°åŒ– <span class="math inline">\(\min_{G}\)</span>): * æœ€å°åŒ–
<span class="math inline">\(\log(1 - D(G(\mathbf{z})))\)</span>
(å³è®©ç”Ÿæˆæ•°æ® <span class="math inline">\(G(\mathbf{z})\)</span>
è¢«åˆ¤åˆ«ä¸ºçœŸçš„æ¦‚ç‡ <span class="math inline">\(D(G(\mathbf{z}))\)</span>
å°½é‡é«˜)ã€‚</p>
<ul>
<li><span class="math inline">\(G\)</span> (ç”Ÿæˆå™¨)
è¾“å‡ºçš„æ˜¯è¯æ±‡è¡¨ä¸Šæ¦‚ç‡åˆ†å¸ƒã€‚
<ul>
<li>ä¸ºäº†å¾—åˆ°ä¸€ä¸ªå…·ä½“çš„è¯åºåˆ—ï¼ˆæ–‡æœ¬ï¼‰ï¼Œéœ€è¦ä»è¿™ä¸ªåˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼ˆä¾‹å¦‚ï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯æˆ–ç”¨
Top-K é‡‡æ ·ï¼‰ã€‚</li>
<li>è¯­è¨€æ¨¡å‹ä¸­çš„è¯æ˜¯ç¦»æ•£çš„ï¼ˆä¾‹å¦‚ï¼Œ"çŒ«"ã€"ç‹—"ã€"è·‘"ï¼‰ï¼Œè€Œä¸æ˜¯åƒå›¾åƒåƒç´ å€¼é‚£æ ·çš„è¿ç»­å€¼ã€‚
gradient cannot flow back through discrete samplingï¼</li>
</ul></li>
</ul>
<h2 id="the-gumbel-softmax-reparameterization">The gumbel-softmax
reparameterization</h2>
<p>Gumbel-Maxï¼šç¦»æ•£é‡‡æ ·çš„è¿‡ç¨‹ï¼šargmaxéè¿ç»­ä¸å¯å¾® <span
class="math display">\[z = \text{one\_hot}(\arg \max_i [\log \pi_i +
g_i])\]</span></p>
<p>Gumbel-Softmaxç”¨ Softmax å‡½æ•°æ›¿æ¢äº† <span class="math inline">\(\arg
\max\)</span> æ“ä½œ</p>
<p><span class="math display">\[y_i = \frac{\exp((\log \pi_i + g_i) /
\tau)}{\sum_{j=1}^{k} \exp((\log \pi_j + g_j) / \tau)}\]</span></p>
<p>æ¸©åº¦çš„ä½œç”¨ï¼š * <span class="math inline">\(\tau\)</span> è¾ƒå¤§ï¼š<span
class="math inline">\(y\)</span>
çš„åˆ†å¸ƒä¼šæ›´å¹³æ»‘ï¼Œæ›´æ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼Œä½†è¿‘ä¼¼è¯¯å·®è¾ƒå¤§ã€‚ * <span
class="math inline">\(\tau\)</span> è¾ƒå°ï¼š<span
class="math inline">\(y\)</span> çš„åˆ†å¸ƒä¼šæ›´å°–é” (sharper)ï¼Œæ›´æ¥è¿‘
One-Hot å‘é‡ï¼Œä»è€Œæ›´å¥½åœ°è¿‘ä¼¼ç¦»æ•£çš„é‡‡æ ·ï¼Œå¹¶ä¸”æ›´æ¥è¿‘ Gumbel-Max
çš„ç»“æœã€‚</p>
<p>å¯¹äºæ¸©åº¦ä¹Ÿæœ‰diversity-quality trade-off</p>
<p>However, it is shown that language GANs are actually worse than the
MLE baseline.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gumbel_softmax</span>(<span class="hljs-params">logits, tau=<span class="hljs-number">1.0</span>, hard=<span class="hljs-literal">False</span>, dim=-<span class="hljs-number">1</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    å®ç°äº† Gumbel-Softmax é‡å‚æ•°åŒ–æŠ€å·§ã€‚</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    å‚æ•°è¯´æ˜:</span><br><span class="hljs-string">        logits (Tensor): æœªæ ‡å‡†åŒ–çš„å¯¹æ•°æ¦‚ç‡ï¼ˆæ¥è‡ªç”Ÿæˆå™¨ G çš„è¾“å‡ºï¼‰ã€‚</span><br><span class="hljs-string">        tau (float): æ¸©åº¦å‚æ•° (Ï„)ï¼Œæ§åˆ¶ Softmax è¿‘ä¼¼çš„å¹³æ»‘ç¨‹åº¦ã€‚</span><br><span class="hljs-string">        hard (bool): å¦‚æœä¸º Trueï¼Œåˆ™åº”ç”¨ Straight-Throughï¼ˆç›´é€šï¼‰æŠ€å·§ï¼š</span><br><span class="hljs-string">                     åœ¨å‰å‘ä¼ æ’­ä¸­ä½¿ç”¨ One-Hot å‘é‡ï¼Œè€Œåœ¨åå‘ä¼ æ’­ä¸­ä½¿ç”¨è¿ç»­æ¢¯åº¦ã€‚</span><br><span class="hljs-string">        dim (int): åº”ç”¨ Softmax çš„ç»´åº¦ï¼ˆé€šå¸¸æ˜¯è¯æ±‡è¡¨ç»´åº¦ï¼‰ã€‚</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># --- 1. Gumbel å™ªå£°æ³¨å…¥ ---</span><br>    <br>    <span class="hljs-comment"># 1a. ç”Ÿæˆ Gumbel å™ªå£° g_i ~ Gumbel(0, 1)ã€‚</span><br>    <span class="hljs-comment"># è¿™æ˜¯é€šè¿‡é€†å˜æ¢é‡‡æ ·æ–¹æ³•å®ç°çš„ï¼šg_i = -log(-log(U))ï¼Œå…¶ä¸­ U ~ Uniform(0, 1)ã€‚</span><br>    gumbels = -torch.empty_like(logits).exponential_().log() <br>    <br>    <span class="hljs-comment"># 1b. å°† Gumbel å™ªå£°æ·»åŠ åˆ° logits å¹¶é™¤ä»¥æ¸©åº¦ (tau)ã€‚</span><br>    <span class="hljs-comment"># è¿™å¯¹åº”äº Gumbel-Softmax å…¬å¼ä¸­çš„åˆ†å­éƒ¨åˆ†ï¼š(log(pi_i) + g_i) / tau</span><br>    gumbels = (logits + gumbels) / tau<br>    <br>    <span class="hljs-comment"># --- 2. è¿ç»­ Softmax è¾“å‡º (y_soft) ---</span><br>    <br>    <span class="hljs-comment"># è®¡ç®—è¿ç»­çš„ã€å¯å¾®åˆ†çš„ Softmax è¾“å‡ºã€‚</span><br>    <span class="hljs-comment"># è¿™ä¸ª y_soft å€¼ç”¨äºåœ¨åå‘ä¼ æ’­ä¸­è®¡ç®—æ¢¯åº¦ã€‚</span><br>    y_soft = gumbels.softmax(dim)<br><br>    <span class="hljs-comment"># --- 3. Straight-Throughï¼ˆç›´é€šï¼‰æŠ€å·§å®ç° ---</span><br>    <br>    <span class="hljs-keyword">if</span> hard:<br>        <span class="hljs-comment"># A. å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨ç¡¬æ€§ One-Hot å‘é‡ï¼‰</span><br>        <br>        <span class="hljs-comment"># æ‰¾åˆ° y_soft ä¸­æœ€å¤§æ¦‚ç‡å€¼å¯¹åº”çš„ç´¢å¼•ã€‚</span><br>        <span class="hljs-comment"># [1] ä» .max() è¿”å›çš„å…ƒç»„ä¸­æå–ç´¢å¼•ï¼ˆmax_indicesï¼‰ã€‚</span><br>        <span class="hljs-comment"># è¿™æ¨¡æ‹Ÿäº† Gumbel-Max æŠ€å·§ä¸­çš„ arg max æ“ä½œã€‚</span><br>        index = y_soft.<span class="hljs-built_in">max</span>(dim, keepdim=<span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]<br>        <br>        <span class="hljs-comment"># åŸºäºæœ€å¤§ç´¢å¼•ï¼Œåˆ›å»ºä¸€ä¸ªç¡¬æ€§ï¼ˆç¦»æ•£ï¼‰çš„ One-Hot å‘é‡ y_hardã€‚</span><br>        <span class="hljs-comment"># åœ¨å‰å‘è®¡ç®—ä¸­ï¼ˆä¾‹å¦‚ä½œä¸ºåˆ¤åˆ«å™¨ D çš„è¾“å…¥ï¼‰ï¼Œä½¿ç”¨çš„æ˜¯ y_hardã€‚</span><br>        y_hard = torch.zeros_like(logits).scatter_(dim, index, <span class="hljs-number">1.0</span>)<br>        <br>        <span class="hljs-comment"># B. åå‘ä¼ æ’­ï¼ˆä½¿ç”¨æŸ”æ€§è¿ç»­æ¢¯åº¦ï¼‰</span><br>        <br>        <span class="hljs-comment"># Straight-Through è¡¨è¾¾å¼ï¼šret = y_hard - y_soft.detach() + y_soft</span><br>        <span class="hljs-comment"># æ¢¯åº¦åˆ†æï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼š</span><br>        <span class="hljs-comment"># 1. d(y_hard)/d(logits) â‰ˆ 0 ï¼ˆç”±äºç¦»æ•£/arg max æ“ä½œï¼Œæ¢¯åº¦è¢«å¿½ç•¥ï¼‰ã€‚</span><br>        <span class="hljs-comment"># 2. d(y_soft.detach())/d(logits) = 0 ï¼ˆæ¢¯åº¦è¢« .detach() æ˜¾å¼åˆ‡æ–­ï¼‰ã€‚</span><br>        <span class="hljs-comment"># 3. d(y_soft)/d(logits) æ˜¯å”¯ä¸€æœ‰æ•ˆçš„ã€å¹³æ»‘çš„æ¢¯åº¦ã€‚</span><br>        <span class="hljs-comment"># æœ€ç»ˆæ¢¯åº¦ï¼šd(ret)/d(logits) â‰ˆ d(y_soft)/d(logits)</span><br>        ret = y_hard - y_soft.detach() + y_soft<br>        <br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># æ ‡å‡†é‡å‚æ•°åŒ–æŠ€å·§ï¼ˆä¸ä½¿ç”¨ ST æŠ€å·§ï¼‰ã€‚</span><br>        <span class="hljs-comment"># è¿ç»­çš„ y_soft å‘é‡ç”¨äºå‰å‘å’Œåå‘ä¼ æ’­ã€‚</span><br>        ret = y_soft<br>        <br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure>
<h2 id="policy-gradient">policy gradient</h2>
<p><img src="/img/nlp3/nlp=rl.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>é¿å…äº†é‡‡æ ·å¯¼è‡´çš„ä¸å¯å¾®åˆ†</p>
<p>å…¬å¼å³ä¾§çš„æ¢¯åº¦ <span class="math inline">\(\nabla_{\theta}\)</span>
åªä½œç”¨äºå¯¹æ•°æ¦‚ç‡ <span class="math inline">\(\log
P_{\theta}(\mathbf{y}|\mathbf{x})\)</span>ã€‚ * <span
class="math inline">\(\log P_{\theta}(\mathbf{y}|\mathbf{x})\)</span>
æ˜¯ä¸€ä¸ªè¿ç»­ä¸”å¯å¾®åˆ†çš„å‡½æ•°ï¼Œå®ƒæ˜¯æ¨¡å‹è¾“å‡ºæ¦‚ç‡çš„å¯¹æ•°ï¼Œä¸æ¨¡å‹å‚æ•° <span
class="math inline">\(\theta\)</span> ä¹‹é—´æœ‰æ˜ç¡®çš„ã€å¯å¾®åˆ†çš„è®¡ç®—å›¾ã€‚ *
å›æŠ¥ <span class="math inline">\(r(\mathbf{x}, \mathbf{y})\)</span>
åªæ˜¯ä¸€ä¸ªåœ¨é‡‡æ ·å®Œæˆåè®¡ç®—å‡ºæ¥çš„æ ‡é‡æƒé‡ï¼ˆå®ƒä¸ä¾èµ–äº <span
class="math inline">\(\theta\)</span>ï¼Œå› æ­¤æ±‚æ¢¯åº¦æ—¶è¢«è§†ä¸ºå¸¸é‡ï¼‰ã€‚ *
é‡‡æ ·è¿‡ç¨‹ <span class="math inline">\(\mathbf{y} \sim P_{\theta}\)</span>
è¢«ç§»åˆ°äº†æœŸæœ› <span class="math inline">\(\mathbb{E}\)</span>
çš„å¤–éƒ¨ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›ï¼ˆMonte Carloï¼‰æ–¹æ³•è¿‘ä¼¼å®ç°ã€‚</p>
<h2 id="mle-å¸Œæœ›æ›´å¤šçš„diversity">MLE å¸Œæœ›æ›´å¤šçš„diversity</h2>
<p>æœ€å¤§ä¼¼ç„¶ä¼°è®¡ - MLEï¼š <span class="math display">\[\arg \min_{\theta}
\underset{W \sim P_D}{\mathbb{E}} \left[ -\frac{1}{L} \sum_{l=0}^{L-1}
\log P_M(W_{l+1}|W_{1:l}) \right]\]</span></p>
<p>æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œ<span class="math inline">\(P_M(W) = P_M(W_1) \cdot
P_M(W_2|W_1) \cdots
P_M(W_L|W_{1:L-1})\)</span>ï¼Œå› æ­¤è´Ÿå¯¹æ•°ä¼¼ç„¶å¯ä»¥ç®€åŒ–ä¸ºå¯¹æ•´ä¸ªåºåˆ— <span
class="math inline">\(W\)</span> çš„è´Ÿå¯¹æ•°ä¼¼ç„¶çš„æœŸæœ›ã€‚</p>
<p><span class="math display">\[\arg \min_{\theta} \underset{W \sim
P_D}{\mathbb{E}} \left[ -\log P_M(W) \right]\]</span></p>
<p><span class="math display">\[D_{KL}(P_D || P_M) = \underset{W \sim
P_D}{\mathbb{E}} [\log P_D(W)] - \underset{W \sim P_D}{\mathbb{E}} [\log
P_M(W)]\]</span></p>
<p>å‰ä¸€é¡¹å°±æ˜¯çœŸå®åˆ†å¸ƒçš„entropyçš„è´Ÿæ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹å‡ºMLEæ˜¯å¸Œæœ›modelçš„åˆ†å¸ƒéå¸¸æ¥è¿‘çœŸå®çš„åˆ†å¸ƒ</p>
<p><img src="/img/nlp3/nlp=kld.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2
id="exposure-bias-ä¸æ˜¯å¤§é—®é¢˜the-prefix-switching-experiment">exposure
bias ä¸æ˜¯å¤§é—®é¢˜â€”â€”The Prefix Switching Experiment</h2>
<p>ç¬¬ä¸€ç»„ï¼Œç»™çœŸå®çš„å¥å­</p>
<p>ç¬¬äºŒç»„ï¼Œè‡ªå›å½’</p>
<p>ç¬¬ä¸‰ç»„ï¼Œéšæœºtokenå‰ç¼€</p>
<p>Did the error accumulate ?
æ¨¡å‹åœ¨å‰ç¼€ä¸­æ¥æ”¶åˆ°äº†ä¸¥é‡çš„é”™è¯¯æˆ–å¹²æ‰°ï¼ˆæ‰“ä¹±çš„æˆ–éšæœºçš„
Tokenï¼‰ï¼Œä½†å®ƒå¹¶æ²¡æœ‰å¯¼è‡´æ•´ä¸ªåç»­åºåˆ—çš„ç”Ÿæˆè´¨é‡ç¾éš¾æ€§åœ°ä¸‹é™ã€‚</p>
<p>Mysteriously, the model self-recovers from the errors in the
prefix.</p>
<p>For the shuffled data prefix, the model still generates something
related to the air force.</p>
<p>EB-M, quantifies the quality ratio between generations from data
prefix and a given (imperfect) prefix.</p>
<p>The combination of MLE training and sampling algorithm is very strong
and is the default choice.</p>
<h1 id="sampling">sampling</h1>
<p>topk</p>
<p><span class="math display">\[\hat{p}_i = \frac{p_i \cdot
\mathbf{1}\{i \leq K\}}{Z}\]</span></p>
<p>topp</p>
<p><span class="math display">\[\hat{p}_i = \frac{p_i \cdot
\mathbf{1}\{\sum_{j=1}^{i-1} p_j &lt; P\}}{Z}\]</span></p>
<p>Temperature Sampling</p>
<p><span class="math display">\[\hat{p}_i = \frac{\exp(\log(p_i)/T)}{Z}
= \frac{(p_i)^{1/T}}{Z}\]</span></p>
<h2 id="å®ƒä»¬æœ‰ä»€ä¹ˆå…±åŒç‚¹">ğŸ¤” å®ƒä»¬æœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿ</h2>
<p>Entropy Reduction Order Preservation Slope Preservation</p>
<h3 id="å…ƒç´ é¡ºåºå¾—ä»¥ä¿ç•™">1. å…ƒç´ é¡ºåºå¾—ä»¥ä¿ç•™</h3>
<p><span class="math inline">\(p_i \ge p_j \implies \hat{p}_i \ge
\hat{p}_j\)</span></p>
<h3 id="åˆ†å¸ƒçš„ç†µè¢«é™ä½">2. åˆ†å¸ƒçš„ç†µè¢«é™ä½</h3>
<p><span class="math inline">\(\mathcal{H}(\hat{p}) \le
\mathcal{H}(p)\)</span></p>
<h3 id="åˆ†å¸ƒçš„æ–œç‡å¾—ä»¥ä¿ç•™">3. åˆ†å¸ƒçš„æ–œç‡å¾—ä»¥ä¿ç•™</h3>
<p><span class="math display">\[\frac{\log p_i - \log p_j}{\log p_l -
\log p_k} = \frac{\log \hat{p}_i - \log \hat{p}_j}{\log \hat{p}_l - \log
\hat{p}_k}\]</span></p>
<h1 id="correcting-bad-behavior-of-nlg-models">Correcting bad behavior
of NLG models</h1>
<h2 id="repeat">repeat</h2>
<p>è¿™ç§åŸºäºç‚¹ç§¯çš„åŒ¹é…æœºåˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨è§£ç  (Decoding) é˜¶æ®µä¸è´ªå©ªæœç´¢
(Greedy Search) æˆ–æ³¢æŸæœç´¢ (Beam Search) ç»“åˆæ—¶ï¼Œç‰¹åˆ«å®¹æ˜“å¯¼è‡´é‡å¤ï¼š</p>
<ul>
<li>è¯­ä¹‰æƒ¯æ€§ï¼šTransformer
æœ¬è´¨ä¸Šæ˜¯æ“…é•¿æ•æ‰å±€éƒ¨ä¾èµ–å’Œè¯­ä¹‰ä¸€è‡´æ€§çš„ã€‚å½“ä¸€ä¸ªåºåˆ—åœ¨çŸ­æ—¶é—´å†…é‡å¤å‡ºç°æ—¶ï¼Œæ¨¡å‹ä¼šè®¤ä¸ºâ€œä¿æŒè¿™ç§æ¨¡å¼â€æ˜¯æœ€å®‰å…¨ã€æœ€ä¸€è‡´çš„é€‰æ‹©ï¼Œä»è€Œåœ¨ä¼—å¤šå¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯ä¸­ï¼Œå€¾å‘äºé€‰æ‹©ä¸å½“å‰å†å²ç›¸ä¼¼åº¦æœ€é«˜çš„è¯ï¼ˆå³ç‚¹ç§¯åˆ†æ•°æœ€é«˜çš„è¯ï¼‰ã€‚</li>
<li>ç¼ºä¹å…¨å±€æŠ‘åˆ¶ï¼šæ ‡å‡†çš„ Transformer
æ¶æ„åœ¨è®¾è®¡ä¸Šç¼ºä¹ä¸€ä¸ªå†…ç½®çš„æœºåˆ¶æ¥æƒ©ç½šæˆ–æŠ‘åˆ¶åˆšåˆšç”Ÿæˆè¿‡çš„å†…å®¹ã€‚å®ƒåªæ˜¯åœ¨å¯»æ‰¾å±€éƒ¨æœ€ä½³åŒ¹é…ï¼Œè€Œç‚¹ç§¯æ­£æ˜¯æ‰¾åˆ°è¿™ä¸ªå±€éƒ¨æœ€ä½³åŒ¹é…ï¼ˆç›¸ä¼¼åº¦ï¼‰çš„æœ‰æ•ˆå·¥å…·ã€‚</li>
</ul>
<h3 id="biased-decoding">Biased decoding</h3>
<p><span class="math display">\[p_i = \frac{\exp(x_i / (T \cdot I(i \in
g)))}{\sum_{j} \exp(x_j / (T \cdot I(j \in g)))}\]</span></p>
<ul>
<li><p><span class="math inline">\(I(c)\)</span>ï¼šè¿™æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‡½æ•°
(Indicator Function) æˆ–æƒ©ç½šå› å­ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š <span
class="math display">\[I(c) = \begin{cases} \theta &amp; \text{if } c
\text{ is True} \\ 1 &amp; \text{else} \end{cases}\]</span></p></li>
<li><p><span class="math inline">\(\theta\)</span> (Theta)ï¼šè®¾å®šä¸º <span
class="math inline">\(1.2\)</span>ï¼Œå¯¹é‡å¤ token
çš„æƒ©ç½š/åå·®å› å­ã€‚</p></li>
<li><p>å¯¹äºæ–°çš„ã€æœªé‡å¤çš„ Token <span class="math inline">\(i\)</span>ï¼š
æŒ‡ç¤ºå‡½æ•° <span class="math inline">\(I(i \in g)\)</span> çš„å€¼ä¸º <span
class="math inline">\(1\)</span>ã€‚</p></li>
<li><p>å¯¹äºå·²ç”Ÿæˆçš„ã€é‡å¤çš„ Token <span
class="math inline">\(j\)</span>ï¼š æŒ‡ç¤ºå‡½æ•° <span
class="math inline">\(I(j \in g)\)</span> çš„å€¼ä¸º <span
class="math inline">\(\theta = 1.2\)</span>ï¼Œå¯¼è‡´æ•´ä¸ªåˆ†æ•° <span
class="math inline">\(x_j / (T \cdot \theta)\)</span>
å˜å°ï¼Œé™ä½é‡å¤çš„æ¦‚ç‡</p></li>
</ul>
<h3 id="unlikelihood-training-for-repetition">Unlikelihood training for
repetition</h3>
<p><span class="math display">\[\mathcal{L}^t_{\text{UL-token}} \left(
p_\theta(\cdot|x_{&lt;t}), c^t \right) = - \alpha \sum_{c \in c^t}
\log(1 - p_\theta(c|x_{&lt;t})) - \log
p_\theta(x_t|x_{&lt;t})\]</span></p>
<p>æœ€å¤§åŒ–ä¼¼ç„¶ï¼ˆåä¸€é¡¹ï¼‰ï¼Œæœ€å°åŒ–é¢„æµ‹å‡ºé‡å¤tokençš„æ¦‚ç‡ï¼ˆå‰ä¸€é¡¹ï¼‰</p>
<h2 id="generic-response-problem">Generic Response Problem</h2>
<p>æ¨¡å‹ç»™å‡ºçš„genericçš„å›å¤ï¼Œè€Œä¸æ˜¯æœ‰é’ˆå¯¹æ€§çš„ä¿¡æ¯é‡å¤§çš„å›å¤</p>
<p>Combined with MLE training, when the model is not sure about what to
say, it degrades to some simple and â€œsafeâ€ pattern in data.</p>
<h3 id="maximum-mutual-information-mmi">Maximum Mutual Information
ï¼ˆMMIï¼‰</h3>
<p><span class="math display">\[\text{MMI}(S; T) = \log \frac{P(S,
T)}{P(S)P(T)}\]</span></p>
<p><span class="math display">\[\hat{T} = \arg \max_T \left\{ \log
P(T|S) - \log P(T) \right\}\]</span></p>
<p>åœ¨æœ‰å‰æ–‡çš„æ¡ä»¶ä¸‹åæ–‡å‡ºç°çš„æ¦‚ç‡-åæ–‡å•ç‹¬å‡ºç°çš„æ¦‚ç‡ï¼Œæƒ©ç½šå’Œå‰æ–‡å…³ç³»ä¸å¤§çš„é€šç”¨å›ç­”ã€‚</p>
<h3 id="negative-training">negative training</h3>
<p>ç”¨è´Ÿæ ·æœ¬å‘Šè¯‰å¤§æ¨¡å‹ä¸åº”è¯¥å»è¯´ä»€ä¹ˆ</p>
<p><span class="math display">\[\text{Loss}_{\text{new}} = \underbrace{-
\log P_\theta(y_{\text{pos}}|x_{\text{pos}})}_{\text{æœ€å¤§ä¼¼ç„¶é¡¹
(æ­£æ ·æœ¬)}} + \underbrace{\log
P_\theta(y_{\text{neg}}|x_{\text{neg}})}_{\text{è´Ÿæ ·æœ¬æƒ©ç½šé¡¹}}\]</span></p>
<h1 id="transformer-encoder-decoder">Transformer encoder-decoder</h1>
<p>Each decoder layer is a selfattention followed by a
crossattention.</p>
<p>The query vector for a transformer decoderâ€™s cross-attention head is
from the output of the previous decoder layer. However, the key and
value vectors are from the encodersâ€™ outputs.</p>
<h2
id="why-decoder-only-lm-is-more-convenient-less-design-choiceto-consider-or-efficient-for-both-training-or-application">Why
decoder-only LM is more convenient (less design choiceto consider) or
efficient (for both training or application)?</h2>
<p>Pretraining (left) and chatbot generation (right) are highly
consistent.</p>
<p>â€¢ Naturally handles variable-length text generation during
pretraining!</p>
<p>â€¢ During application, we just do natural concatenation (always causal
attention). No computation is wasted (assuming we save hidden states of
the history).</p>
<p><img src="/img/nlp3/nlp=dec.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>In pretraining, we need to build text of variable length, and the
training signal is only from the decoder side.</p>
<p>â€¢ During application, we need to re-encode (especially when the
encoder is bi-directional) the whole history for each dialogue turn.</p>
<p><img src="/img/nlp3/nlp=encdec.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="rope">rope</h2>
<p>We want the dot product between query (position <span
class="math inline">\(m\)</span>) and key (position <span
class="math inline">\(n\)</span>) to directly be a function of (<span
class="math inline">\(m-n\)</span>).</p>
<p><span class="math display">\[
        \langle \boldsymbol{f}_{\boldsymbol{q}}(\boldsymbol{x}_m, m),
\boldsymbol{f}_{\boldsymbol{k}}(\boldsymbol{x}_n, n) \rangle =
\boldsymbol{g}(\boldsymbol{x}_m, \boldsymbol{x}_n, m-n)
        \]</span></p>
<h1 id="gpt3-and-in-context-learning">GPT3 and in-context learning</h1>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ˜¯æŒ‡
å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åˆ†æå…¶è¾“å…¥æç¤ºï¼ˆPromptï¼‰ä¸­åµŒå…¥çš„å°‘æ•°ç¤ºä¾‹æˆ–æ¼”ç¤ºï¼ˆDemonstrationsï¼‰ï¼Œæ¥å¿«é€Ÿç†è§£å¹¶æ‰§è¡Œç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œä¼ ç»Ÿçš„æ¨¡å‹å‚æ•°æ›´æ–°ï¼ˆå³æ— éœ€å¾®è°ƒæˆ–æ¢¯åº¦ä¸‹é™ï¼‰ã€‚</p>
<h2 id="few-shot-before-gpt3">few shot before GPT3</h2>
<p>Before GPT3, few-shot learning still refers to how a model can
quickly adapt to a new task demonstrated with only a few examples via
gradient update.</p>
<p>We have a meta-learning phase on a wide set of tasks. In effect, the
meta-learning problem treats entire tasks as training
examples.ï¼ˆæŠŠä¸€ä¸ªä»»åŠ¡çœ‹ä½œä¸€ä¸ªæ ·æœ¬ï¼Œæƒ³è¦å­¦ä¹ åˆ°ä¸€ä¸ªæ¯”è¾ƒå¥½çš„åˆå§‹é…ç½®ï¼‰</p>
<h2 id="chain-of-thought-cot">chain of thought ï¼ˆcotï¼‰</h2>
<p>ideaï¼šreasoning is more consuming than computationï¼Œgive llm more
time to think</p>
<p>cot çš„æ•ˆæœåœ¨æ¨¡å‹å¤§çš„æ—¶å€™æ›´åŠ æ˜æ˜¾</p>
<p>few-shot: ç»™å‡ ä¸ªä¾‹å­â€”â€”add manually written reasoning before giving
answer in prompt.</p>
<p>zero-shot: chaining of 2 prompts.
ç¬¬ä¸€æ­¥æ˜ç¡®è¯´è¦ä¸€æ­¥ä¸€æ­¥æ¨ç†ï¼Œè·å¾—æ¨ç†çš„è¿‡ç¨‹ã€‚ç¬¬äºŒæ­¥æ¥ç€è¿™ä¸ªå‘Šè¯‰å®ƒè¾“å‡ºçš„æ ¼å¼ï¼Œè·å¾—æ­£ç¡®çš„è¾“å‡ºã€‚</p>
<h3 id="research">research</h3>
<h4 id="cot-with-self-consistency">CoT with self-consistency</h4>
<p>For CoT, we could sample multiple reasoning path from the LLM with
temperature sampling. æ¸©åº¦è¶Šé«˜è¶Šéšæœºï¼Œè¶Šæœ‰diversity</p>
<p>And then take a majority voting over the answers!</p>
<h4 id="tree-of-thoughts">Tree of Thoughts</h4>
<p>Maintain and expand a thought-tree.</p>
<p>â€¢ For each existing step, we prompt the LLM to propose multiple next
steps, and also to judge which path (by giving a value) is more
promising (pls refer to paper for how the prompts are designed).</p>
<p>â€¢ The nodes that are judged to be unlikely will be discarded</p>
<p><img src="/img/nlp3/nlp=cot.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h4 id="bias-in-icl">bias in icl</h4>
<p>Majority and recency bias</p>
<p>å¤šæ•°åå·®æ˜¯æŒ‡åœ¨å°‘æ ·æœ¬å­¦ä¹ çš„æç¤ºä¸­ï¼Œå¦‚æœæä¾›çš„è®­ç»ƒç¤ºä¾‹çš„ç±»åˆ«åˆ†å¸ƒæ˜¯ä¸å¹³è¡¡çš„ï¼Œæ¨¡å‹å°±ä¼šå€¾å‘äºé¢„æµ‹å‡ºç°æ¬¡æ•°æœ€å¤šçš„é‚£ä¸ªç±»åˆ«ã€‚å³ä½¿ä¸€ä¸ªæ–°çš„æµ‹è¯•æ ·æœ¬å®¢è§‚ä¸Šå±äºå°‘æ•°ç±»åˆ«ï¼Œæ¨¡å‹ä¹Ÿæ›´å€¾å‘äºè¾“å‡ºå¤šæ•°ç±»åˆ«ï¼Œä»è€Œç‰ºç‰²äº†å°‘æ•°ç±»åˆ«çš„å¬å›ç‡å’Œæ•´ä½“å‡†ç¡®æ€§ã€‚</p>
<p>è¿‘å› åå·®æ˜¯æŒ‡åœ¨å°‘æ ·æœ¬å­¦ä¹ çš„æç¤ºä¸­ï¼Œæ¨¡å‹ä¼šå€¾å‘äºé¢„æµ‹åœ¨æç¤ºæœ«å°¾ï¼ˆæˆ–æœ€è¿‘ï¼‰å‡ºç°çš„ç±»åˆ«ã€‚å³ä½¿æç¤ºä¸­çš„ç±»åˆ«åˆ†å¸ƒæ˜¯å¹³è¡¡çš„ï¼Œä»…ä»…æ”¹å˜ç¤ºä¾‹çš„é¡ºåºï¼Œä¹Ÿä¼šæ˜¾è‘—æ”¹å˜æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œå¯¼è‡´é¢„æµ‹çš„é«˜æ–¹å·®ï¼ˆä¸ç¨³å®šï¼‰ã€‚</p>
<p>Calibration ï¼ˆä¿®æ­£ï¼‰of few-shot prediction</p>
<p><span class="math display">\[\mathbf{\hat{q}} =
\text{softmax}(\mathbf{W}\mathbf{\hat{p}} + \mathbf{b})\]</span> * <span
class="math inline">\(\mathbf{\hat{p}}\)</span>ï¼šæ¨¡å‹ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹
LLMï¼‰åŸå§‹é¢„æµ‹çš„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒå‘é‡ã€‚ * <span
class="math inline">\(\mathbf{W}\)</span>ï¼šæƒé‡çŸ©é˜µã€‚ * <span
class="math inline">\(\mathbf{b}\)</span>ï¼šåç½®å‘é‡ (bias)ã€‚ * <span
class="math inline">\(\mathbf{\hat{q}}\)</span>ï¼šæ ¡å‡†åçš„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒå‘é‡ã€‚</p>
<p>åˆ©ç”¨æ¨¡å‹å¯¹â€œç©ºè¾“å…¥â€çš„é¢„æµ‹ <span
class="math inline">\(\text{prediction}_{\text{null}}\)</span>
æ¥æŠµæ¶ˆå…¶å›ºæœ‰çš„åå·®ã€‚ * è®¾ç½® <span
class="math inline">\(\mathbf{b}\)</span>ï¼š å°†åç½®å‘é‡ <span
class="math inline">\(\mathbf{b}\)</span> è®¾ä¸º <span
class="math inline">\(\mathbf{0}\)</span>ã€‚ * è®¾ç½® <span
class="math inline">\(\mathbf{W}\)</span>ï¼š å°†æƒé‡çŸ©é˜µ <span
class="math inline">\(\mathbf{W}\)</span>
è®¾ä¸ºä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå…¶å¯¹è§’çº¿å…ƒç´ æ˜¯ç©ºè¾“å…¥é¢„æµ‹æ¦‚ç‡çš„å€’æ•°ã€‚ <span
class="math display">\[\mathbf{W} =
\text{diag}(\text{prediction}_{\text{null}})^{-1}\]</span>
è¿™æ˜¯å¸Œæœ›æœ€åç©ºè¾“å…¥çš„è¾“å‡ºåˆ†å¸ƒæ˜¯å‡åŒ€çš„</p>
<h4 id="induction-attention-head-for-repetition">Induction attention
head: for repetition</h4>
<p>â€œå½’çº³æ³¨æ„åŠ›å¤´â€ä¸æ˜¯ Transformer
æ¶æ„ä¸­é¢„è®¾çš„ç»„ä»¶ï¼Œè€Œæ˜¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªå‘å­¦ä¹ åˆ°çš„ã€ç”±ä¸€ä¸ªæˆ–å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼ˆé€šå¸¸æ˜¯ä¸¤ä¸ªå¤´åœ¨ä¸åŒå±‚ä¸­åä½œï¼‰ç»„æˆçš„åŠŸèƒ½æ€§ç”µè·¯ï¼ˆCircuitï¼‰ã€‚</p>
<p>å‡è®¾è¾“å…¥åºåˆ—æ˜¯ï¼š...[A][B]...[A]</p>
<p>å½“æ¨¡å‹å¤„ç†ç¬¬äºŒä¸ª [A] æ—¶ï¼Œå½’çº³å¤´ä¼šæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š</p>
<p>åŒ¹é…ï¼ˆMatchï¼‰ï¼š å®ƒä¼šå›é¡¾åºåˆ—ï¼Œæ‰¾åˆ°ä¸Šä¸€æ¬¡å‡ºç° [A] çš„ä½ç½®ã€‚</p>
<p>å¤åˆ¶/é¢„æµ‹ï¼ˆCopy/Predictï¼‰ï¼š å®ƒä¼šæŸ¥çœ‹ä¸Šä¸€æ¬¡ [A] åé¢ç´§è·Ÿç€çš„æ ‡è®°
[B]ï¼Œå¹¶åˆ©ç”¨è¿™ä¸ªä¿¡æ¯æ¥é¢„æµ‹å½“å‰ç¬¬äºŒä¸ª [A] åé¢ä¹Ÿåº”è¯¥è·Ÿç€ [B]ã€‚</p>
<p>ç®€å•æ¥è¯´ï¼Œå®ƒèƒ½å‘ç°å¹¶åº”ç”¨åºåˆ—ä¸­é‡å¤å‡ºç°çš„ [A] â†’ [B] æ¨¡å¼ã€‚</p>
<h1 id="instruction-tuning">Instruction tuning</h1>
<p>motivation: we are lazy, å¸Œæœ›zero-shot promptingï¼ˆä¸ç»™ä¾‹å­ï¼‰</p>
<h2 id="flan-finetuned-language-net">FLAN (Finetuned Language Net)</h2>
<p>Simple idea: After pretraining, we finetune the language model on a
good amount of â€œinstruction followingâ€ data.</p>
<p>Each training samples contains the task description, an input, and
the target output.</p>
<p>During evaluation, we hope the model can generalize to unseen task
type.</p>
<p>FLAN data construction: Collected data from 62 existing NLP tasks.
For each task, manually compose ten unique templates (for diversity)
that use natural language instructions to describe the task.</p>
<p><img src="/img/nlp3/nlp=comp.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h1
id="alignment-with-reinforcement-learning-human-feed-back-rlhf">Alignment
with reinforcement learning human feed back (RLHF)</h1>
<p>We collect samples from the model, and ask labelers to rank them.
These ranks are used to train the reward model</p>
<p>This reward model is used for RL.</p>
<h2 id="why-is-it-practical">Why is it practical?</h2>
<p>â€¢ Itâ€™s also easier for the human labeler to rank the responses, than
coming up with a better response.</p>
<p>â€¢ From pretraining, the LLM might be strong enough to give a good
sample when you sample enough times.</p>
<h2 id="what-could-be-its-advantage">what could be its advantage</h2>
<p>(comparing to, say, more supervised finetuning on high-quality
data)?</p>
<p>â€¢ Itâ€™s usually easier to train a good discriminator than a good
generator (especially now that we can use base the reward model on an
existing LLM).</p>
<p>â€¢ By giving low reward, we are teaching the model â€œwhat not to sayâ€
by sampling from it.</p>
<h2
id="è®©æœºå™¨ç”Ÿæˆæ›´ç¬¦åˆæˆ‘ä»¬éœ€æ±‚çš„å›ç­”">è®©æœºå™¨ç”Ÿæˆæ›´ç¬¦åˆæˆ‘ä»¬éœ€æ±‚çš„å›ç­”</h2>
<p>Trivial method: Promptingï¼ˆDirectly prompt the LM to alignï¼‰</p>
<p>Pros: Training-free;</p>
<p>Cons: No guarantee that the model will precisely follow, and requires
careful prompt design</p>
<p>Best-of-N</p>
<ol type="1">
<li>Samples multiple solutions;</li>
<li>Chooses the one with the highest score given by a reward model.</li>
</ol>
<p>Pros: Do not need to train the policy model, simple and powerful;</p>
<p>Cons: not efficient and you might need a large N</p>
<h2 id="objective">objective</h2>
<p><span class="math display">\[\max_{\pi_{\theta}} \mathbb{E}_{x \sim
\mathcal{D}, y \sim \pi_{\theta}(\cdot|x)} \left[ r_{\phi}(x, y) \right]
- \beta \mathbb{D}_{\text{KL}} \left[ \pi_{\theta}(\cdot | x) \parallel
\pi_{\text{ref}}(\cdot | x) \right]\]</span></p>
<p><span class="math display">\[\mathbb{E}_{x \sim \mathcal{D}, y \sim
\pi_{\theta}(\cdot|x)} \left[ r_{\phi}(x, y) - \beta (\log
\pi_{\theta}(y|x) - \log \pi_{\text{ref}}(y|x)) \right]\]</span></p>
<p>å¥–åŠ±æ¨¡å‹ <span class="math inline">\(\phi\)</span> ,
å‰ä¸€é¡¹æ˜¯ä¸ºäº†æé«˜å¥–åŠ±</p>
<p>åä¸€é¡¹é˜²æ­¢ä¼˜åŒ–åçš„æ¨¡å‹ <span
class="math inline">\(\pi_{\theta}\)</span> åç¦»å¤ªè¿œï¼ˆdeviating too
farï¼‰äºåˆå§‹å‚è€ƒæ¨¡å‹ <span
class="math inline">\(\pi_{\text{ref}}\)</span> ï¼ˆReward
over-optimization issueï¼‰ The reward model is an imperfect proxy,
optimizing its value too much can hinder ground truth performance (first
increase, then decrease).</p>
<p><span class="math inline">\(\pi_{\theta}(\cdot | x)\)</span> å’Œ <span
class="math inline">\(\pi_{\text{ref}}(\cdot | x)\)</span>å«ä¹‰ï¼š
å®ƒä»¬ä»£è¡¨åœ¨ç»™å®šè¾“å…¥ <span class="math inline">\(x\)</span>
çš„æ¡ä»¶ä¸‹ï¼Œæ‰€æœ‰å¯èƒ½çš„è¾“å‡º <span class="math inline">\(y\)</span>
ä¸Šçš„å®Œæ•´æ¦‚ç‡åˆ†å¸ƒã€‚</p>
<p><span class="math inline">\(\pi_{\theta}(y|x)\)</span> å’Œ <span
class="math inline">\(\pi_{\text{ref}}(y|x)\)</span>å«ä¹‰ï¼š
å®ƒä»¬ä»£è¡¨åœ¨ç»™å®šè¾“å…¥ <span class="math inline">\(x\)</span>
çš„æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹ç”Ÿæˆç‰¹å®šå›å¤ <span class="math inline">\(y\)</span>
çš„æ¦‚ç‡ã€‚</p>
<p>syntheticï¼ˆäººé€ çš„ï¼‰ setting for the Gold
modelï¼šå®é™…ä¸Šå¹¶æ²¡æœ‰çœŸçš„ç”¨äººç±»æ ‡è®°ï¼Œè€Œæ˜¯ä½¿ç”¨å¤§æ¨¡å‹æ ‡è®°</p>
<h2 id="ppo">PPO</h2>
<table>

<thead>
<tr>
<th style="text-align: left;">ç®—æ³•</th>
<th style="text-align: left;">å¯¹åº”è¡Œä¸º</th>
<th style="text-align: left;">ç»“æœ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ç­–ç•¥æ¢¯åº¦ (PG)</td>
<td
style="text-align: left;">å¸æœºéå¸¸æ¿€è¿›ï¼Œä¸€è„šæ²¹é—¨åˆ°åº•æˆ–ä¸€è„šåˆ¹è½¦è¸©æ­»ã€‚</td>
<td style="text-align: left;">æ–¹å·®å¤ªå¤§ (Variances are too
high)ï¼Œå®¹æ˜“å¯¼è‡´è®­ç»ƒä¸ç¨³å®šç”šè‡³å´©æºƒã€‚</td>
</tr>
<tr>
<td style="text-align: left;">TRPO</td>
<td
style="text-align: left;">å¸æœºçŸ¥é“è¦æ¸©å’Œé©¾é©¶ï¼Œä½†æ¯æ¬¡å¯åŠ¨å‰éƒ½è¦ç”¨å¤æ‚çš„æ•°å­¦å…¬å¼ç²¾ç¡®è®¡ç®—æ–¹å‘ç›˜è½¬è§’å’Œæ²¹é—¨æ·±åº¦ã€‚</td>
<td
style="text-align: left;">å®‰å…¨ç¨³å®šï¼Œä½†å®ç°æå…¶å¤æ‚ï¼Œè®¡ç®—æˆæœ¬é«˜ã€‚</td>
</tr>
<tr>
<td style="text-align: left;">PPO-Clip</td>
<td
style="text-align: left;">å¸æœºå­¦äº†ä¸€ä¸ªç®€å•çš„â€œå®‰å…¨è§„åˆ™â€ï¼šå¦‚æœå½“å‰æ“ä½œè¢«è®¤ä¸ºå¾ˆå¥½ï¼Œå°±é¼“åŠ±ä»–ç»§ç»­ï¼Œä½†ä¸èƒ½è¶…è¿‡ä¸€ä¸ªå›ºå®šçš„é™åº¦ã€‚å¦‚æœæ“ä½œä¸å¥½ï¼Œå°±é™åˆ¶ä»–åˆ«åšå¾—å¤ªå·®ã€‚</td>
<td style="text-align: left;">å®‰å…¨ä¸”é«˜æ•ˆã€‚
å®ƒç”¨ä¸€ä¸ªç®€å•çš„â€œæˆªæ–­â€æœºåˆ¶ï¼Œè¾¾åˆ°äº†ä¸ TRPO
ç›¸ä¼¼çš„ç¨³å®šæ•ˆæœï¼Œä½†é¿å…äº†å¤æ‚çš„è®¡ç®—ã€‚</td>
</tr>
</tbody>
</table>
<p>PPO çš„ç›´è§‰å°±ä½“ç°åœ¨å®ƒçš„ CLIP ç›®æ ‡å‡½æ•°ä¸­ï¼š</p>
<p><span class="math display">\[L^{CLIP}(\theta) = \mathbb{E}_t \left[
\min\left( p_t(\theta) \hat{A}_t, \text{clip}(p_t(\theta), 1-\epsilon,
1+\epsilon) \hat{A}_t \right) \right]\]</span></p>
<ol type="1">
<li><span class="math inline">\(p_t(\theta)\)</span>
(æ–°æ—§ç­–ç•¥çš„æ¯”ç‡)ï¼šä»£è¡¨æ–°ç­–ç•¥ <span
class="math inline">\(\pi_\theta\)</span> ç›¸å¯¹äºæ—§ç­–ç•¥ <span
class="math inline">\(\pi_{\theta_{old}}\)</span> â€œå˜åŠ¨äº†å¤šå°‘â€ã€‚</li>
<li>ä¼˜åŠ¿å‡½æ•° <span
class="math inline">\(\hat{A}_t\)</span>ï¼šä»£è¡¨å½“å‰åŠ¨ä½œ <span
class="math inline">\(a_t\)</span> çš„å¥½åç¨‹åº¦ã€‚</li>
<li>æ ¸å¿ƒç›´è§‰ï¼ˆæœ€å°åŒ–å’Œæˆªæ–­ï¼‰ï¼š
<ul>
<li>å½“ <span class="math inline">\(\hat{A}_t\)</span>
ä¸ºæ­£ï¼ˆåŠ¨ä½œå¥½ï¼‰ï¼šPPO æƒ³è¦æé«˜è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼ˆå¢å¤§ <span
class="math inline">\(p_t(\theta)\)</span>ï¼‰ï¼Œä½† <span
class="math inline">\(\min\)</span> å‡½æ•°ä¼šç¡®ä¿è¿™ä¸ªæ¯”ç‡ä¸ä¼šè¶…è¿‡ <span
class="math inline">\(1+\epsilon\)</span>ã€‚è¿™å°±å¥½åƒåœ¨è¯´ï¼šâ€œä½ åšå¾—å¾ˆå¥½ï¼Œä½†æˆ‘åªå¥–åŠ±ä½ åˆ°è¿™ä¸ªç¨‹åº¦ï¼Œé˜²æ­¢ä½ å¤ªå¾—æ„å¿˜å½¢ï¼ŒæŠŠç­–ç•¥æ”¹å¾—é¢ç›®å…¨éã€‚â€</li>
<li>å½“ <span class="math inline">\(\hat{A}_t\)</span>
ä¸ºè´Ÿï¼ˆåŠ¨ä½œå·®ï¼‰ï¼šPPO æƒ³è¦é™ä½è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼ˆå‡å° <span
class="math inline">\(p_t(\theta)\)</span>ï¼‰ï¼Œä½† <span
class="math inline">\(\min\)</span> å‡½æ•°ä¼šç¡®ä¿è¿™ä¸ªæ¯”ç‡ä¸ä¼šä½äº <span
class="math inline">\(1-\epsilon\)</span>ã€‚è¿™å°±å¥½åƒåœ¨è¯´ï¼šâ€œä½ åšå¾—å¾ˆå·®ï¼Œæˆ‘æƒ©ç½šä½ ï¼Œä½†æƒ©ç½šä¸èƒ½å¤ªé‡ï¼Œé˜²æ­¢ä½ ä¸€æ¬¡æ€§æŠŠç­–ç•¥æ”¹é”™ã€‚â€</li>
</ul></li>
</ol>
<p>PPOçš„é—®é¢˜ï¼š too much hyper parameters</p>
<h2 id="dpo">DPO</h2>
<p>Advantage: We no longer need a reward model or a value model.</p>
<p><span class="math display">\[\mathcal{L}_{\text{DPO}}(\pi_{\theta};
\pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[
\log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w |
x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_{\theta}(y_l |
x)}{\pi_{\text{ref}}(y_l | x)} \right) \right].\]</span> <span
class="math inline">\(y_w\)</span>ï¼šæ¨¡å‹å¯¹ <span
class="math inline">\(x\)</span> ç”Ÿæˆçš„è¢«é€‰æ‹©/åå¥½ (winner)
çš„å›å¤ã€‚<span class="math inline">\(y_l\)</span>ï¼šæ¨¡å‹å¯¹ <span
class="math inline">\(x\)</span> ç”Ÿæˆçš„è¢«æ‹’ç»/ä¸åå¥½ (loser)
çš„å›å¤ã€‚</p>
<p><span
class="math display">\[\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta};
\pi_{\text{ref}}) = -\beta \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}
\left[ \sigma(\hat{r}_{\theta}(x, y_l) - \hat{r}_{\theta}(x, y_w))
\left[ \nabla_{\theta} \log \pi_{\theta}(y_w | x) - \nabla_{\theta} \log
\pi_{\theta}(y_l | x) \right] \right].\]</span></p>
<h4 id="a.-ç­–ç•¥æ›´æ–°æ–¹å‘-action-term">A. ç­–ç•¥æ›´æ–°æ–¹å‘ (Action Term)</h4>
<p><span class="math display">\[\left[ \nabla_{\theta} \log
\pi_{\theta}(y_w | x) - \nabla_{\theta} \log \pi_{\theta}(y_l | x)
\right]\]</span> * è¿™æ˜¯æ¢¯åº¦ä¸Šå‡æ–¹å‘</p>
<h4 id="b.-å¥–åŠ±ä¼°è®¡-estimated-reward">B. å¥–åŠ±ä¼°è®¡ (Estimated
Reward)</h4>
<p><span class="math display">\[\hat{r}_{\theta}(x, y) = \beta \log
\frac{\pi_{\theta}(y | x)}{\pi_{\text{ref}}(y | x)}\]</span> * DPO
çš„å…³é”®åœ¨äºï¼Œå®ƒéšå«åœ°å°†å¥–åŠ±æ¨¡å‹ <span class="math inline">\(r(x,
y)\)</span> æ›¿æ¢æˆäº†ç­–ç•¥æ¨¡å‹ <span
class="math inline">\(\pi_{\theta}\)</span> ç›¸å¯¹äºå‚è€ƒæ¨¡å‹ <span
class="math inline">\(\pi_{\text{ref}}\)</span> çš„å¯¹æ•°æ¦‚ç‡æ¯”ï¼Œå†ä¹˜ä»¥
<span class="math inline">\(\beta\)</span>ã€‚è¿™ä¸ª <span
class="math inline">\(\hat{r}_{\theta}(x, y)\)</span> è¢«ç§°ä¸ºç»éªŒå¥–åŠ±
(empirical reward)ï¼Œå®ƒæ˜¯ç­–ç•¥ <span
class="math inline">\(\pi_{\theta}\)</span> åœ¨å½“å‰å‚æ•°ä¸‹å¯¹ <span
class="math inline">\(y\)</span> çš„å¥–åŠ±ä¼°è®¡ã€‚</p>
<h4 id="c.-æƒé‡é¡¹-weight-term">C. æƒé‡é¡¹ (Weight Term)</h4>
<p><span class="math display">\[\sigma(\hat{r}_{\theta}(x, y_l) -
\hat{r}_{\theta}(x, y_w))\]</span> * å¦‚æœå½“å‰ç­–ç•¥ <span
class="math inline">\(\pi_{\theta}\)</span> å·²ç»æ­£ç¡®åœ°æ•æ‰äº†åå¥½ï¼ˆå³
<span class="math inline">\(\hat{r}_{\theta}(x, y_w) &gt;
\hat{r}_{\theta}(x, y_l)\)</span>ï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸ªå¥–åŠ±å·®å¼‚ <span
class="math inline">\((\hat{r}_{\theta}(x, y_l) - \hat{r}_{\theta}(x,
y_w))\)</span> æ˜¯ä¸€ä¸ªè¾ƒå¤§çš„è´Ÿæ•°ã€‚Sigmoid å‡½æ•° <span
class="math inline">\(\sigma(\text{å¤§è´Ÿæ•°})\)</span> æ¥è¿‘
0ï¼Œå› æ­¤æƒé‡å¾ˆå°ã€‚ * å¦‚æœå½“å‰ç­–ç•¥ <span
class="math inline">\(\pi_{\theta}\)</span> é”™è¯¯åœ°æ•æ‰äº†åå¥½ï¼ˆå³ <span
class="math inline">\(\hat{r}_{\theta}(x, y_w) &lt; \hat{r}_{\theta}(x,
y_l)\)</span>ï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸ªå¥–åŠ±å·®å¼‚æ˜¯ä¸€ä¸ªæ­£æ•°ã€‚Sigmoid å‡½æ•° <span
class="math inline">\(\sigma(\text{æ­£æ•°})\)</span> æ¥è¿‘ 1 æˆ– 0.5
ä»¥ä¸Šï¼Œå› æ­¤æƒé‡å¾ˆå¤§ã€‚</p>
<h3 id="è¿›ä¸€æ­¥çš„ç›´è§‰æ­£åˆ™åŒ–-regularization-intuition">âœ¨
è¿›ä¸€æ­¥çš„ç›´è§‰ï¼šæ­£åˆ™åŒ– (Regularization Intuition)</h3>
<p><span class="math inline">\(\beta [\log \pi_{\theta}(y_l|x) - \log
\pi_{\theta}(y_w|x)] - \beta [\log \pi_{\text{ref}}(y_l|x) - \log
\pi_{\text{ref}}(y_w|x)]\)</span></p>
<p>è¿™ä¸ªè¡¨è¾¾å¼å®é™…ä¸Šæ˜¯ï¼š <span class="math display">\[\underbrace{\beta
(\log \frac{\pi_{\theta}(y_l|x)}{\pi_{\theta}(y_w|x)})}_{\text{ç­–ç•¥ }
\pi_{\theta} \text{çš„ log-prob å·®å¼‚}} - \underbrace{\beta (\log
\frac{\pi_{\text{ref}}(y_l|x)}{\pi_{\text{ref}}(y_w|x)})}_{\text{å‚è€ƒ }
\pi_{\text{ref}} \text{çš„ log-prob å·®å¼‚}}\]</span></p>
<ul>
<li>DPO ç›®æ ‡ï¼š DPO æŸå¤±çš„ç›®æ ‡æ˜¯è®©ç­–ç•¥ <span
class="math inline">\(\pi_{\theta}\)</span> å¯¹ <span
class="math inline">\(y_w\)</span> å’Œ <span
class="math inline">\(y_l\)</span> çš„å¯¹æ•°æ¦‚ç‡å·®å¼‚ (<span
class="math inline">\(\log \pi_{\theta}(y_w|x) - \log
\pi_{\theta}(y_l|x)\)</span>) å¢å¤§ï¼ˆå³è®© <span
class="math inline">\(y_w\)</span> æ¯” <span
class="math inline">\(y_l\)</span> æ›´å¯èƒ½å‡ºç°ï¼‰ã€‚</li>
<li>æ­£åˆ™åŒ–ä½œç”¨ï¼š DPO æŸå¤±è¦æ±‚ <span
class="math inline">\(\pi_{\theta}\)</span>
çš„è¿™ä¸ªå·®å¼‚ä¸ä»…è¦å¤§ï¼Œè€Œä¸”è¦ç›¸å¯¹äºå‚è€ƒæ¨¡å‹ <span
class="math inline">\(\pi_{\text{ref}}\)</span>
çš„åŸå§‹å·®å¼‚è¿›è¡Œè°ƒæ•´ã€‚</li>
<li>ç»“è®ºï¼š DPO å®é™…ä¸Šæ˜¯åœ¨æ­£åˆ™åŒ–ç­–ç•¥ <span
class="math inline">\(\pi_{\theta}\)</span>
çš„å¯¹æ•°æ¦‚ç‡å·®å¼‚ï¼Œä½¿å…¶åœ¨æ‹Ÿåˆäººç±»åå¥½çš„åŒæ—¶ï¼Œä¸ä¼šè¿‡åº¦åç¦»åŸºç¡€æ¨¡å‹ <span
class="math inline">\(\pi_{\text{ref}}\)</span> çš„è¡Œä¸ºã€‚è¿™ä¸ RLHF-PPO
ä¸­ä½¿ç”¨ KL æ•£åº¦è¿›è¡Œæ­£åˆ™åŒ–æœ‰ç›¸ä¼¼çš„ç›®çš„ï¼Œä½† DPO
å°†è¿™ä¸ªæ­£åˆ™åŒ–ç›´æ¥åµŒå…¥åˆ°äº†æŸå¤±å‡½æ•°çš„å®šä¹‰ä¸­ã€‚</li>
</ul>
<p>DPO é€šè¿‡ç»•è¿‡ å¥–åŠ±æ¨¡å‹ (Reward Model) å’Œ å¼ºåŒ–å­¦ä¹  (RL)
æ­¥éª¤ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´ç®€å•ã€æ›´ç¨³å®šã€‚ RLHF-PPO
ç”±äºå¼•å…¥äº†å¤æ‚çš„å¼ºåŒ–å­¦ä¹ å’Œå¥–åŠ±æ¨¡å‹ï¼Œè¢«è®¤ä¸ºæœ‰æ›´å¤§çš„æ½œåŠ›ï¼ˆå¯èƒ½åœ¨å¤æ‚çš„å¯¹é½ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼‰ã€‚</p>
<h1 id="mixture-of-experts-model">Mixture of Experts Model</h1>
<p>Idea of conditional computation: We still build a super big model,
but we only selectively activate a relevant portion for each training
sample.</p>
<p>MoE is natural for model parallel</p>
<p><span class="math display">\[
G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k))
\]</span></p>
<p><span class="math display">\[
\text{KeepTopK}(v, k)_i = \begin{cases} v_i &amp; \text{if } v_i \text{
is in the top } k \text{ elements of } v \\ -\infty &amp;
\text{otherwise} \end{cases}
\]</span></p>
<p><span class="math display">\[
H(x)_i = (x \cdot W_{g})_i + \text{StandardNormal}() \cdot
\text{Softplus}((x \cdot W_{\text{noise}})_i)
\]</span></p>
<ul>
<li>æ ¸å¿ƒçº¿æ€§å˜æ¢ï¼š <span class="math inline">\((x \cdot
W_{g})_i\)</span> æ˜¯å¯¹è¾“å…¥ <span class="math inline">\(x\)</span>
åº”ç”¨æƒé‡çŸ©é˜µ <span class="math inline">\(W_{g}\)</span> åçš„ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªå…ƒç´ ã€‚è¿™æ˜¯è®¡ç®—ä¸“å®¶ <span
class="math inline">\(i\)</span> åˆå§‹å¾—åˆ†çš„åŸºç¡€ã€‚</li>
<li>å™ªå£°é¡¹ï¼š é˜²æ­¢ä¸“å®¶è¿‡åº¦ä¸“ä¸šåŒ– (Over-Specialization) æˆ–åå¡Œ
(Collapse):é—¨æ§ç½‘ç»œ <span class="math inline">\(G(x)\)</span>
å€¾å‘äºå°†ç›¸ä¼¼çš„è¾“å…¥æŒç»­è·¯ç”±åˆ°å¾—åˆ†æœ€é«˜çš„å°‘æ•°ä¸“å®¶ã€‚è¿™ä¼šå¯¼è‡´è¿™äº›å°‘æ•°ä¸“å®¶è¢«è¿‡åº¦ä½¿ç”¨
(over-utilized)ï¼Œè€Œå…¶ä»–å¤§å¤šæ•°ä¸“å®¶åˆ™åˆ©ç”¨ä¸è¶³
(under-utilized)ï¼Œå‚æ•°æ›´æ–°å°‘ï¼Œå½¢åŒè™šè®¾ã€‚è¿™ç§ä¸å¹³è¡¡ä¹Ÿè¢«ç§°ä¸ºâ€œè·¯ç”±åå¡Œâ€
(Routing Collapse)ã€‚</li>
</ul>
<h2 id="balancing-loads-between-experts">Balancing loads between
experts</h2>
<p><span class="math display">\[\text{Importance}(X) = \sum_{x \in X}
G(x) \quad \text{(6)}\]</span></p>
<ul>
<li><span class="math inline">\(G(x)\)</span> æ˜¯æŒ‡é—¨æ§å€¼ï¼ˆgate
valueï¼‰ã€‚åœ¨MoEæ¨¡å‹ä¸­ï¼Œé—¨æ§ç½‘ç»œä¼šä¸ºæ¯ä¸ªè¾“å…¥ <span
class="math inline">\(x\)</span>
è¾“å‡ºä¸€ä¸ªåˆ†å¸ƒï¼Œå†³å®šå°†è¯¥è¾“å…¥åˆ†é…ç»™å“ªä¸ªä¸“å®¶ã€‚</li>
<li><span class="math inline">\(\text{Importance}(X)\)</span>
æ˜¯è¯¥ä¸“å®¶åœ¨æ•´ä¸ªæ‰¹æ¬¡ <span class="math inline">\(X\)</span>
ä¸­æ‰€æœ‰æ ·æœ¬é—¨æ§å€¼ï¼ˆä½¿ç”¨æ¦‚ç‡ï¼‰çš„æ€»å’Œã€‚è¿™ä¸ªå€¼è¡¡é‡äº†è¯¥ä¸“å®¶åœ¨å½“å‰æ‰¹æ¬¡ä¸­è¢«æ¿€æ´»å’Œä½¿ç”¨çš„ç¨‹åº¦ã€‚</li>
</ul>
<p>è´Ÿè½½å‡è¡¡æŸå¤± <span
class="math inline">\(\mathcal{L}_{\text{importance}}\)</span>
æ˜¯æ ¹æ®æ‰€æœ‰ä¸“å®¶çš„é‡è¦æ€§å€¼é›†åˆè®¡ç®—å¾—å‡ºçš„ï¼š</p>
<p><span class="math display">\[\mathcal{L}_{\text{importance}}(X) =
w_{\text{importance}} \cdot \text{CV}(\text{Importance}(X))^2 \quad
\text{(7)}\]</span></p>
<ul>
<li><span class="math inline">\(\text{Importance}(X)\)</span>
æ­¤æ—¶æ˜¯ä¸€ä¸ªå‘é‡ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¸“å®¶å„è‡ªçš„ <span
class="math inline">\(\text{Importance}\)</span> å€¼ã€‚</li>
<li><span class="math inline">\(\text{CV}(\cdot)\)</span>
æ˜¯å˜å¼‚ç³»æ•°ï¼ˆCoefficient of Variationï¼‰ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š <span
class="math display">\[\text{CV} = \text{std}/\text{mean}\]</span>
å³ï¼š<span class="math inline">\(\text{CV} = \text{æ ‡å‡†å·®} /
\text{å‡å€¼}\)</span>ã€‚</li>
<li><span class="math inline">\(w_{\text{importance}}\)</span>
æ˜¯ä¸€ä¸ªæ‰‹åŠ¨è°ƒæ•´çš„ç¼©æ”¾å› å­ï¼ˆscaling
factorï¼‰ï¼Œç”¨äºæ§åˆ¶æ­¤æŸå¤±åœ¨æ€»æŸå¤±ä¸­çš„æƒé‡å’Œå½±å“åŠ›ã€‚</li>
</ul>
<p>æœ€å°åŒ– <span
class="math inline">\(\mathcal{L}_{\text{importance}}\)</span>
æ„å‘³ç€æœ€å°åŒ– <span
class="math inline">\(\text{CV}(\text{Importance}(X))\)</span>ã€‚ç”±äº
<span class="math inline">\(CV\)</span>
è¶Šå°è¡¨ç¤ºæ•°æ®è¶Šé›†ä¸­ï¼Œå› æ­¤è¿™é¼“åŠ±æ‰€æœ‰ä¸“å®¶å…·æœ‰è¿‘ä¼¼ç›¸ç­‰çš„â€œé‡è¦æ€§â€ï¼Œä»è€Œå®ç°äº†ä¸“å®¶é—´çš„å‡è¡¡è´Ÿè½½ã€‚</p>
<h2 id="æ›´ç®€å•çš„è´Ÿè½½å‡è¡¡æŸå¤±è§£æ">æ›´ç®€å•çš„è´Ÿè½½å‡è¡¡æŸå¤±â€è§£æ</h2>
<p><span class="math display">\[\text{loss} = \alpha \cdot
\sum_{i=1}^{N} f_i \cdot P_i \quad \text{(4)}\]</span></p>
<p>åœ¨è¿™é‡Œï¼Œ<span class="math inline">\(p\)</span> æŒ‡çš„æ˜¯
è·¯ç”±å™¨æ¦‚ç‡ï¼ˆRouter Probabilityï¼‰ï¼Œä¹Ÿç§°ä¸º é—¨æ§æ¦‚ç‡ï¼ˆGate
Probabilityï¼‰ã€‚</p>
<ul>
<li><span class="math inline">\(p_i(x)\)</span>ï¼š
æ˜¯æŒ‡é—¨æ§ç½‘ç»œï¼ˆRouterï¼‰å¯¹è¾“å…¥ <span class="math inline">\(x\)</span>
è®¡ç®—å¾—åˆ°çš„ã€å°†å…¶åˆ†æ´¾ç»™ç¬¬ <span class="math inline">\(i\)</span>
ä¸ªä¸“å®¶çš„æ¦‚ç‡ã€‚
<ul>
<li>åœ¨MoEæ¨¡å‹ä¸­ï¼Œé—¨æ§ç½‘ç»œé€šå¸¸ä¼šè¾“å‡ºä¸€ä¸ª <span
class="math inline">\(N\)</span> ç»´çš„æ¦‚ç‡å‘é‡ <span
class="math inline">\(p(x)\)</span>ï¼Œå…¶ä¸­ <span
class="math inline">\(N\)</span> æ˜¯ä¸“å®¶æ•°é‡ï¼Œ<span
class="math inline">\(\sum_{i=1}^{N} p_i(x) = 1\)</span>ã€‚</li>
</ul></li>
<li><span class="math inline">\(P_i\)</span> (å…¬å¼ 6)ï¼š æ˜¯æŒ‡åœ¨æ•´ä¸ªæ‰¹æ¬¡
<span class="math inline">\(B\)</span> ä¸­ï¼Œåˆ†é…ç»™ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªä¸“å®¶çš„æ¦‚ç‡çš„å¹³å‡å€¼ã€‚ <span
class="math display">\[P_i = \frac{1}{T} \sum_{x \in B} p_i(x) \quad
\]</span></li>
</ul>
<p><span class="math inline">\(f_i\)</span> è¡¨ç¤ºåœ¨å½“å‰æ‰¹æ¬¡ <span
class="math inline">\(B\)</span> ä¸­å®é™…è¢«åˆ†æ´¾ï¼ˆhard-routedï¼‰ç»™ä¸“å®¶ <span
class="math inline">\(i\)</span> çš„ tokens çš„æ¯”ä¾‹ï¼ˆActual Usageï¼‰ï¼š</p>
<p><span class="math display">\[f_i = \frac{1}{T} \sum_{x \in B}
\mathbb{I}\{\operatorname{argmax} p(x) = i\} \quad \]</span></p>
<ul>
<li><span class="math inline">\(\operatorname{argmax} p(x) = i\)</span>
è¡¨ç¤º <span class="math inline">\(p_i(x)\)</span>
æ˜¯æ‰€æœ‰ä¸“å®¶ä¸­æ¦‚ç‡æœ€å¤§çš„ï¼Œå³æ ·æœ¬ <span class="math inline">\(x\)</span>
æœ€ç»ˆè¢«ç¡®å®šåˆ†æ´¾ç»™äº†ä¸“å®¶ <span class="math inline">\(i\)</span>ã€‚</li>
<li><span class="math inline">\(f_i\)</span> è¡¡é‡çš„æ˜¯ä¸“å®¶ <span
class="math inline">\(i\)</span> åœ¨å½“å‰æ‰¹æ¬¡ä¸­
â€œç¡¬æ€§â€å¤„ç†çš„å®é™…å·¥ä½œé‡ã€‚</li>
</ul>
<p>æƒ©ç½šå°‘æ•°ä¸“å®¶è¢«è¿‡åº¦ä½¿ç”¨</p>
<table>

<thead>
<tr>
<th style="text-align: left;">ç‰¹æ€§</th>
<th style="text-align: left;"><span
class="math inline">\(f_i\)</span>ï¼ˆå®é™…ä½¿ç”¨ç‡ - Actual Usageï¼‰</th>
<th style="text-align: left;"><span
class="math inline">\(P_i\)</span>ï¼ˆæ¦‚ç‡åˆ†é…ç‡ - Allocated
Probabilityï¼‰</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">å…¬å¼</td>
<td style="text-align: left;"><span class="math display">\[f_i =
\frac{1}{T} \sum_{x \in B} \mathbb{I}\{\operatorname{argmax} p(x) =
i\}\]</span></td>
<td style="text-align: left;"><span class="math display">\[P_i =
\frac{1}{T} \sum_{x \in B} p_i(x)\]</span></td>
</tr>
<tr>
<td style="text-align: left;">å«ä¹‰</td>
<td style="text-align: left;">ç¡¬æ€§é€‰æ‹©ç»“æœã€‚å®é™…è¢«åˆ†æ´¾ç»™ä¸“å®¶ <span
class="math inline">\(i\)</span> çš„ tokens å æ€» tokens æ•° <span
class="math inline">\(T\)</span> çš„æ¯”ä¾‹ã€‚</td>
<td style="text-align: left;">è½¯æ€§æ¦‚ç‡å‡å€¼ã€‚é—¨æ§ç½‘ç»œåˆ†é…ç»™ä¸“å®¶ <span
class="math inline">\(i\)</span> çš„æ¦‚ç‡ <span
class="math inline">\(p_i(x)\)</span> åœ¨æ•´ä¸ªæ‰¹æ¬¡ <span
class="math inline">\(B\)</span> ä¸­çš„å¹³å‡å€¼ã€‚</td>
</tr>
<tr>
<td style="text-align: left;">è®¡ç®—åŸºå‡†</td>
<td style="text-align: left;">åŸºäº <span
class="math inline">\(\operatorname{argmax}\)</span>
è¿ç®—ï¼šåªå…³å¿ƒå“ªä¸ªä¸“å®¶è·å¾—äº†æœ€é«˜çš„æ¦‚ç‡ï¼Œç»“æœæ˜¯ 0 æˆ– 1ï¼ˆæŒ‡ç¤ºå‡½æ•° <span
class="math inline">\(\mathbb{I}\)</span>ï¼‰ã€‚</td>
<td style="text-align: left;">åŸºäº è½¯æ¦‚ç‡ <span
class="math inline">\(p_i(x)\)</span>
çš„æ±‚å’Œï¼šè€ƒè™‘äº†é—¨æ§ç½‘ç»œå¯¹æ‰€æœ‰ä¸“å®¶çš„æ¦‚ç‡åˆ†é…å¤§å°ã€‚</td>
</tr>
<tr>
<td style="text-align: left;">ä»£è¡¨æ€§</td>
<td style="text-align: left;">è¡¡é‡ä¸“å®¶ <span
class="math inline">\(i\)</span> å®é™…å¤„ç†çš„å·¥ä½œé‡ã€‚</td>
<td style="text-align: left;">è¡¡é‡ä¸“å®¶ <span
class="math inline">\(i\)</span> é¢„æœŸè¢«ä½¿ç”¨çš„å¹³å‡æ¦‚ç‡æƒé‡ã€‚</td>
</tr>
</tbody>
</table>
<h2 id="fine-grained-expert-segmentation">Fine-Grained Expert
Segmentation</h2>
<p>while maintaining the number of parameters constant, we segment the
experts into a finer grain by splitting the FFN intermediate hidden
dimension.</p>
<h2 id="shared-expert-isolation">Shared Expert Isolation</h2>
<p>we isolate certain experts to serve as shared experts that are always
activated, aiming at capturing and consolidating common knowledge across
varying contexts. ï¼ˆä¸€éƒ¨åˆ†æ˜¯ä¸“å®¶foré€šç”¨çŸ¥è¯†ï¼Œä¸€ç›´è¢«æ¿€æ´»ï¼‰</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/NLP/" class="print-no-link">#NLP</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>ç ´æ™“ä¹‹åˆ»ï¼šTransformerçš„è¯ç”Ÿä¸è‡ªç„¶è¯­è¨€å¤„ç†å‰æ²¿</div>
      <div>http://example.com/2025/12/12/nlp3/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>ç‘¾ç‘œç•¶å¹´</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2025å¹´12æœˆ12æ—¥</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2026/02/24/post-2/" title="Diffusion LMåˆæ¢ï¼šLLaDA">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Diffusion LMåˆæ¢ï¼šLLaDA</span>
                        <span class="visible-mobile">ä¸Šä¸€ç¯‡</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/12/12/nlp2/" title="é»æ˜ä¹‹å‰ï¼šTransformerä¹‹å‰çš„è‡ªç„¶è¯­è¨€å¤„ç†">
                        <span class="hidden-mobile">é»æ˜ä¹‹å‰ï¼šTransformerä¹‹å‰çš„è‡ªç„¶è¯­è¨€å¤„ç†</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="twikoo"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/twikoo/1.6.39/twikoo.all.min.js', function() {
        var options = Object.assign(
          {"envId":"https://twikoo-haru.netlify.app/.netlify/functions/twikoo","region":"ap-shanghai","path":"window.location.pathname","enable":true},
          {
            el: '#twikoo',
            path: 'window.location.pathname',
            onCommentLoaded: function() {
              Fluid.utils.listenDOMLoaded(function() {
                var imgSelector = '#twikoo .tk-content img:not(.tk-owo-emotion)';
                Fluid.plugins.imageCaption(imgSelector);
                Fluid.plugins.fancyBox(imgSelector);
              });
            }
          }
        )
        twikoo.init(options)
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>ç›®å½•</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡ 
        <span id="busuanzi_value_site_pv"></span>
         æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•° 
        <span id="busuanzi_value_site_uv"></span>
         äºº
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
<!-- hexo injector body_end start --><script data-pjax src="https://unpkg.com/oh-my-live2d"></script><script>const oml2d = OML2D.loadOml2d({dockedPosition:"left",mobileDisplay:true,models:[{"path":"https://model.hacxy.cn/HK416-1-normal/model.json","scale":0.1,"motionPreloadStrategy":"ALL","position":[-10,60],"stageStyle":{"height":520},"mobileScale":0.06,"mobilePosition":[10,30],"mobileStageStyle":{"height":250}}],parentElement:document.body,primaryColor:"#336699",sayHello:false,tips:{style: {"width":230,"height":120,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":180,"height":80,"left":"calc(50% - 30px)","top":"-100px"},idleTips:{interval:15000,message:function(){
  return axios.get('https://v1.hitokoto.cn?c=i')
    .then(function (response) {
      return response.data.hitokoto ;
    })
    .catch(function (error) {
      console.error(error);
    });
}
}}});</script><!-- hexo injector body_end end --></body>
</html>
