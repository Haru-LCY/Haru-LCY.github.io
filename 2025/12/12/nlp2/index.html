

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/kazuki.jpg">
  <link rel="icon" href="/img/kazuki.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="ç‘¾ç‘œç•¶å¹´">
  <meta name="keywords" content="">
  
    <meta name="description" content="Review KLD \[D_{KL}(p(x) || q(x)) &#x3D; \int_{-\infty}^{\infty} p(x) \ln \frac{p(x)}{q(x)} dx\] \[D_{KL}(p(x) || q(x)) &#x3D; \sum_{x \in X} p(x) \ln \frac{p(x)}{q(x)}\] KLD is non-symmetric è¿™æ˜¯ä¸€ä¸ªå…³äº KL æ•£åº¦ï¼ˆKullb">
<meta property="og:type" content="article">
<meta property="og:title" content="é»æ˜ä¹‹å‰ï¼šTransformerä¹‹å‰çš„è‡ªç„¶è¯­è¨€å¤„ç†">
<meta property="og:url" content="http://example.com/2025/12/12/nlp2/index.html">
<meta property="og:site_name" content="æ¸¯æ¹¾">
<meta property="og:description" content="Review KLD \[D_{KL}(p(x) || q(x)) &#x3D; \int_{-\infty}^{\infty} p(x) \ln \frac{p(x)}{q(x)} dx\] \[D_{KL}(p(x) || q(x)) &#x3D; \sum_{x \in X} p(x) \ln \frac{p(x)}{q(x)}\] KLD is non-symmetric è¿™æ˜¯ä¸€ä¸ªå…³äº KL æ•£åº¦ï¼ˆKullb">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=0.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=nnlm.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=class.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=rnn.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=rnn0.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=rnnp.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=ar.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=grnn.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=resnet.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=birnn.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=sembed.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=att.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=b.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=tf.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=head.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=win.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=vae.png">
<meta property="og:image" content="http://example.com/img/nlp2_part1/nlp=re.png">
<meta property="article:published_time" content="2025-12-12T14:51:02.000Z">
<meta property="article:modified_time" content="2026-02-24T15:12:58.719Z">
<meta property="article:author" content="ç‘¾ç‘œç•¶å¹´">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/nlp2_part1/nlp=0.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>é»æ˜ä¹‹å‰ï¼šTransformerä¹‹å‰çš„è‡ªç„¶è¯­è¨€å¤„ç† - æ¸¯æ¹¾</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>è§€ç€¾</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>é¦–é¡µ</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>å½’æ¡£</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>æ ‡ç­¾</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>å…³äº</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>å‹é“¾</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/sea.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="é»æ˜ä¹‹å‰ï¼šTransformerä¹‹å‰çš„è‡ªç„¶è¯­è¨€å¤„ç†"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-12-12 22:51" pubdate>
          2025å¹´12æœˆ12æ—¥ æ™šä¸Š
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.9k å­—
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          50 åˆ†é’Ÿ
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> æ¬¡
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">é»æ˜ä¹‹å‰ï¼šTransformerä¹‹å‰çš„è‡ªç„¶è¯­è¨€å¤„ç†</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    æœ¬æ–‡æœ€åæ›´æ–°äº 2026-02-24T23:12:58+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="review">Review</h1>
<h2 id="kld">KLD</h2>
<p><span class="math display">\[D_{KL}(p(x) || q(x)) =
\int_{-\infty}^{\infty} p(x) \ln \frac{p(x)}{q(x)} dx\]</span></p>
<p><span class="math display">\[D_{KL}(p(x) || q(x)) = \sum_{x \in X}
p(x) \ln \frac{p(x)}{q(x)}\]</span></p>
<p>KLD is non-symmetric</p>
<p>è¿™æ˜¯ä¸€ä¸ªå…³äº <strong>KL æ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰</strong>
æ ¸å¿ƒæ¦‚å¿µçš„éå¸¸å¥½çš„é—®é¢˜ã€‚ç†è§£è¿™ä¸¤ä¸ªæ–¹å‘çš„å·®å¼‚ï¼Œæ˜¯ç†è§£å˜åˆ†æ¨æ–­ï¼ˆVIï¼‰å’Œç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚
VAE å’Œ GANï¼‰çš„å…³é”®ã€‚</p>
<p><span class="math display">\[D_{KL}(P_D \| P_M)\]</span>å’Œ<span
class="math display">\[D_{KL}(P_M \| P_D)\]</span> éƒ½è¡¨ç¤ºæ¨¡å‹åˆ†å¸ƒ <span
class="math inline">\(P_M\)</span> æ¥è¿‘çœŸå®æ•°æ®åˆ†å¸ƒ <span
class="math inline">\(P_D\)</span>
çš„ç¨‹åº¦ï¼Œä½†å®ƒä»¬å¯¹â€œæ¥è¿‘â€çš„å®šä¹‰æ˜¯<strong>ä¸å¯¹ç§°</strong>çš„ï¼Œå› æ­¤åœ¨æœ€å°åŒ–æ—¶ä¼šäº§ç”Ÿæˆªç„¶ä¸åŒçš„ç»“æœã€‚</p>
<hr />
<h2 id="d_klp_d-p_m-çš„ç›´è§‰æ¨¡å¼è¦†ç›–-mode-covering">âš–ï¸ <span
class="math inline">\(D_{KL}(P_D \| P_M)\)</span>
çš„ç›´è§‰ï¼š<strong>æ¨¡å¼è¦†ç›– (Mode-Covering)</strong></h2>
<p><span class="math display">\[D_{KL}(P_D \| P_M) = \mathbb{E}_{x \sim
\mathbf{P_D}} \left[ \log \frac{P_D(x)}{P_M(x)} \right]\]</span></p>
<h3
id="æ ¸å¿ƒç›´è§‰å®³æ€•é”™è¿‡å¯¹-p_d-çš„é«˜å¯†åº¦åŒºæ•æ„Ÿ">æ ¸å¿ƒç›´è§‰ï¼š<strong>å®³æ€•é”™è¿‡ï¼ˆå¯¹
<span class="math inline">\(P_D\)</span> çš„é«˜å¯†åº¦åŒºæ•æ„Ÿï¼‰</strong></h3>
<p>è¿™ä¸ªè¡¨è¾¾å¼æ˜¯ä»¥<strong>çœŸå®æ•°æ®åˆ†å¸ƒ <span
class="math inline">\(P_D\)</span></strong> ä¸ºæƒé‡è®¡ç®—æœŸæœ›çš„ã€‚</p>
<ul>
<li><strong>æƒ©ç½šæœºåˆ¶ï¼š</strong> å¦‚æœ <span
class="math inline">\(P_D(x)\)</span>
çš„å€¼å¾ˆé«˜ï¼ˆæ¯”å¦‚åœ¨ä¸€ä¸ªçœŸå®æ•°æ®æ¨¡å¼çš„<strong>å³°å€¼</strong>å¤„ï¼‰ï¼Œä½†æ¨¡å‹
<span class="math inline">\(P_M(x)\)</span>
åœ¨æ­¤å¤„çš„å€¼å¾ˆä½ï¼ˆæ¨¡å‹<strong>é”™è¿‡</strong>äº†è¿™ä¸ªæ¨¡å¼ï¼‰ï¼Œé‚£ä¹ˆ <span
class="math inline">\(\log(P_D(x)/P_M(x))\)</span>
ä¼šæ˜¯ä¸€ä¸ªéå¸¸å¤§çš„æ­£æ•°ã€‚ç”±äº <span class="math inline">\(P_D(x)\)</span>
æƒé‡ä¹Ÿå¾ˆå¤§ï¼Œè¿™ä¸ªé¡¹çš„è´¡çŒ®ä¼šè®© <span class="math inline">\(D_{KL}(P_D \|
P_M)\)</span> <strong>æ€¥å‰§å¢å¤§</strong>ã€‚</li>
<li><strong>ä¼˜åŒ–ç»“æœï¼ˆæœ€å°åŒ–ï¼‰ï¼š</strong> ä¸ºäº†é¿å…è¿™ç§å·¨å¤§çš„æƒ©ç½šï¼Œæ¨¡å‹
<span class="math inline">\(P_M\)</span> å¿…é¡»ç¡®ä¿åœ¨ <strong><span
class="math inline">\(P_D\)</span> æ‰€æœ‰å¯†åº¦é«˜çš„åŒºåŸŸï¼ˆæ‰€æœ‰æ¨¡å¼ï¼‰</strong>
éƒ½åˆ†é…æ¦‚ç‡ã€‚</li>
<li><strong>æ‹Ÿåˆç‰¹æ€§ï¼š</strong> <strong>æ¨¡å¼è¦†ç›–
(Mode-Covering)</strong>ã€‚æ¨¡å‹ä¼š<strong>æ‰©å¼ </strong>ï¼ŒåŠªåŠ›è¦†ç›– <span
class="math inline">\(P_D\)</span>
çš„æ‰€æœ‰æ¨¡å¼ï¼Œå³ä½¿è¿™æ„å‘³ç€å®ƒå¿…é¡»åœ¨æ¨¡å¼ä¹‹é—´çš„ä½å¯†åº¦åŒºä¹Ÿåˆ†é…ä¸€äº›æ¦‚ç‡ã€‚</li>
<li><strong>åº”ç”¨åœºæ™¯ï¼š</strong> <strong>æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)</strong> å’Œ
<strong>å˜åˆ†è‡ªç¼–ç å™¨ (VAE)</strong> çš„ä¸»è¦ä¼˜åŒ–æ–¹å‘ï¼ˆä¸ ELBO
ç›¸å…³ï¼‰ï¼Œå¼ºè°ƒ<strong>å¤šæ ·æ€§</strong>ã€‚</li>
</ul>
<blockquote>
<p>ğŸ’¡ <strong>ç±»æ¯”ï¼š</strong> <span class="math inline">\(P_M\)</span>
å°±åƒä¸€ä¸ª<strong>å¹¿æ’’ç½‘çš„æ•é±¼äºº</strong>ã€‚ä»–æœ€å®³æ€•çš„æ˜¯é”™è¿‡ä»»ä½•ä¸€ä¸ªé±¼ç¾¤ï¼ˆ<span
class="math inline">\(P_D\)</span>
çš„æ¨¡å¼ï¼‰ï¼Œæ‰€ä»¥ä»–ä¼šå°½åŠ›æ‰©å¤§ä»–çš„ç½‘ï¼ˆ<span
class="math inline">\(P_M\)</span>
çš„åˆ†å¸ƒèŒƒå›´ï¼‰ï¼Œå³ä½¿è¿™ä¼šæ•åˆ°ä¸€äº›æ— å…³çš„æ‚ç‰©ï¼ˆåœ¨ <span
class="math inline">\(P_D\)</span>
ä½å¯†åº¦åŒºåˆ†é…æ¦‚ç‡ï¼Œå¯¼è‡´ç”Ÿæˆæ ·æœ¬æ¨¡ç³Šï¼‰ã€‚</p>
</blockquote>
<hr />
<h2 id="d_klp_m-p_d-çš„ç›´è§‰æ¨¡å¼æœç´¢-mode-seeking">âš”ï¸ <span
class="math inline">\(D_{KL}(P_M \| P_D)\)</span>
çš„ç›´è§‰ï¼š<strong>æ¨¡å¼æœç´¢ (Mode-Seeking)</strong></h2>
<p><span class="math display">\[D_{KL}(P_M \| P_D) = \mathbb{E}_{x \sim
\mathbf{P_M}} \left[ \log \frac{P_M(x)}{P_D(x)} \right]\]</span></p>
<h3
id="æ ¸å¿ƒç›´è§‰å®³æ€•çŠ¯é”™å¯¹-p_d-çš„ä½å¯†åº¦åŒºæ•æ„Ÿ">æ ¸å¿ƒç›´è§‰ï¼š<strong>å®³æ€•çŠ¯é”™ï¼ˆå¯¹
<span class="math inline">\(P_D\)</span> çš„ä½å¯†åº¦åŒºæ•æ„Ÿï¼‰</strong></h3>
<p>è¿™ä¸ªè¡¨è¾¾å¼æ˜¯ä»¥<strong>æ¨¡å‹åˆ†å¸ƒ <span
class="math inline">\(P_M\)</span></strong> ä¸ºæƒé‡è®¡ç®—æœŸæœ›çš„ã€‚</p>
<ul>
<li><strong>æƒ©ç½šæœºåˆ¶ï¼š</strong> å¦‚æœæ¨¡å‹ <span
class="math inline">\(P_M(x)\)</span>
çš„å€¼å¾ˆé«˜ï¼ˆæ¨¡å‹è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå¯èƒ½ç”Ÿæˆçš„æ ·æœ¬ï¼‰ï¼Œä½†çœŸå®æ•°æ® <span
class="math inline">\(P_D(x)\)</span>
åœ¨æ­¤å¤„çš„å€¼å¾ˆä½ï¼ˆ<strong>å±±è°·</strong>æˆ–<strong>ç©ºéš™</strong>ï¼‰ï¼Œé‚£ä¹ˆ
<span class="math inline">\(\log(P_M(x)/P_D(x))\)</span>
ä¼šæ˜¯ä¸€ä¸ªéå¸¸å¤§çš„æ­£æ•°ã€‚ç”±äº <span class="math inline">\(P_M(x)\)</span>
æƒé‡ä¹Ÿå¾ˆå¤§ï¼Œè¿™ä¸ªé¡¹çš„è´¡çŒ®ä¼šè®© <span class="math inline">\(D_{KL}(P_M \|
P_D)\)</span> <strong>æ€¥å‰§å¢å¤§</strong>ã€‚</li>
<li><strong>ä¼˜åŒ–ç»“æœï¼ˆæœ€å°åŒ–ï¼‰ï¼š</strong> ä¸ºäº†é¿å…è¿™ç§æƒ©ç½šï¼Œæ¨¡å‹ <span
class="math inline">\(P_M\)</span> ä¼šè¢«å¼ºçƒˆè¿«ä½¿å°†è‡ªå·±çš„æ¦‚ç‡è´¨é‡é›†ä¸­åˆ°
<strong><span class="math inline">\(P_D\)</span>
å¯†åº¦é«˜çš„åŒºåŸŸ</strong>ã€‚å®ƒå®æ„¿å¿½ç•¥ä¸€äº›æ¨¡å¼ï¼Œä¹Ÿä¸æ„¿æ„åœ¨ <span
class="math inline">\(P_D\)</span> çš„ç¨€ç–åŒºåŸŸâ€œè¯´è°â€ï¼ˆåˆ†é…æ¦‚ç‡ï¼‰ã€‚</li>
<li><strong>æ‹Ÿåˆç‰¹æ€§ï¼š</strong> <strong>æ¨¡å¼æœç´¢
(Mode-Seeking)</strong>ã€‚æ¨¡å‹ä¼š<strong>æ”¶ç¼©</strong>ï¼Œé€‰æ‹©å¹¶èšç„¦äº <span
class="math inline">\(P_D\)</span> çš„ä¸€ä¸ªæˆ–å‡ ä¸ªæ¨¡å¼è¿›è¡Œæ‹Ÿåˆã€‚</li>
<li><strong>åº”ç”¨åœºæ™¯ï¼š</strong> <strong>å˜åˆ†æ¨æ–­ (VI)</strong>
ä¸­å¯¹è¿‘ä¼¼åéªŒåˆ†å¸ƒ <span class="math inline">\(q\)</span> çš„è¦æ±‚ï¼Œä»¥åŠ
<strong>GAN</strong> æ¨¡å‹çš„å†…åœ¨å€¾å‘ï¼Œå¼ºè°ƒ<strong>è´¨é‡</strong>ã€‚</li>
</ul>
<blockquote>
<p>ğŸ’¡ <strong>ç±»æ¯”ï¼š</strong> <span class="math inline">\(P_M\)</span>
å°±åƒä¸€ä¸ª<strong>ç²¾å‡†ç„å‡†çš„ç‹™å‡»æ‰‹</strong>ã€‚ä»–æœ€å®³æ€•çš„æ˜¯å°„åï¼ˆåœ¨ <span
class="math inline">\(P_D\)</span>
çš„ä½å¯†åº¦åŒºåˆ†é…æ¦‚ç‡ï¼‰ï¼Œæ‰€ä»¥ä»–åªä¼šç„å‡†ä»–èƒ½ç¡®ä¿å‘½ä¸­çš„<strong>ä¸€ä¸ªæˆ–å‡ ä¸ªæœ€æ¸…æ™°çš„ç›®æ ‡</strong>ï¼ˆ<span
class="math inline">\(P_D\)</span>
çš„é«˜å³°ï¼‰ã€‚è¿™å¯¼è‡´ä»–å¯èƒ½ä¼šé”™è¿‡å…¶ä»–æ¨¡å¼ï¼Œä½†ç¡®ä¿äº†æé«˜çš„ç²¾åº¦ã€‚</p>
</blockquote>
<h2 id="multi-class-classification">Multi-class classification</h2>
<p>Encode: x -&gt; d-dim vector h</p>
<p>Predict:</p>
<p>z=Wh + b (z is logits)</p>
<p>Use softmax to map z to probability P(y|x).</p>
<p>é˜²æ­¢softmax exlode: normal softmax åˆ†å­åˆ†æ¯åŒæ—¶é™¤ä»¥ <span
class="math inline">\(e^{max}\)</span></p>
<p>Traning:</p>
<p>use cross entropy loss <span class="math display">\[L_{\text{CE}} =
\sum_i -\log P(y = y_i | \mathbf{x}_i)\]</span></p>
<p>use SGD:</p>
<p>neural nets we donâ€™t have a closed-form optimal solution</p>
<p>minibatch is much faster and effective, also have some regularization
effect</p>
<h1 id="nn-basics">NN Basics</h1>
<p>MLP <img src="/img/nlp2_part1/nlp=0.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="bag-of-words">Bag of words</h2>
<p>A simple way to encode a sentence: a |V|-dim vector, the i-th
dimension indicates whether the i-th word in V(vocabulary) exists in
x</p>
<h2 id="hwx-embed">h=Wx embed</h2>
<p>æ³¨æ„ï¼šThe difference with LSA and word2vec is that here the word
embedding matrix is treated as part (the first layer) of the parameters
of the NN model. But indeed, you can use word2vec/LSA (trained on larger
data without label) to initialize this matrix.</p>
<p><span class="math display">\[\sigma&#39;(z) = \sigma(z) \cdot (1 -
\sigma(z))\]</span></p>
<p>A linear transform <span class="math inline">\(y=Wx\)</span>,
stacking linear transfroms is still a linear transform.</p>
<h2 id="back-propagation">Back-Propagation</h2>
<p>Chain Rule</p>
<p><span class="math display">\[z = Wh + b \]</span></p>
<p><span class="math display">\[ \frac{\partial z}{\partial h}=W
\]</span></p>
<p><span class="math display">\[ \frac{\partial z}{\partial W}=h^\top
\]</span></p>
<p><span class="math display">\[ \frac{\partial z}{\partial b}=I
\]</span></p>
<p>one hotï¼š åªæœ‰çœŸå®çš„labelä¸º1ï¼Œå…¶ä»–çš„å…¨0</p>
<p><span class="math display">\[\frac{\partial
\mathcal{L}_{\text{CE}}(\mathbf{y}|\mathbf{x})}{\partial \mathbf{z}} =
\frac{\partial (-\log(\text{softmax}(\mathbf{z}))[\mathbf{y}])}{\partial
\mathbf{z}} = \text{softmax}(\mathbf{z}) -
\mathbf{\tilde{y}}\]</span></p>
<p>proofï¼š <span class="math inline">\(\mathcal{L}(\mathbf{z},
\mathbf{\tilde{y}}) = -\sum_{j=1}^K \tilde{y}_j
\log(\hat{y}_j)\)</span></p>
<p><span class="math inline">\(\mathbf{\hat{y}} =
\text{softmax}(\mathbf{z})\)</span>ã€‚å…¶ä¸­ <span
class="math inline">\(\hat{y}_j = \frac{e^{z_j}}{\sum_{k=1}^K
e^{z_k}}\)</span>ã€‚ * <strong>çœŸå®æ ‡ç­¾ï¼š</strong> <span
class="math inline">\(\mathbf{\tilde{y}}\)</span>ï¼Œå‡è®¾çœŸå®æ ‡ç­¾æ˜¯ <span
class="math inline">\(c\)</span>ï¼Œåˆ™ <span
class="math inline">\(\tilde{y}_c = 1\)</span>ï¼Œå…¶ä½™ <span
class="math inline">\(\tilde{y}_j = 0\)</span>ã€‚ *
<strong>äº¤å‰ç†µæŸå¤±ï¼š</strong> <span
class="math inline">\(\mathcal{L}(\mathbf{z}, \mathbf{\tilde{y}}) =
-\sum_{j=1}^K \tilde{y}_j \log(\hat{y}_j)\)</span>ã€‚ * å› ä¸º <span
class="math inline">\(\mathbf{\tilde{y}}\)</span> æ˜¯ one-hot
å‘é‡ï¼Œæ‰€ä»¥ä¸Šå¼ç®€åŒ–ä¸ºï¼š<span class="math inline">\(\mathcal{L} =
-\log(\hat{y}_c) = -\log\left(\frac{e^{z_c}}{\sum_{k=1}^K
e^{z_k}}\right)\)</span>ã€‚</p>
<h4 id="i-æ˜¯æ­£ç¡®åˆ†ç±»-c-çš„ç´¢å¼•å³-ic"><span
class="math inline">\(i\)</span> æ˜¯æ­£ç¡®åˆ†ç±» <span
class="math inline">\(c\)</span> çš„ç´¢å¼•ï¼ˆå³ <span
class="math inline">\(i=c\)</span>ï¼‰</h4>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial
z_c} = \frac{\partial}{\partial z_c} \left[
-\log\left(\frac{e^{z_c}}{\sum_{k=1}^K e^{z_k}}\right)
\right]\]</span></p>
<p>åˆ©ç”¨å¯¹æ•°æ€§è´¨ <span class="math inline">\(\log(A/B) = \log A - \log
B\)</span>ï¼š <span class="math display">\[\mathcal{L} = -z_c +
\log\left(\sum_{k=1}^K e^{z_k}\right)\]</span></p>
<p>æ±‚å¯¼ï¼š <span class="math display">\[\frac{\partial
\mathcal{L}}{\partial z_c} = \frac{\partial}{\partial z_c}(-z_c) +
\frac{\partial}{\partial z_c}\left(\log\left(\sum_{k=1}^K
e^{z_k}\right)\right)\]</span> <span
class="math display">\[\frac{\partial \mathcal{L}}{\partial z_c} = -1 +
\underbrace{\frac{1}{\sum_{k=1}^K e^{z_k}}}_{\text{å¤–å±‚å¯¼æ•°}} \cdot
\underbrace{e^{z_c}}_{\text{å†…å±‚å¯¼æ•°}}\]</span></p>
<p>å›ä»£ Softmax çš„å®šä¹‰ <span class="math inline">\(\hat{y}_c =
\frac{e^{z_c}}{\sum_{k=1}^K e^{z_k}}\)</span>ï¼š <span
class="math display">\[\frac{\partial \mathcal{L}}{\partial z_c} = -1 +
\hat{y}_c\]</span></p>
<p>å› ä¸º <span class="math inline">\(i=c\)</span>ï¼Œæ‰€ä»¥ <span
class="math inline">\(\tilde{y}_i = 1\)</span>ã€‚ <span
class="math display">\[\frac{\partial \mathcal{L}}{\partial z_i} =
\hat{y}_i - 1 = \hat{y}_i - \tilde{y}_i\]</span></p>
<h4 id="i-ä¸æ˜¯æ­£ç¡®åˆ†ç±»çš„ç´¢å¼•å³-i-neq-c"><span
class="math inline">\(i\)</span> ä¸æ˜¯æ­£ç¡®åˆ†ç±»çš„ç´¢å¼•ï¼ˆå³ <span
class="math inline">\(i \neq c\)</span>ï¼‰</h4>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial
z_i} = \frac{\partial}{\partial z_i} \left[ -z_c +
\log\left(\sum_{k=1}^K e^{z_k}\right) \right]\]</span></p>
<p>æ±‚å¯¼ï¼ˆæ³¨æ„ <span class="math inline">\(-z_c\)</span> é¡¹å¯¹ <span
class="math inline">\(z_i\)</span> çš„å¯¼æ•°ä¸º 0ï¼‰ï¼š <span
class="math display">\[\frac{\partial \mathcal{L}}{\partial z_i} = 0 +
\underbrace{\frac{1}{\sum_{k=1}^K e^{z_k}}}_{\text{å¤–å±‚å¯¼æ•°}} \cdot
\underbrace{e^{z_i}}_{\text{å†…å±‚å¯¼æ•°}}\]</span></p>
<p>å›ä»£ Softmax çš„å®šä¹‰ <span class="math inline">\(\hat{y}_i =
\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}\)</span>ï¼š <span
class="math display">\[\frac{\partial \mathcal{L}}{\partial z_i} =
\hat{y}_i\]</span></p>
<p>å› ä¸º <span class="math inline">\(i \neq c\)</span>ï¼Œæ‰€ä»¥ <span
class="math inline">\(\tilde{y}_i = 0\)</span>ã€‚ <span
class="math display">\[\frac{\partial \mathcal{L}}{\partial z_i} =
\hat{y}_i - 0 = \hat{y}_i - \tilde{y}_i\]</span></p>
<h2 id="dropout">Dropout</h2>
<ul>
<li><p><strong>è®­ç»ƒçš„æ—¶å€™éšæœºå¤±æ´»ï¼š</strong> åœ¨å¤„ç†æ¯ä¸ª
<strong>mini-batch</strong>
æ•°æ®æ—¶ï¼Œç¥ç»ç½‘ç»œä¸­çš„<strong>æ¯ä¸ªç¥ç»å…ƒå•å…ƒï¼ˆUnitï¼‰</strong>ï¼ˆåŠå…¶æ‰€æœ‰ä¼ å…¥å’Œä¼ å‡ºçš„è¿æ¥ï¼‰éƒ½ä¼šä»¥ä¸€ä¸ªé¢„è®¾çš„æ¦‚ç‡
<span class="math inline">\(p\)</span>
è¢«éšæœºåœ°â€œä¸¢å¼ƒâ€æˆ–<strong>å¤±æ´»</strong>ã€‚</p></li>
<li><p>At test time, all units are present, but with weights scaled by p
(i.e. w becomes pw)</p></li>
</ul>
<p>parallel computing: For a minibatch of input, we can concat them into
a input matrix. Matrix-vector operation now becomes matrix-matrix
operation.</p>
<h1 id="nnlm">NNLM</h1>
<h2 id="feed-forward-neural-network">feed forward neural network</h2>
<p><img src="/img/nlp2_part1/nlp=nnlm.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>Note a big difference with the sentiment classifier is that the
output class number is now |V|, making the model slow.</p>
<h2 id="class-based-neural-network">class-based neural network</h2>
<p><img src="/img/nlp2_part1/nlp=class.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="rnn">RNN</h2>
<p><img src="/img/nlp2_part1/nlp=rnn.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/img/nlp2_part1/nlp=rnn0.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>better long distance dependenceï¼š hidden statesï¼Œ shared <span
class="math inline">\(W_ih\)</span> and <span
class="math inline">\(W_hh\)</span></p>
<p>Back propagation through time</p>
<p>problem: gradient explode/ vanish</p>
<p>æ ‡å‡†çš„ RNN éšè—çŠ¶æ€è®¡ç®—æ˜¯ï¼š<span class="math inline">\(h_t =
\text{activation}(W_{hh} h_{t-1} + W_{ih} x_t)\)</span>ã€‚</p>
<p>è¿™é‡Œç®€åŒ–ä¸ºï¼š<span class="math inline">\(h_t \approx W_{hh} h_{t-1} +
W_{ih} x_t\)</span>ã€‚</p>
<ul>
<li>ä» <span class="math inline">\(h_t\)</span> åˆ° <span
class="math inline">\(h_{t-1}\)</span> çš„æ¢¯åº¦ <span
class="math inline">\(\frac{\partial h_t}{\partial h_{t-1}} \approx
W_{hh}\)</span>ã€‚</li>
</ul>
<p><span class="math display">\[\frac{\partial L_t}{\partial h_1}
\approx W_{hh}^{T^{t-1}} \frac{\partial L_t}{\partial h_t}\]</span></p>
<p>t-1æ¬¡è¿ä¹˜å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±</p>
<p>Gradient exploding is more serious because it makes training
impossibleï¼Œå› ä¸ºæ¢¯åº¦æ— é™å¤§äº†</p>
<p>è§£å†³æ–¹æ¡ˆï¼šgradient clippingï¼Œset the maximum norm of gradient to be
<span class="math inline">\(\gamma\)</span>.</p>
<p>parallel of rnnï¼šparallel across scentence</p>
<p><img src="/img/nlp2_part1/nlp=rnnp.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>parallel traning: use padding to get same scentence length</p>
<p>you can also design a cnn to deal with variable seqlen</p>
<h3 id="autoregressive-lm">autoregressive LM</h3>
<p><img src="/img/nlp2_part1/nlp=ar.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h3 id="gated-recurrent-unit">Gated recurrent unit</h3>
<p><span class="math display">\[h_t = z_t \odot \hat{h}_t + (1 - z_t)
\odot h_{t-1}\]</span></p>
<p><img src="/img/nlp2_part1/nlp=grnn.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>resnet</p>
<p><img src="/img/nlp2_part1/nlp=resnet.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h3 id="bidirectional-rnn">Bidirectional RNN</h3>
<p>cannot be applied to AR-LM, because we cannot uitilize the
information from the future. æœ€ç»ˆçš„hidden
stateå¿…é¡»ç­‰åˆ°æ‰€æœ‰è®¡ç®—å…¨éƒ¨ç»“æŸä¹‹åæ‰èƒ½å®Œæˆã€‚</p>
<p><img src="/img/nlp2_part1/nlp=birnn.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h3 id="encoder-decoder">Encoder-decoder</h3>
<p><img src="/img/nlp2_part1/nlp=sembed.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>scentence-encoding:
å¥å­æœ€å¼€å§‹åŠ ä¸€ä¸ªç‰¹æ®Šçš„tokenï¼Œè¿™ä¸ªä¸»è¦åˆ©ç”¨çš„æ˜¯åå‘çš„h0</p>
<p>We can use a bi-rnn encoder for the input sequence, and use a uni-rnn
decoder for the output.</p>
<h3 id="attention">attention</h3>
<p><img src="/img/nlp2_part1/nlp=att.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>æ­¥éª¤ä¸€ï¼šè®¡ç®—å¯¹é½åˆ†æ•° (Alignment Score) <span
class="math display">\[\tilde{a}_i = (h_i^{enc})^T W_a
h_{t-1}^{dec}\]</span> æ­¥éª¤äºŒï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†å¸ƒ (Attention Distribution)
<span class="math display">\[a = \text{softmax}(\tilde{a})\]</span>
æ­¥éª¤ä¸‰ï¼šè®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ (Context Vector) <span class="math display">\[c_t
= \sum_{i} a_i h_i^{enc}\]</span></p>
<h3 id="greedy-decoding">greedy decoding</h3>
<p>autoregressiveï¼Œæ¯ä¸€æ¬¡éƒ½é€‰æ‹©å±€éƒ¨æœ€ä¼˜çš„ï¼Œæ‰€ä»¥ä¸ä¸€å®šæ˜¯å…¨å±€æœ€ä¼˜çš„</p>
<h3 id="beam-search-decoding">beam search decoding</h3>
<p><img src="/img/nlp2_part1/nlp=b.png" srcset="/img/loading.gif" lazyload alt=""></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-comment">// æ­¥éª¤ 1: åˆå§‹åŒ–</span><br><span class="hljs-comment">// åˆå§‹åŒ–é›†æŸ (beams)ã€‚é›†æŸæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå­˜å‚¨å½“å‰å¾—åˆ†æœ€é«˜çš„ B ä¸ªå€™é€‰åºåˆ—ã€‚</span><br><span class="hljs-comment">// æ ¼å¼ï¼š[(åºåˆ—, ç´¯ç§¯å¯¹æ•°æ¦‚ç‡å¾—åˆ†)]</span><br>beams = <span class="hljs-selector-attr">[[<span class="hljs-string">&quot;&lt;s&gt;&quot;</span>, 0]</span>] <br><span class="hljs-comment">// åˆå§‹åºåˆ—ä¸ºèµ·å§‹ç¬¦å· &quot;&lt;s&gt;&quot;ï¼Œç´¯ç§¯å¾—åˆ†ä¸º 0ã€‚</span><br><br><span class="hljs-comment">// æ­¥éª¤ 2: è¿­ä»£ç”Ÿæˆåºåˆ—</span><br><span class="hljs-comment">// L æ˜¯ç›®æ ‡åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚å¾ªç¯ä» t=1ï¼ˆç”Ÿæˆç¬¬ä¸€ä¸ªè¯ï¼‰åˆ° Lã€‚</span><br><span class="hljs-keyword">for</span> t from <span class="hljs-number">1</span> to L:<br>    <span class="hljs-comment">// åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„é›†æŸåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å½“å‰æ—¶é—´æ­¥ t æ‰©å±•åçš„æ‰€æœ‰æ–°åºåˆ—ã€‚</span><br>    new_b = <span class="hljs-selector-attr">[]</span> <br>    <br>    <span class="hljs-comment">// éå†å½“å‰é›†æŸä¸­çš„æ‰€æœ‰ B ä¸ªå€™é€‰åºåˆ—ã€‚</span><br>    <span class="hljs-keyword">for</span> <span class="hljs-selector-tag">b</span> <span class="hljs-keyword">in</span> beams:<br>        <span class="hljs-comment">// è§£æ„å½“å‰å€™é€‰åºåˆ—ï¼šhis æ˜¯å†å²åºåˆ—ï¼Œscore æ˜¯å®ƒçš„ç´¯ç§¯å¯¹æ•°æ¦‚ç‡å¾—åˆ†ã€‚</span><br>        his, score = <span class="hljs-selector-tag">b</span><span class="hljs-selector-attr">[0]</span>, <span class="hljs-selector-tag">b</span><span class="hljs-selector-attr">[1]</span><br>        <br>        <span class="hljs-comment">// ä½¿ç”¨è¯­è¨€æ¨¡å‹ (LM) é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚</span><br>        <span class="hljs-comment">// logprob å­˜å‚¨åŸºäº his å¾—åˆ°çš„è¯æ±‡è¡¨æ‰€æœ‰è¯çš„å¯¹æ•°æ¦‚ç‡ã€‚</span><br>        logprob = LM-NextPredict<span class="hljs-selector-attr">[his]</span> <br>        <br>        <span class="hljs-comment">// ä» logprob ä¸­é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ k=B ä¸ªè¯æ±‡åŠå…¶å¯¹æ•°æ¦‚ç‡ã€‚</span><br>        <span class="hljs-comment">// idx_s: è¿™ B ä¸ªè¯æ±‡åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚</span><br>        <span class="hljs-comment">// logprob_s: è¿™ B ä¸ªè¯æ±‡å¯¹åº”çš„å¯¹æ•°æ¦‚ç‡ã€‚</span><br>        idx_s, logprob_s = <span class="hljs-attribute">top</span><span class="hljs-built_in">-k</span>(logprob, k=B)<br>        <br>        <span class="hljs-comment">// æ­¥éª¤ 3: æ‰©å±•åºåˆ—å¹¶è®¡ç®—æ–°å¾—åˆ†</span><br>        <span class="hljs-comment">// éå†è¿™ B ä¸ªæœ€ä½³çš„ä¸‹ä¸€ä¸ªè¯æ±‡ã€‚</span><br>        <span class="hljs-keyword">for</span> j from <span class="hljs-number">0</span> to (B-<span class="hljs-number">1</span>):<br>            <span class="hljs-comment">// æ‰©å±•åºåˆ—ï¼šå°†æ–°çš„æœ€ä½³è¯ V[idx_s[j]] æ·»åŠ åˆ°å†å²åºåˆ— his åé¢ã€‚</span><br>            <span class="hljs-comment">// V æ˜¯è¯æ±‡è¡¨ (Vocabulary)ã€‚</span><br>            <span class="hljs-comment">// æ›´æ–°å¾—åˆ†ï¼šæ–°å¾—åˆ† = æ—§å¾—åˆ† + æ–°è¯æ±‡çš„å¯¹æ•°æ¦‚ç‡ (å› ä¸º log(P1*P2) = log(P1)+log(P2))ã€‚</span><br>            new_b<span class="hljs-selector-class">.append</span>((his + V<span class="hljs-selector-attr">[idx_s[j]</span>], score + logprob_s<span class="hljs-selector-attr">[j]</span>))<br>    <br>    <span class="hljs-comment">// æ­¥éª¤ 4: ç­›é€‰å’Œæ›´æ–°é›†æŸ</span><br>    <span class="hljs-comment">// å¯¹ new_b ä¸­çš„æ‰€æœ‰åºåˆ—ï¼ˆå½“å‰æœ‰ B*B ä¸ªï¼‰æŒ‰å¾—åˆ†ï¼ˆscoreï¼‰è¿›è¡Œé™åºæ’åºã€‚</span><br>    <span class="hljs-comment">// åªä¿ç•™å¾—åˆ†æœ€å¤§çš„ B ä¸ªåºåˆ—ã€‚</span><br>    sort and only keep B sequences <span class="hljs-keyword">in</span> new_b with largest score.<br>    <br>    <span class="hljs-comment">// ç”¨ç­›é€‰åçš„ B ä¸ªæœ€ä½³åºåˆ—æ›´æ–° beamsï¼Œè¿›å…¥ä¸‹ä¸€è½®è¿­ä»£ã€‚</span><br>    beams = new_b<br><br><span class="hljs-comment">// æ­¥éª¤ 5: è¿”å›ç»“æœ</span><br><span class="hljs-comment">// å¾ªç¯ç»“æŸåï¼Œbeams ä¸­å­˜å‚¨äº†é•¿åº¦ä¸º L çš„ B ä¸ªæœ€ä½³åºåˆ—ã€‚</span><br><span class="hljs-comment">// è¿”å›å¾—åˆ†æœ€é«˜ï¼ˆæ’åœ¨ç¬¬ä¸€ä½ï¼‰çš„åºåˆ—ã€‚</span><br>return beams<span class="hljs-selector-attr">[0]</span> <br></code></pre></td></tr></table></figure>
<p>å…³é”®ï¼›æ¯ä¸€æ¬¡ä¸€ä¸ªnodeéƒ½ä¼šç”Ÿæˆbä¸ªåˆ†æ”¯ï¼Œä½†æˆ‘ä»¬åªä¿ç•™å¾—åˆ†æœ€å¤§çš„bä¸ª</p>
<h3 id="belu-metric-for-machine-translation">belu metric for machine
translation</h3>
<p><span class="math display">\[
\text{precision}_n = \frac{\text{number of } n\text{-gram matches in
reference}}{\text{number of } n\text{-grams in predicted}}
\]</span></p>
<p>ç®€æ´æƒ©ç½šç”¨äº<strong>æƒ©ç½š</strong>é‚£äº›<strong>æ¯”å‚è€ƒè¯‘æ–‡çŸ­</strong>çš„æœºå™¨è¯‘æ–‡ï¼Œä»¥ç¡®ä¿ç¿»è¯‘çš„é•¿åº¦åˆç†ã€‚</p>
<p><span class="math display">\[
\text{brevity-penalty} = \min \left\{ 1, \exp \left( 1 -
\frac{|\text{reference}|}{|\text{predicted}|} \right) \right\}
\]</span></p>
<p>BLEU æœ€ç»ˆå¾—åˆ†æ˜¯<strong>ç®€æ´æƒ©ç½š</strong>å’Œ<strong>å‡ ä½•å¹³å‡ <span
class="math inline">\(n\)</span>-gram ç²¾å‡†åº¦</strong>çš„ä¹˜ç§¯ã€‚</p>
<p><span class="math display">\[
\text{BLEU} = \text{brevity-penalty} \times \left( \prod_{n=1}^{4}
\text{precision}_n \right)^{\frac{1}{4}}
\]</span></p>
<h3 id="creating-paired-data">creating paired data</h3>
<p>Given a decent amount of bilingual data (X, Y) and a good amount
monolingual data in target language Y.</p>
<p>â€¢ Q: What can you do to create more paired bilingual data?</p>
<p>â€¢ You can train a backward model Y-&gt;X, and conduct generation on
the monolingual data. Thatâ€™s called back translation.</p>
<h2 id="glue-benchmark">GLUE benchmark</h2>
<p>The General Language Understanding Evaluation , human level</p>
<p>SuperGLUE, harder</p>
<h2 id="elmo">ELMo</h2>
<p>ELMo (Embeddings from Language Models): build deep contextualized
word representation.</p>
<p>Model: multilayer bidirectional LSTM</p>
<p>Objective: predict the next word in both directions independently;
i.e., left-to-right and right-to-left</p>
<h1 id="subword-tokenization">subword tokenization</h1>
<p>â€¢ (1) Start with a unigram vocabulary of all characters in the
data.</p>
<p>â€¢ (2) Each iteration: In the data, find the most frequent pair, merge
it, and add to the vocabulary.</p>
<p>â€¢ (3) Stop when vocabulary is of pre-determined size (e.g., 50k).</p>
<h1 id="transformer">Transformer</h1>
<p><img src="/img/nlp2_part1/nlp=tf.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="self-attention">Self-attention</h2>
<p>q, k, v å‡ä¸º Wx</p>
<p>è®¡ç®—ç¬¬ <span class="math inline">\(i\)</span> ä¸ªä½ç½®çš„æŸ¥è¯¢å‘é‡ <span
class="math inline">\(\mathbf{q}_i\)</span> å¯¹æ‰€æœ‰é”®å‘é‡ <span
class="math inline">\(\mathbf{k}_*\)</span>ï¼ˆå…¶ä¸­ <span
class="math inline">\(*\)</span>
ä»£è¡¨å¥å­ä¸­çš„æ‰€æœ‰ä½ç½®ï¼‰çš„æ³¨æ„åŠ›æƒé‡çš„å…¬å¼å¦‚ä¸‹ï¼š</p>
<p><span class="math display">\[a_{i*} =
\mathbf{softmax}\left(\frac{\mathbf{q}_i^\top
\mathbf{k}_*}{\sqrt{\text{dim}_k}}\right)\]</span></p>
<p>divide <span class="math inline">\(\sqrt{\text{dim}_k}\)</span>,
normalize the variance, achieve more stable and smooth softmax
output.</p>
<p><span class="math inline">\(Z =
softmax(\frac{QK^\top}{\sqrt{dimk}})V\)</span> parallel computation</p>
<h2 id="layernorm">layernorm</h2>
<p><span class="math display">\[\mathbf{LayerNorm}(\mathbf{h}) = \alpha
\cdot \frac{\mathbf{h} -
\text{mean}(\mathbf{h})}{\text{std}(\mathbf{h})} + \beta\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNorm</span>(nn.Module):<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, features, eps=<span class="hljs-number">1e-6</span></span>):<br>        <span class="hljs-built_in">super</span>(LayerNorm, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.a_2 = nn.Parameter(torch.ones(features)) <span class="hljs-comment">#å‚æ•°</span><br>        <span class="hljs-variable language_">self</span>.b_2 = nn.Parameter(torch.zeros(features))<br>        <span class="hljs-variable language_">self</span>.eps = eps <span class="hljs-comment">#ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œé˜²æ­¢é™¤ä»¥0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x:(seqlen,feature)</span><br>        mean = x.mean(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        std = x.std(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.a_2 * (x - mean) / (std + <span class="hljs-variable language_">self</span>.eps) + <span class="hljs-variable language_">self</span>.b_2<br></code></pre></td></tr></table></figure>
<p>conbined with resnet: <span
class="math inline">\(h_{out}=F(layernorm(h))+h\)</span></p>
<p>batchnormå¾€å¾€éœ€è¦batchsizeæ¯”è¾ƒå¤§ï¼Œä½†å½“æˆ‘ä»¬è®­å¤§è¯­è¨€æ¨¡å‹çš„æ—¶å€™batchsizeå¯èƒ½è¾¾ä¸åˆ°é‚£ä¹ˆå¤§ã€‚è€Œä¸”qkvçš„ç»´åº¦dæ˜¯ç¡®å®šçš„ï¼Œä½†seqlenä¸ç¡®å®šï¼Œæ‰€ä»¥layernormæ›´åŠ é€‚åˆsequence
modelã€‚</p>
<h2 id="multihead">multihead</h2>
<p>æ¯ä¸€ä¸ªheadä¸€å¥—qkvçš„æƒé‡çŸ©é˜µï¼Œæœ€åconcatå†linear</p>
<p><img src="/img/nlp2_part1/nlp=head.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="positional-encoding">positional encoding</h2>
<p>Without RNN, attention alone does not have order information!</p>
<blockquote>
<p><strong>Exercise:</strong> Given any <span
class="math inline">\(\text{pos}\)</span> vector and a fixed number
<span class="math inline">\(k\)</span>, can you represent <span
class="math inline">\(\text{pos} + k\)</span> as a linear transform of
<span class="math inline">\(\text{pos}\)</span>?</p>
<p><strong>Hint:</strong> <span class="math inline">\(\sin(A+B) = \sin A
\cos B + \sin B \cos A\)</span></p>
</blockquote>
<p>è¿™é‡Œçš„ <span class="math inline">\(\text{pos}\)</span> æ˜¯æŒ‡ä½ç½® <span
class="math inline">\(pos\)</span> çš„ä½ç½®ç¼–ç å‘é‡ï¼Œè€Œ <span
class="math inline">\(\text{pos}+k\)</span> æ˜¯æŒ‡ä½ç½® <span
class="math inline">\(pos+k\)</span> çš„ä½ç½®ç¼–ç å‘é‡ã€‚</p>
<p>å‡è®¾ä½ç½®ç¼–ç å‘é‡çš„ç¬¬ <span class="math inline">\(i\)</span>
ä¸ªç»´åº¦åˆ†é‡å®šä¹‰å¦‚ä¸‹ï¼š <span class="math display">\[\text{PE}_{\text{pos},
i} = \sin(\omega_i \cdot \text{pos})\]</span></p>
<p>å…¶ä¸­ <span class="math inline">\(\omega_i\)</span> æ˜¯ä¸ç»´åº¦ <span
class="math inline">\(i\)</span> ç›¸å…³çš„å›ºå®šé¢‘ç‡é¡¹ã€‚</p>
<p>ç°åœ¨ï¼Œæˆ‘ä»¬æ¥çœ‹<strong>ä½ç½® <span
class="math inline">\(\text{pos}+k\)</span></strong> çš„ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªåˆ†é‡ï¼š <span
class="math display">\[\text{PE}_{\text{pos}+k, i} = \sin(\omega_i \cdot
(\text{pos} + k))\]</span></p>
<p>åˆ©ç”¨æç¤ºä¸­çš„<strong>ä¸‰è§’æ’ç­‰å¼</strong> <span
class="math inline">\(\sin(A+B) = \sin A \cos B + \sin B \cos
A\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(A = \omega_i \cdot
\text{pos}\)</span>ï¼Œ<span class="math inline">\(B = \omega_i \cdot
k\)</span>ï¼š</p>
<p><span class="math display">\[\text{PE}_{\text{pos}+k, i} =
\underbrace{\sin(\omega_i \cdot \text{pos})}_{\text{PE}_{\text{pos}, i}}
\cdot \underbrace{\cos(\omega_i \cdot k)}_{\text{å›ºå®šå¸¸é‡}} +
\underbrace{\cos(\omega_i \cdot \text{pos})}_{\text{PE}_{\text{pos},
i}^{\text{perp}}} \cdot \underbrace{\sin(\omega_i \cdot
k)}_{\text{å›ºå®šå¸¸é‡}}\]</span></p>
<p><span class="math display">\[\mathbf{PE}_{\text{pos}+k} =
\mathbf{A}_k \cdot \mathbf{PE}_{\text{pos}}\]</span></p>
<h2 id="lr-warm-up-and-linear-decay">lr warm-up and linear decay</h2>
<p>We usually start with a large learning rate (lr), and then decay over
time.</p>
<p>For transformer models, it is very useful to set a small warmup
stage, where we first gradually increase lr from zero to the starting
value.</p>
<p>Without this trick, training is likely to get stuck.</p>
<h1 id="bert">BERT</h1>
<p>pretrained with <strong>self supervised training</strong>, no
labels!</p>
<p>Two major objective used in BERT pretraining:</p>
<p>â€¢ Masked language modeling (MLM)</p>
<p>â€¢ Next sentence prediction (NSP)</p>
<h2 id="mlm-masked-language-modeling">MLM Masked language modeling</h2>
<p>Randomly mask (via a [mask] token) 15% of the tokens in each
sequence.</p>
<p>Ask the transformer model to predict the masked token on the top
layer via standard cross-entropy loss.</p>
<table>

<thead>
<tr>
<th style="text-align: left;">ç‰¹æ€§</th>
<th style="text-align: left;">æ©ç è¯­è¨€å»ºæ¨¡ (MLM) - BERT</th>
<th style="text-align: left;">è¿ç»­è¯è¢‹æ¨¡å‹ (CBOW) - Word2Vec</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>æ¨¡å‹æ¶æ„</strong></td>
<td style="text-align: left;"><strong>æ·±å±‚</strong> Transformer ç¼–ç å™¨
(Deep Transformer Encoder)ã€‚</td>
<td style="text-align: left;"><strong>æµ…å±‚</strong> ç¥ç»ç½‘ç»œ (Shallow
Neural Network)ã€‚</td>
</tr>
<tr>
<td style="text-align: left;"><strong>è®­ç»ƒç›®æ ‡</strong></td>
<td style="text-align: left;"><strong>é¢„æµ‹</strong> è¾“å…¥åºåˆ—ä¸­è¢«
<strong><code>[MASK]</code></strong> æ ‡è®°éšæœºæ›¿æ¢çš„è¯å…ƒã€‚</td>
<td style="text-align: left;"><strong>é¢„æµ‹</strong> çª—å£å†…ç¼ºå¤±çš„
<strong>ç›®æ ‡è¯è¯­</strong>ï¼Œæ ¹æ®å…¶å‘¨å›´çš„ä¸Šä¸‹æ–‡è¯è¯­ã€‚</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ä¸Šä¸‹æ–‡åˆ©ç”¨</strong></td>
<td style="text-align: left;">åˆ©ç”¨ <strong>åŒå‘ä¸Šä¸‹æ–‡</strong>
(å·¦ä¾§å’Œå³ä¾§) ä»¥åŠ <strong>æ•´ä¸ªåºåˆ—</strong> çš„ä¿¡æ¯ã€‚ä¾èµ– Transformer çš„
<strong>è‡ªæ³¨æ„åŠ›æœºåˆ¶</strong> æ¥æ•æ‰è¿œè·ç¦»ä¾èµ–ã€‚</td>
<td
style="text-align: left;">ä»…åˆ©ç”¨<strong>å›ºå®šå¤§å°</strong>çª—å£å†…çš„ä¸Šä¸‹æ–‡è¯è¯­ã€‚å°†ä¸Šä¸‹æ–‡è¯è¯­è§†ä¸ºâ€œè¯è¢‹â€ï¼Œé€šå¸¸é€šè¿‡æ±‚å’Œæˆ–å¹³å‡å®ƒä»¬çš„å‘é‡æ¥è¡¨ç¤ºä¸Šä¸‹æ–‡ã€‚</td>
</tr>
<tr>
<td style="text-align: left;"><strong>è¯åºæ•æ„Ÿæ€§</strong></td>
<td style="text-align: left;"><strong>æ•æ„Ÿã€‚</strong> ç”±äºä½¿ç”¨äº†
Transformer
æ¶æ„å’Œ<strong>ä½ç½®ç¼–ç </strong>ï¼Œæ¨¡å‹çŸ¥é“è¯è¯­çš„å…ˆåé¡ºåºã€‚</td>
<td style="text-align: left;"><strong>ä¸æ•æ„Ÿã€‚</strong>
å› ä¸ºå®ƒå°†ä¸Šä¸‹æ–‡è¯è¯­è§†ä¸ºä¸€ä¸ªâ€œè¢‹å­â€è¿›è¡Œå¤„ç†
(æ±‚å’Œ/å¹³å‡)ï¼Œä¸¢å¤±äº†è¯è¯­çš„å…ˆåé¡ºåºä¿¡æ¯ã€‚</td>
</tr>
<tr>
<td style="text-align: left;"><strong>è¾“å‡ºç»“æœ</strong></td>
<td style="text-align: left;"><strong>è¯­å¢ƒåŒ–/åŠ¨æ€åµŒå…¥
(Contextualized/Dynamic Embeddings)ã€‚</strong>
åŒä¸€ä¸ªè¯åœ¨ä¸åŒå¥å­ä¸­çš„å«ä¹‰ä¸åŒï¼Œå…¶å‘é‡ä¹Ÿ<strong>ä¸åŒ</strong>ã€‚</td>
<td style="text-align: left;"><strong>é™æ€åµŒå…¥ (Static
Embeddings)ã€‚</strong>
æ— è®ºå‡ºç°åœ¨ä»€ä¹ˆè¯­å¢ƒä¸­ï¼Œä¸€ä¸ªè¯è¯­éƒ½åªæœ‰ä¸€ä¸ª<strong>å›ºå®š</strong>çš„å‘é‡è¡¨ç¤ºã€‚</td>
</tr>
<tr>
<td style="text-align: left;"><strong>å¤„ç†å¤šä¹‰è¯</strong></td>
<td style="text-align: left;"><strong>å‡ºè‰²ã€‚</strong>
èƒ½åŒºåˆ†å¤šä¹‰è¯çš„ä¸åŒå«ä¹‰ï¼ˆä¾‹å¦‚ï¼Œâ€œé“¶è¡Œâ€ä½œä¸ºé‡‘èæœºæ„å’Œâ€œæ²³å²¸â€çš„å«ä¹‰ï¼‰ã€‚</td>
<td style="text-align: left;"><strong>è¾ƒå¼±ã€‚</strong>
æ— æ³•åŒºåˆ†å¤šä¹‰è¯çš„ä¸åŒå«ä¹‰ï¼Œä¼šä¸ºæ‰€æœ‰è¯­å¢ƒä¸‹çš„å¤šä¹‰è¯å­¦ä¹ <strong>åŒä¸€ä¸ª</strong>å‘é‡ã€‚</td>
</tr>
</tbody>
</table>
<p>Problem: If we only add loss for masked tokens, then the transformer
would not build good representations for nonmasked tokens.</p>
<p>For 10% of the time, we replace [M] with a random token.</p>
<p>For another 10% of the time, we do not change the original token.</p>
<p>80% remaining time, the mask token is used.</p>
<h2 id="nsp-next-sentence-prediction">NSP Next Sentence Prediction</h2>
<p>In addition to MLM, we also add a [CLS] token and ask BERT to predict
whether sentence2 is the next sentence of sentence1.</p>
<p>[CLS] æ¥åˆ†ç±»ï¼Œ[SEP] æ¥åˆ†å‰²å¥å­</p>
<h2 id="finetune">finetune</h2>
<p>BERT finetuning cont.</p>
<p>â€¢ After pretraining, we slightly modify the top layers of BERT and
tune it on downstream tasks</p>
<p>top layer: è¾“å‡º <span class="math inline">\(T\)</span> çš„å±‚</p>
<h2 id="efficient-approach">efficient approach</h2>
<h3 id="electra">ELECTRA</h3>
<p>ç”Ÿæˆå™¨ (Generator)</p>
<ul>
<li><strong>å·¥ä½œåŸç†ï¼š</strong>
å®ƒé€šå¸¸æ˜¯ä¸€ä¸ª<strong>å°å‹</strong>çš„<strong>æ©ç è¯­è¨€æ¨¡å‹
(MLM)</strong>ã€‚</li>
<li><strong>è¾“å…¥å¤„ç†ï¼š</strong> å®ƒé¦–å…ˆåƒ BERT
é‚£æ ·ï¼Œå¯¹è¾“å…¥å¥å­è¿›è¡Œ<strong>æ©ç </strong>ï¼ˆå¦‚å°† "the chef cooked the
meal" å˜ä¸º "the chef [MASK] the meal"ï¼‰ã€‚</li>
<li><strong>è¾“å‡ºï¼š</strong> å®ƒé¢„æµ‹è¢«æ©ç çš„è¯å…ƒï¼Œå¹¶ç”¨è¿™äº›<strong>è²Œä¼¼åˆç†
(plausible)</strong> çš„é¢„æµ‹æ›¿ä»£åŸå¥å­ä¸­çš„ä¸€äº›è¯å…ƒã€‚
<ul>
<li><em>ç¤ºä¾‹ï¼š</em> åŸå§‹å¥å­ä¸­çš„ <code>cooked</code> è¢«
<code>[MASK]</code>ï¼Œç”Ÿæˆå™¨å¯èƒ½ä¼šé¢„æµ‹å‡º <code>ate</code>ï¼Œç„¶åç”¨
<code>ate</code> æ›¿æ¢åŸè¯ã€‚</li>
</ul></li>
</ul>
<p>åˆ¤åˆ«å™¨ (Discriminator) - ELECTRA æ¨¡å‹æœ¬èº«</p>
<ul>
<li><strong>è¾“å…¥ï¼š</strong> æ¥æ”¶è¢«ç”Ÿæˆå™¨<strong>æ›¿æ¢ï¼ˆæŸåï¼‰</strong>
è¿‡çš„è¾“å…¥åºåˆ—ã€‚</li>
<li>è¿™æ˜¯ä¸€ä¸ªäºŒå…ƒåˆ†ç±»ä»»åŠ¡ï¼šå¯¹æ¯ä¸ªè¯å…ƒ <span
class="math inline">\(x_i\)</span>ï¼Œé¢„æµ‹ <span
class="math inline">\(P(\text{IsReplaced}|x_i)\)</span>ã€‚</li>
</ul>
<p>This new pre-training task is more efficient than MLM because the
task is defined over all input tokens rather than just the small subset
that was masked out.</p>
<h3 id="longformer-sliding-window-attention">Longformer Sliding Window
Attention</h3>
<p><strong>æ¯ä¸ª Token çš„è®¡ç®—é‡ï¼š</strong> å¯¹äºåºåˆ—ä¸­çš„<strong>ä»»æ„ä¸€ä¸ª
token</strong>ï¼ˆæŸ¥è¯¢ Qï¼‰ï¼Œå®ƒåªéœ€è¦è®¡ç®—ä¸å®ƒå±€éƒ¨çª—å£ <span
class="math inline">\(w\)</span> å†…çš„<strong>å…¶ä»– <span
class="math inline">\(w\)</span> ä¸ª token</strong>ï¼ˆé”® Kï¼‰çš„æ³¨æ„åŠ›å¾—åˆ†,
<span class="math inline">\(O(w)\)</span>ã€‚ æ€»è®¡ç®—é‡ <span
class="math inline">\(\approx n \times O(w) = O(n \cdot
w)\)</span>ã€‚</p>
<p>For an embedding on layer L, whatâ€™s its receptive field? (how many
input tokens does it cover?)</p>
<p>Lxw</p>
<p><img src="/img/nlp2_part1/nlp=win.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>Combined with dilated sliding window</p>
<p>We can add gaps in the window to make it even wider with the same
amount of compute(æ•æ‰é•¿è·ç¦»ä¾èµ–)</p>
<p>We can use a combination of 2 heads of dilated and other heads with
local sliding window.ï¼ˆç”¨multiheadå®ç°çŸ­è·ç¦»é•¿è·ç¦»ç»“åˆï¼‰</p>
<h1 id="vae-lmä¸è€ƒ">VAE-LMï¼ˆä¸è€ƒï¼‰</h1>
<p><img src="/img/nlp2_part1/nlp=vae.png" srcset="/img/loading.gif" lazyload alt=""></p>
<table>

<thead>
<tr>
<th style="text-align: left;">ç»„ä»¶</th>
<th style="text-align: left;">åç§°</th>
<th style="text-align: left;">åŠŸèƒ½</th>
<th style="text-align: left;">ç»“æ„</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(q_{\phi}\)</span> (Encoder)</strong></td>
<td style="text-align: left;"><strong>ç¼–ç å™¨/åéªŒæ¨¡å‹</strong></td>
<td style="text-align: left;">æ¥æ”¶è¾“å…¥å¥å­ <span
class="math inline">\(\mathbf{x}\)</span>ï¼Œå°†å®ƒå‹ç¼©æˆä¸€ä¸ªæ½œåœ¨è¯­ä¹‰å‘é‡
<span class="math inline">\(\mathbf{z}\)</span> çš„åˆ†å¸ƒã€‚</td>
<td style="text-align: left;">1. <strong><span
class="math inline">\(\text{RNNs}\)</span> (LSTM Cell):</strong>
ç¼–ç æ•´ä¸ªå¥å­ <span class="math inline">\(\mathbf{x}\)</span>ã€‚ 2.
<strong>Linear Layers:</strong> æ ¹æ® <span
class="math inline">\(\text{RNN}\)</span> çš„æœ€ç»ˆéšè—çŠ¶æ€ï¼Œè¾“å‡ºæ½œå˜é‡
<span class="math inline">\(\mathbf{z}\)</span> åˆ†å¸ƒçš„å‚æ•° <span
class="math inline">\(\mu\)</span> (å‡å€¼) å’Œ <span
class="math inline">\(\sigma\)</span> (æ–¹å·®)ã€‚</td>
</tr>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(\mathbf{z}\)</span></strong></td>
<td style="text-align: left;"><strong>æ½œåœ¨å˜é‡ (Latent
Variable)</strong></td>
<td
style="text-align: left;">ä¸€ä¸ªè¿ç»­çš„ä½ç»´å‘é‡ï¼Œä»£è¡¨æ•´ä¸ªå¥å­çš„<strong>å…¨å±€è¯­ä¹‰</strong>ã€‚</td>
<td style="text-align: left;">ä» <span
class="math inline">\(\mathbf{z}\)</span> çš„åˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°ã€‚</td>
</tr>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(p_{\theta}\)</span> (Decoder)</strong></td>
<td style="text-align: left;"><strong>è§£ç å™¨/ç”Ÿæˆæ¨¡å‹</strong></td>
<td style="text-align: left;">æ¥æ”¶ <span
class="math inline">\(\mathbf{z}\)</span>
ä½œä¸ºè¾“å…¥ï¼Œå¹¶é€å­—ç”Ÿæˆï¼ˆé‡æ„ï¼‰å¥å­ <span
class="math inline">\(\mathbf{x}\)</span>ã€‚</td>
<td style="text-align: left;">1. <strong><span
class="math inline">\(\text{RNNs}\)</span> (LSTM Cell):</strong> æ¥æ”¶
<span class="math inline">\(\mathbf{z}\)</span> ä½œä¸ºåˆå§‹çŠ¶æ€æˆ–è¾“å…¥ã€‚ 2.
<strong>ç”Ÿæˆè¿‡ç¨‹:</strong> ä»èµ·å§‹ç¬¦å·ï¼ˆå¦‚ <code>&lt;EOS&gt;</code>
åœ¨å›¾ä¸­ä¼¼ä¹è¢«è¯¯ç½®ï¼Œé€šå¸¸æ˜¯ <code>&lt;BOS&gt;</code> æˆ– <span
class="math inline">\(\mathbf{z}\)</span>ï¼‰å¼€å§‹ï¼Œé€æ­¥ç”Ÿæˆ <span
class="math inline">\(\text{work} \rightarrow \text{work} \rightarrow
\text{&lt;EOS&gt;}\)</span>ã€‚</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>åéªŒ (<span class="math inline">\(q_{\phi}\)</span>) å’Œå…ˆéªŒ
(<span class="math inline">\(p(\mathbf{z})\)</span>) åˆ†å¸ƒï¼š</strong>
å›¾ä¸­æ˜ç¡®æŒ‡å‡ºï¼Œå…ˆéªŒåˆ†å¸ƒ <span
class="math inline">\(p(\mathbf{z})\)</span> å’ŒåéªŒåˆ†å¸ƒ <span
class="math inline">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span>
éƒ½æ˜¯<strong>é«˜æ–¯åˆ†å¸ƒ
(Gaussian)</strong>ï¼Œä¸”é€šå¸¸æ˜¯å¯¹è§’åæ–¹å·®çŸ©é˜µï¼ˆdiagonalï¼‰ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åœ¨æ½œç©ºé—´ä¸­æ˜¯å¯å‚æ•°åŒ–çš„ã€‚</li>
</ul>
<hr />
<h2 id="ç”Ÿæˆ-generation-è¿‡ç¨‹">ğŸš€ ç”Ÿæˆ (Generation) è¿‡ç¨‹</h2>
<p>ç”Ÿæˆæ–°å¥å­æ—¶ï¼Œæˆ‘ä»¬åªä½¿ç”¨<strong>è§£ç å™¨ <span
class="math inline">\(p_{\theta}\)</span></strong>ï¼š</p>
<ol type="1">
<li><strong>Sample <span class="math inline">\(\mathbf{z}\)</span> from
prior <span class="math inline">\(p(\mathbf{z})\)</span>:</strong>
ä»ä¸€ä¸ªç®€å•çš„<strong>å…ˆéªŒåˆ†å¸ƒ</strong>ï¼ˆå¦‚æ ‡å‡†æ­£æ€åˆ†å¸ƒ <span
class="math inline">\(\mathcal{N}(\mathbf{0},
\mathbf{I})\)</span>ï¼‰ä¸­éšæœºé‡‡æ ·ä¸€ä¸ªæ½œåœ¨è¯­ä¹‰å‘é‡ <span
class="math inline">\(\mathbf{z}\)</span>ã€‚è¿™ä¸ª <span
class="math inline">\(\mathbf{z}\)</span>
å°±æ˜¯æˆ‘ä»¬å¸Œæœ›ç”Ÿæˆå¥å­æ‰€æ‹¥æœ‰çš„<strong>æ„å›¾æˆ–ä¸»é¢˜</strong>ã€‚</li>
<li><strong>Sample <span class="math inline">\(\mathbf{x}\)</span> from
our generative model <span class="math inline">\(p_{\theta}(\mathbf{x} |
\mathbf{z})\)</span>:</strong> å°†è¿™ä¸ªé‡‡æ ·çš„ <span
class="math inline">\(\mathbf{z}\)</span>
å–‚ç»™<strong>è§£ç å™¨</strong>ï¼Œè§£ç å™¨ <span
class="math inline">\(\text{RNN}\)</span> å°±ä¼šé€è¯ç”Ÿæˆå¥å­ <span
class="math inline">\(\mathbf{x}\)</span>ï¼Œç›´åˆ°ç”Ÿæˆç»“æŸç¬¦å· (<span
class="math inline">\(\text{&lt;EOS&gt;}\)</span>)ã€‚</li>
</ol>
<hr />
<h2 id="è®­ç»ƒç›®æ ‡textvae-textelbo-æœ€å¤§åŒ–">ğŸ“ˆ è®­ç»ƒç›®æ ‡ï¼š<span
class="math inline">\(\text{VAE}\)</span> "<span
class="math inline">\(\text{ELBO}\)</span>" (æœ€å¤§åŒ–)</h2>
<p><span class="math display">\[
\mathcal{L}(\theta; \mathbf{x}) =
\underbrace{-\text{KL}(q_{\phi}(\mathbf{z} | \mathbf{x}) \|
p(\mathbf{z}))}_{\text{I. KL Divergence (Regularization Term)}} +
\underbrace{\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}[\log
p_{\theta}(\mathbf{x} | \mathbf{z})]}_{\text{II. Expectation of
Log-Likelihood (Reconstruction Term)}}
\]</span></p>
<ul>
<li><h3 id="i.-textkl-æ•£åº¦é¡¹-textkldots">I. <span
class="math inline">\(\text{KL}\)</span> æ•£åº¦é¡¹ï¼ˆ<span
class="math inline">\(-\text{KL}(\dots)\)</span>ï¼‰</h3>
<span class="math display">\[-\text{KL}(q_{\phi}(\mathbf{z} |
\mathbf{x}) \| p(\mathbf{z}))\]</span>
<ul>
<li><strong>ä½œç”¨ï¼š</strong>
è¿™æ˜¯ä¸€ä¸ª<strong>æ­£åˆ™åŒ–é¡¹</strong>ï¼Œå®ƒè¡¡é‡äº†<strong>ç¼–ç å™¨</strong>è¾“å‡ºçš„åéªŒåˆ†å¸ƒ
<span class="math inline">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span>
ä¸ç®€å•çš„<strong>å…ˆéªŒåˆ†å¸ƒ</strong> <span
class="math inline">\(p(\mathbf{z})\)</span> ä¹‹é—´çš„å·®å¼‚ã€‚</li>
</ul></li>
<li><h3 id="ii.-é‡æ„é¡¹-mathbbelog-p_thetamathbfx-mathbfz">II. é‡æ„é¡¹
(<span class="math inline">\(\mathbb{E}[\log p_{\theta}(\mathbf{x} |
\mathbf{z})]\)</span>)</h3>
<span class="math display">\[\mathbb{E}_{q_{\phi}(\mathbf{z} |
\mathbf{x})}[\log p_{\theta}(\mathbf{x} | \mathbf{z})]\]</span>
<ul>
<li><strong>ä½œç”¨ï¼š</strong>
è¿™æ˜¯ä¸€ä¸ª<strong>é‡æ„é¡¹</strong>ï¼Œå®ƒè¡¡é‡äº†<strong>è§£ç å™¨</strong>ä»æ½œåœ¨å‘é‡
<span class="math inline">\(\mathbf{z}\)</span> é‡æ„å‡ºåŸå§‹å¥å­ <span
class="math inline">\(\mathbf{x}\)</span>
çš„<strong>å¯¹æ•°æ¦‚ç‡</strong>ã€‚</li>
</ul></li>
<li><h3 id="è¯æ®ä¸‹ç•Œ">è¯æ®ä¸‹ç•Œ</h3>
<span class="math inline">\(\text{ELBO}\)</span>
çš„å€¼<strong>å°äºæˆ–ç­‰äº</strong>æ•°æ®çš„å¯¹æ•°è¾¹ç¼˜ä¼¼ç„¶ <span
class="math inline">\(\log p(\mathbf{x})\)</span>ï¼š <span
class="math display">\[\mathcal{L}(\theta; \mathbf{x}) \leq \log
p(\mathbf{x})\]</span> å› æ­¤ï¼Œæœ€å¤§åŒ– <span
class="math inline">\(\mathcal{L}(\theta; \mathbf{x})\)</span>
å°±æ˜¯åœ¨æœ€å¤§åŒ– <span class="math inline">\(\log p(\mathbf{x})\)</span>
çš„ä¸€ä¸ª<strong>ä¸‹ç•Œ</strong>ï¼Œä»è€Œé—´æ¥ä¼˜åŒ–äº†æ•´ä¸ªæ¨¡å‹ã€‚</li>
</ul>
<h3 id="textelbo-æ¨å¯¼è¿‡ç¨‹è§£é‡Š">ğŸ“ˆ <span
class="math inline">\(\text{ELBO}\)</span> æ¨å¯¼è¿‡ç¨‹è§£é‡Š</h3>
<p>æ¨å¯¼çš„ç›®æ ‡æ˜¯æ‰¾åˆ° <span class="math inline">\(\log
p(\mathbf{x})\)</span> çš„ä¸€ä¸ª<strong>ä¸‹ç•Œ</strong>ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º <span
class="math inline">\(\mathcal{L}(\theta; \mathbf{x})\)</span> æˆ– <span
class="math inline">\(\text{ELBO}\)</span>ã€‚</p>
<p><span class="math display">\[
\ln p(\mathbf{x}) = \ln \int_{\mathbf{z}} p(\mathbf{x}, \mathbf{z})
d\mathbf{z}
\]</span></p>
<p><span class="math display">\[
= \ln \int_{\mathbf{z}} p(\mathbf{x}, \mathbf{z}) \frac{q(\mathbf{z} |
\mathbf{x})}{q(\mathbf{z} | \mathbf{x})} d\mathbf{z}
\]</span></p>
<p><span class="math display">\[
\ge \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln \frac{p(\mathbf{x},
\mathbf{z})}{q(\mathbf{z} | \mathbf{x})}\right]
\]</span></p>
<ul>
<li><strong>è¯´æ˜ï¼š</strong> è¿™æ˜¯æ¨å¯¼çš„å…³é”®ä¸€æ­¥ã€‚ç”±äº <span
class="math inline">\(\ln(\cdot)\)</span> æ˜¯ä¸€ä¸ª<strong>å‡¹å‡½æ•° (concave
function)</strong>ï¼Œæ ¹æ® <strong>è©¹æ£®ä¸ç­‰å¼ (Jensen's
Inequality)</strong>ï¼Œå¯¹äºä»»ä½•éšæœºå˜é‡ <span
class="math inline">\(Y\)</span>ï¼š <span
class="math display">\[\ln(\mathbb{E}[Y]) \ge
\mathbb{E}[\ln(Y)]\]</span></li>
</ul>
<p><span class="math display">\[
= \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln \frac{p(\mathbf{x},
\mathbf{z})}{q(\mathbf{z} | \mathbf{x})}\right]
\]</span> <span class="math display">\[
= \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln \frac{p(\mathbf{x} |
\mathbf{z}) p(\mathbf{z})}{q(\mathbf{z} | \mathbf{x})}\right]
\]</span></p>
<p><span class="math display">\[
= \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln p(\mathbf{x} |
\mathbf{z})\right] + \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln
\frac{p(\mathbf{z})}{q(\mathbf{z} | \mathbf{x})}\right]
\]</span></p>
<p><span class="math display">\[
= \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln p(\mathbf{x} |
\mathbf{z})\right] - \int_{\mathbf{z}} q(\mathbf{z} | \mathbf{x}) \ln
\frac{q(\mathbf{z} | \mathbf{x})}{p(\mathbf{z})} d\mathbf{z}
\]</span> <span class="math display">\[
= \text{likelihood} - \mathbb{D}_{\text{KL}}[q(\mathbf{z} | \mathbf{x})
\| p(\mathbf{z})]
\]</span></p>
<p>åœ¨ <span class="math inline">\(\text{VAE}\)</span>
è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬é€šè¿‡<strong>æœ€å¤§åŒ–</strong>è¿™ä¸ª <span
class="math inline">\(\text{ELBO}\)</span>
ç›®æ ‡å‡½æ•°ï¼Œé—´æ¥å®ç°äº†å¯¹çœŸå®æ•°æ®åˆ†å¸ƒ <span
class="math inline">\(p(\mathbf{x})\)</span> çš„å»ºæ¨¡ã€‚</p>
<p>ä» <strong>KL æ•£åº¦çš„å®šä¹‰</strong>å¼€å§‹ï¼š <span
class="math display">\[D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) ||
p_{\theta}(\mathbf{z}|\mathbf{x})) = E_{q_{\phi}(\mathbf{z}|\mathbf{x})}
\left[ \log
\frac{q_{\phi}(\mathbf{z}|\mathbf{x})}{p_{\theta}(\mathbf{z}|\mathbf{x})}
\right]\]</span></p>
<p>å°†å¯¹æ•°ä¸­çš„é™¤æ³•åˆ†è§£ä¸ºå‡æ³•ï¼š <span class="math display">\[D_{KL} =
E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log
q_{\phi}(\mathbf{z}|\mathbf{x}) \right] -
E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log
p_{\theta}(\mathbf{z}|\mathbf{x}) \right]\]</span></p>
<p>æ ¹æ®è´å¶æ–¯å®šç† <span class="math inline">\(p(\mathbf{z}|\mathbf{x}) =
\frac{p(\mathbf{x}, \mathbf{z})}{p(\mathbf{x})}\)</span>ï¼Œæˆ‘ä»¬æœ‰ <span
class="math inline">\(\log p(\mathbf{z}|\mathbf{x}) = \log p(\mathbf{x},
\mathbf{z}) - \log p(\mathbf{x})\)</span>ã€‚ä»£å…¥ä¸Šå¼ï¼š</p>
<p><span class="math display">\[D_{KL} =
E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log
q_{\phi}(\mathbf{z}|\mathbf{x}) \right] -
E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log p_{\theta}(\mathbf{x},
\mathbf{z}) - \log p_{\theta}(\mathbf{x}) \right]\]</span></p>
<p>å°†æœŸæœ›ä¸­çš„ <span class="math inline">\(\log
p_{\theta}(\mathbf{x})\)</span> ç§»å‡ºï¼ˆå› ä¸ºå®ƒä¸ <span
class="math inline">\(\mathbf{z}\)</span> æ— å…³ï¼ŒæœŸæœ›å€¼å°±æ˜¯å®ƒæœ¬èº«ï¼‰ï¼š</p>
<p><span class="math display">\[D_{KL} =
E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log
q_{\phi}(\mathbf{z}|\mathbf{x}) \right] -
E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log p_{\theta}(\mathbf{x},
\mathbf{z}) \right] + \log p_{\theta}(\mathbf{x})\]</span></p>
<p>é‡æ–°æ’åˆ—å„é¡¹ï¼š <span class="math display">\[\log
p_{\theta}(\mathbf{x}) = E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log
p_{\theta}(\mathbf{x}, \mathbf{z}) \right] -
E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log
q_{\phi}(\mathbf{z}|\mathbf{x}) \right] +
D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) ||
p_{\theta}(\mathbf{z}|\mathbf{x}))\]</span></p>
<p>æœ€åï¼Œæˆ‘ä»¬å°†å‰ä¸¤é¡¹åˆå¹¶ï¼Œå¾—åˆ° ELBO çš„å®šä¹‰ï¼š <span
class="math display">\[\mathcal{L}(\theta, \phi) =
E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log
\frac{p_{\theta}(\mathbf{x},
\mathbf{z})}{q_{\phi}(\mathbf{z}|\mathbf{x})} \right]\]</span></p>
<p>æ‰€ä»¥ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°è¿™ä¸ªå…³é”®ç­‰å¼ï¼š <span class="math display">\[\log
p(\mathbf{x}) = \mathcal{L}(\theta, \phi) +
D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) ||
p_{\theta}(\mathbf{z}|\mathbf{x}))\]</span></p>
<h2 id="é‡å‚æ•°">é‡å‚æ•°</h2>
<p><span class="math display">\[
\mathcal{L}(\theta; \mathbf{x}) =
\underbrace{-\text{KL}(q_{\phi}(\mathbf{z} | \mathbf{x}) \|
p(\mathbf{z}))}_{\text{I. KL Divergence (Regularization Term)}} +
\underbrace{\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}[\log
p_{\theta}(\mathbf{x} | \mathbf{z})]}_{\text{II. Expectation of
Log-Likelihood (Reconstruction Term)}}
\]</span></p>
<p>the second term involves a sampling operation from the parameterized
q, which we can not directly back-prop</p>
<p>åœ¨å¼•å…¥é‡æ–°å‚æ•°åŒ–æŠ€å·§ä¹‹å‰ï¼Œæˆ‘ä»¬è®¡ç®—é‡æ„é¡¹çš„æ¢¯åº¦æ˜¯è¿™æ ·çš„ï¼š</p>
<p><span class="math display">\[\nabla_{\phi}
E_{q_{\phi}(\mathbf{z}|\mathbf{x})} [\log
p_{\theta}(\mathbf{x}|\mathbf{z})]\]</span></p>
<p>å½“æˆ‘ä»¬ä½¿ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·æ¥è¿‘ä¼¼è¿™ä¸ªæœŸæœ›æ—¶ï¼š</p>
<p><span class="math display">\[\nabla_{\phi} \left( \frac{1}{M}
\sum_{m=1}^{M} \log p_{\theta}(\mathbf{x}|\mathbf{z}^{(m)}) \right)
\quad \text{å…¶ä¸­ } \mathbf{z}^{(m)} \sim
q_{\phi}(\mathbf{z}|\mathbf{x})\]</span></p>
<p><strong>é—®é¢˜æ ¸å¿ƒåœ¨äºï¼š</strong></p>
<ul>
<li><strong>é‡‡æ ·è¿‡ç¨‹ (<span class="math inline">\(\mathbf{z} \sim
q_{\phi}(\mathbf{z}|\mathbf{x})\)</span>)
æ˜¯ä¸€ä¸ªç¦»æ•£çš„ã€ä¸å¯é€†çš„éšæœºæ“ä½œã€‚</strong></li>
<li>å®ƒå°±åƒä¸€ä¸ª<strong>é»‘ç®±</strong>ï¼Œè¾“å…¥æ˜¯å‚æ•° <span
class="math inline">\(\phi\)</span>ï¼ˆå†³å®šäº† <span
class="math inline">\(q\)</span> çš„ <span
class="math inline">\(\mu\)</span> å’Œ <span
class="math inline">\(\sigma\)</span>ï¼‰ï¼Œè¾“å‡ºæ˜¯æ ·æœ¬ <span
class="math inline">\(\mathbf{z}\)</span>ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œ<strong>è¿™ä¸ªä»
<span class="math inline">\(\phi\)</span> åˆ° <span
class="math inline">\(\mathbf{z}\)</span>
çš„æ˜ å°„æ˜¯ä¸å¯å¯¼çš„</strong>ã€‚</li>
<li>å› æ­¤ï¼Œæˆ‘ä»¬æ— æ³•åˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®— <span
class="math inline">\(\mathbf{z}\)</span> å¯¹ <span
class="math inline">\(\phi\)</span> çš„æ¢¯åº¦ <span
class="math inline">\(\frac{\partial \mathbf{z}}{\partial
\phi}\)</span>ï¼Œä¹Ÿå°±æ— æ³•å°†æŸå¤±å‡½æ•°çš„æ¢¯åº¦ <span
class="math inline">\(\frac{\partial \mathcal{L}}{\partial
\mathbf{z}}\)</span> ä¼ é€’å› <span
class="math inline">\(\phi\)</span>ã€‚</li>
</ul>
<p>é‡æ–°å‚æ•°åŒ–æŠ€å·§å°±æ˜¯é€šè¿‡<strong>é‡å†™éšæœºå˜é‡çš„ç”Ÿæˆè¿‡ç¨‹</strong>ï¼Œå°†éšæœºæ€§ï¼ˆ<span
class="math inline">\(\epsilon\)</span>ï¼‰å’Œå‚æ•°ä¾èµ–æ€§ (<span
class="math inline">\(\phi\)</span> é€šè¿‡ <span
class="math inline">\(\mu, \sigma\)</span>)
<strong>åˆ†ç¦»</strong>ï¼Œä»è€Œåˆ›å»ºä¸€æ¡<strong>å¯å¯¼çš„è·¯å¾„ (pathwise
gradient)</strong>ï¼š</p>
<p><span class="math display">\[\mathbf{z} = \mu(\mathbf{x}, \phi) +
\sigma(\mathbf{x}, \phi) \odot \epsilon\]</span></p>
<p><strong>ç°åœ¨çš„è®¡ç®—å›¾æ˜¯ï¼š</strong></p>
<ol type="1">
<li><span class="math inline">\(\phi \to (\mu, \sigma)\)</span>
(ç¡®å®šæ€§ï¼Œå¯å¯¼)</li>
<li><span class="math inline">\((\mu, \sigma)\)</span> å’Œ <span
class="math inline">\(\epsilon\)</span> (éšæœºä½†<strong>ç‹¬ç«‹äº</strong>
<span class="math inline">\(\phi\)</span>) <span
class="math inline">\(\to \mathbf{z}\)</span> (ç¡®å®šæ€§å‡½æ•° <span
class="math inline">\(g\)</span>ï¼Œå¯å¯¼)</li>
<li><span class="math inline">\(\mathbf{z} \to \log
p(\mathbf{x}|\mathbf{z})\)</span> (ç¡®å®šæ€§ï¼Œå¯å¯¼)</li>
<li><span class="math inline">\(\log p(\mathbf{x}|\mathbf{z}) \to
\mathcal{L}\)</span></li>
</ol>
<p>ç”±äº <span class="math inline">\(\mathbf{z}\)</span> ç°åœ¨æ˜¯ <span
class="math inline">\(\phi\)</span>
çš„ä¸€ä¸ª<strong>ç¡®å®šæ€§å‡½æ•°</strong>ï¼ˆé€šè¿‡ <span
class="math inline">\(\mu\)</span> å’Œ <span
class="math inline">\(\sigma\)</span>ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨é“¾å¼æ³•åˆ™æ±‚å¯¼ï¼š</p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial
\phi} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}} \frac{\partial
\mathbf{z}}{\partial \mu} \frac{\partial \mu}{\partial \phi} +
\frac{\partial \mathcal{L}}{\partial \mathbf{z}} \frac{\partial
\mathbf{z}}{\partial \sigma} \frac{\partial \sigma}{\partial
\phi}\]</span></p>
<p>æ³¨ï¼š<span class="math inline">\(\frac{\partial \mathbf{z}}{\partial
\sigma} =\epsilon\)</span></p>
<p><img src="/img/nlp2_part1/nlp=re.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="optimization-challenge">optimization challenge</h2>
<p>KL term quickly decrease to zero (throwing away latent
information)</p>
<p>Comparing to the second term, the KL-prior term is easier to optimize
(why?).</p>
<p>â€¢ The RNN decoder is strong and has ground-truth history in its
input.</p>
<h3 id="kl-ä»£ä»·é€€ç«-kl-cost-annealing">1. KL ä»£ä»·é€€ç« (KL Cost
Annealing) ğŸŒ¡ï¸</h3>
<ul>
<li><strong>æ–¹æ³•ï¼š</strong> åœ¨ç›®æ ‡å‡½æ•°ä¸­ï¼Œä¸º KL
æ•£åº¦é¡¹æ·»åŠ ä¸€ä¸ª<strong>å¯å˜æƒé‡ <span
class="math inline">\(\beta\)</span></strong>ã€‚
<ul>
<li>åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œè®¾ç½® <span class="math inline">\(\beta\)</span>
<strong>æ¥è¿‘æˆ–ç­‰äºé›¶</strong>ã€‚</li>
<li>éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œ<span class="math inline">\(\beta\)</span>
<strong>é€æ¸å¢åŠ </strong>ï¼Œç›´åˆ°è¾¾åˆ° 1ã€‚</li>
</ul></li>
<li><strong>æ•°å­¦å½¢å¼ï¼š</strong> <span
class="math display">\[\mathcal{L}(\theta; \vec{x}) = \mathbb{E}[\log
p_\theta(\vec{x}|\vec{z})] - \beta \cdot
\text{KL}(q_\phi(\vec{z}|\vec{x})||p(\vec{z}))\]</span></li>
<li><strong>ç›´è§‰è§£é‡Šï¼š</strong>
<ol type="1">
<li><strong>æ—©æœŸ ( <span class="math inline">\(\beta \approx 0\)</span>
):</strong>
ç›®æ ‡å‡½æ•°å‡ ä¹åªå‰©ä¸‹<strong>é‡æ„é¡¹</strong>ã€‚è¿™ç»™äº†ç¼–ç å™¨å’Œè§£ç å™¨å……è¶³çš„æœºä¼šï¼Œå»å­¦ä¹ ä¸€ä¸ª<strong>æœ€å¤§åŒ–ä¿¡æ¯é‡</strong>çš„æ½œåœ¨è¡¨ç¤º
<span class="math inline">\(\vec{z}\)</span>ï¼Œè€Œä¸å¿…æ‹…å¿ƒ <span
class="math inline">\(\vec{z}\)</span> æ˜¯å¦æ¥è¿‘å…ˆéªŒåˆ†å¸ƒ <span
class="math inline">\(p(\vec{z})\)</span>ã€‚</li>
<li><strong>åæœŸ ( <span class="math inline">\(\beta \to 1\)</span>
):</strong> KL
é¡¹çš„æƒé‡é€æ¸å¢å¤§ï¼Œå¼€å§‹å‘æŒ¥å…¶<strong>æ­£åˆ™åŒ–</strong>ä½œç”¨ï¼Œè¿«ä½¿å­¦ä¹ åˆ°çš„æ½œåœ¨åˆ†å¸ƒ
<span class="math inline">\(q_\phi(\vec{z}|\vec{x})\)</span> æ¥è¿‘å…ˆéªŒ
<span class="math inline">\(p(\vec{z})\)</span>ã€‚</li>
</ol></li>
<li><strong>æ•ˆæœï¼š</strong> è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°è®©æ¨¡å‹å…ˆâ€œå­¦ä¼šè¯´è¯â€ï¼ˆå­¦ä¹ æœ‰ç”¨çš„
<span class="math inline">\(\vec{z}\)</span>ï¼‰ï¼Œç„¶åå†â€œè§„èŒƒè¯­æ³•â€ï¼ˆæ»¡è¶³
KL çº¦æŸï¼‰ï¼Œä»è€Œé¿å…äº† <span class="math inline">\(\vec{z}\)</span>
åœ¨è®­ç»ƒåˆæœŸå°±è¢«ä¼˜åŒ–å™¨è½»æ˜“æŠ›å¼ƒã€‚</li>
</ul>
<hr />
<h3 id="è¾“å…¥è¯ä¸¢å¼ƒ-input-word-dropping">2. è¾“å…¥è¯ä¸¢å¼ƒ (Input Word
Dropping) âœ‚ï¸</h3>
<ul>
<li><strong>æ–¹æ³•ï¼š</strong>
è¿™æ˜¯ä¸€ç§ç›´æ¥<strong>å‰Šå¼±è§£ç å™¨</strong>çš„æ–¹æ³•ã€‚
<ul>
<li>åœ¨è®­ç»ƒæ—¶ï¼Œéšæœºåœ°<strong>ç§»é™¤éƒ¨åˆ†æˆ–å…¨éƒ¨</strong>ç”¨äº<strong>æ•™å¸ˆå¼ºåˆ¶</strong>çš„
<strong>Ground-Truth å†å²è¯</strong>ï¼ˆå³ <span
class="math inline">\(x_{t-1}\)</span>ï¼‰ã€‚forcing the model to rely on
the latent vectorã€‚ ---</li>
</ul></li>
</ul>
<h3 id="è¯è¢‹æŸå¤±-bag-of-words-loss-bow-loss">3. è¯è¢‹æŸå¤± (Bag-of-Words
Loss, BoW Loss) ğŸ›ï¸</h3>
<ul>
<li><strong>æ–¹æ³•ï¼š</strong>
é™¤äº†æ ‡å‡†çš„åºåˆ—é‡æ„æŸå¤±å¤–ï¼Œ<strong>å¹¶è¡Œåœ°</strong>è®­ç»ƒä¸€ä¸ªè¾…åŠ©è§£ç å™¨ï¼ˆæˆ–è€…åœ¨ä¸»è§£ç å™¨ä¸­å¢åŠ ä¸€ä¸ªåˆ†æ”¯ï¼‰æ¥é¢„æµ‹è¾“å…¥
<span class="math inline">\(\vec{x}\)</span> çš„<strong>è¯è¢‹è¡¨ç¤º
(Bag-of-Words, BoW)</strong>ã€‚
<ul>
<li><strong>è¯è¢‹è¡¨ç¤ºï¼š</strong> æ˜¯ä¸€ä¸ªå‘é‡ï¼ŒåªåŒ…å« <span
class="math inline">\(\vec{x}\)</span>
ä¸­<strong>æ¯ä¸ªè¯å‡ºç°çš„æ¬¡æ•°</strong>ï¼Œè€Œå¿½ç•¥è¯çš„é¡ºåºï¼ˆå³<strong>éåºåˆ—ä¿¡æ¯</strong>ï¼‰ã€‚</li>
<li>è¿™ä¸ª BoW æŸå¤±æ—¨åœ¨ç¡®ä¿ <span class="math inline">\(\vec{z}\)</span>
è‡³å°‘ç¼–ç äº† <span class="math inline">\(\vec{x}\)</span>
çš„<strong>å†…å®¹ä¿¡æ¯</strong>ï¼Œå³ä½¿æ²¡æœ‰ç¼–ç é¡ºåºä¿¡æ¯ã€‚</li>
</ul></li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/NLP/" class="print-no-link">#NLP</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>é»æ˜ä¹‹å‰ï¼šTransformerä¹‹å‰çš„è‡ªç„¶è¯­è¨€å¤„ç†</div>
      <div>http://example.com/2025/12/12/nlp2/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>ç‘¾ç‘œç•¶å¹´</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2025å¹´12æœˆ12æ—¥</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/12/12/nlp3/" title="ç ´æ™“ä¹‹åˆ»ï¼šTransformerçš„è¯ç”Ÿä¸è‡ªç„¶è¯­è¨€å¤„ç†å‰æ²¿">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">ç ´æ™“ä¹‹åˆ»ï¼šTransformerçš„è¯ç”Ÿä¸è‡ªç„¶è¯­è¨€å¤„ç†å‰æ²¿</span>
                        <span class="visible-mobile">ä¸Šä¸€ç¯‡</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/12/12/nlp1/" title="è€æ´¾è‡ªç„¶è¯­è¨€å¤„ç†ä¹‹å¿…è¦">
                        <span class="hidden-mobile">è€æ´¾è‡ªç„¶è¯­è¨€å¤„ç†ä¹‹å¿…è¦</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="twikoo"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/twikoo/1.6.39/twikoo.all.min.js', function() {
        var options = Object.assign(
          {"envId":"https://twikoo-haru.netlify.app/.netlify/functions/twikoo","region":"ap-shanghai","path":"window.location.pathname","enable":true},
          {
            el: '#twikoo',
            path: 'window.location.pathname',
            onCommentLoaded: function() {
              Fluid.utils.listenDOMLoaded(function() {
                var imgSelector = '#twikoo .tk-content img:not(.tk-owo-emotion)';
                Fluid.plugins.imageCaption(imgSelector);
                Fluid.plugins.fancyBox(imgSelector);
              });
            }
          }
        )
        twikoo.init(options)
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>ç›®å½•</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡ 
        <span id="busuanzi_value_site_pv"></span>
         æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•° 
        <span id="busuanzi_value_site_uv"></span>
         äºº
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
<!-- hexo injector body_end start --><script data-pjax src="https://unpkg.com/oh-my-live2d"></script><script>const oml2d = OML2D.loadOml2d({dockedPosition:"left",mobileDisplay:true,models:[{"path":"https://model.hacxy.cn/HK416-1-normal/model.json","scale":0.1,"motionPreloadStrategy":"ALL","position":[-10,60],"stageStyle":{"height":520},"mobileScale":0.06,"mobilePosition":[10,30],"mobileStageStyle":{"height":250}}],parentElement:document.body,primaryColor:"#336699",sayHello:false,tips:{style: {"width":230,"height":120,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":180,"height":80,"left":"calc(50% - 30px)","top":"-100px"},idleTips:{interval:15000,message:function(){
  return axios.get('https://v1.hitokoto.cn?c=i')
    .then(function (response) {
      return response.data.hitokoto ;
    })
    .catch(function (error) {
      console.error(error);
    });
}
}}});</script><!-- hexo injector body_end end --></body>
</html>
