<h1 id="Transformer-decoder-and-GPT"><a href="#Transformer-decoder-and-GPT" class="headerlink" title="Transformer decoder and GPT"></a>Transformer decoder and GPT</h1><p>Apply attention mask to forbid the attention from future timesteps to train AR-LM.</p>
<p>A transformer model for AR-LM is also<br>referred to as a transformer decoder.</p>
<pre><code class="lang-python">def attention(query, key, value, mask=None, dropout=None):
    &quot;Compute &#39;Scaled Dot Product Attention&#39;&quot;
     d_k = query.size(-1)
     scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
     if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9) #åœ¨maskä¸º0çš„åœ°æ–¹åŠ ä¸€ä¸ªå¾ˆå°çš„è´Ÿæ•°ï¼Œä¿è¯expä¹‹åè¶‹äº0ï¼Œå½¢çŠ¶ä¸º(L_Q,L_k)
     p_attn = scores.softmax(dim=-1)
     # å¯¹keyç»´åº¦åšsoftmax
    output = torch.matmul(p_attn, value)
    return output, p_attn
</code></pre>
<h2 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h2><p>apply the learned model zero-shot to some<br>downstream language generation task (translation,<br>summarization, QA, etc.).</p>
<p>zero-shotï¼šå®Œå…¨ä¸å¾®è°ƒ</p>
<p>è®¾è®¡ä¸€äº›å¾ˆå¥½çš„promptæ¥å®ç°ï¼Œæ¯”å¦‚ï¼›</p>
<p>â€œTranslate the following text to French. Text: [ENG TEXT] French:â€<br>â€œGiven the document, answer the question. Document: [DOC] Question:<br>[Q] Answer:â€</p>
<p>open-ended generation:  tasks that has big<br>freedom and diversity, like story or news generation.The model needs to rely its own (memory, consistency or<br>creativity)</p>
<h2 id="topk-sampling"><a href="#topk-sampling" class="headerlink" title="topk sampling"></a>topk sampling</h2><p> We will represent $P(\cdot | W<em>{1..i})$ by $p = (p_1, p_2, \dots, p</em>{|V|})$ (where the elements are sorted so that $p<em>1 \geq p_2 \geq p_3 \dots \geq p</em>{|V|}$).</p>
<p> Top-K sampling transforms $p$ to $\hat{p}$ by:</p>
<script type="math/tex; mode=display">\hat{p}_i = \frac{p_i \cdot \mathbb{1}\{i \leq K\}}{Z}</script><p><strong>$Z$</strong> æ˜¯<strong>å½’ä¸€åŒ–å¸¸æ•° (normalization constant)</strong>ã€‚</p>
<pre><code>        $$Z = \sum_{i=1}^{|V|} p_i \cdot \mathbb{1}\{i \leq K\} = \sum_{i=1}^{K} p_i$$
</code></pre><p>ä¿ç•™æ¦‚ç‡æœ€å¤§çš„kä¸ªï¼Œå¹¶é‡æ–°å½’ä¸€åŒ–æ€»æ¦‚ç‡ä¸º1ã€‚å¦‚æœk=1ï¼Œé‚£å°±å˜æˆäº†greedy decodingäº†ã€‚</p>
<p>quality-diversity trade-off</p>
<h1 id="Rethink-MLEï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰"><a href="#Rethink-MLEï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰" class="headerlink" title="Rethink MLEï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰"></a>Rethink MLEï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰</h1><p> <strong>MLE ç›®æ ‡å‡½æ•° (The MLE objective):</strong></p>
<pre><code>$$\log P(W) = \sum \log P(W_i | W_{1:i-1})$$
</code></pre><ul>
<li><strong>$P(W<em>i | W</em>{1:i-1})$</strong> æ˜¯åœ¨ç»™å®šå†å²ä¸Šä¸‹æ–‡ $W_{1:i-1}$ çš„æƒ…å†µä¸‹ï¼Œä¸‹ä¸€ä¸ªè¯æ˜¯ $W_i$ çš„æ¦‚ç‡ã€‚</li>
</ul>
<ul>
<li><strong>â€œResearchers thinks the </strong>teacher forcing<strong> in MLE training is to blame.â€</strong>     åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå½“æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ $W<em>i$ æ—¶ï¼Œå®ƒä½¿ç”¨çš„å†å²ä¸Šä¸‹æ–‡ $W</em>{1:i-1}$ <strong>æ€»æ˜¯</strong>æ¥è‡ªäº<strong>çœŸå®çš„è®­ç»ƒæ•°æ®</strong>ï¼Œè€Œä¸æ˜¯æ¨¡å‹è‡ªå·±ä¹‹å‰é¢„æµ‹çš„è¯ã€‚</li>
</ul>
<p>The exposure bias hypothesis: Due to the exposure to ground-truth prefix, the model is biased to only perform well during training, but not generation. ï¼ˆè®­ç»ƒçš„æ—¶å€™å‰æ–‡æ˜¯å®Œç¾çš„æ‰€ä»¥è¡¨ç°å¥½ï¼‰</p>
<p>Importantly, the error is assumed to accumulate during generation, and the generation will be incrementally distortedï¼ˆæ‰­æ›²ï¼‰. è‡ªå›å½’ä¸­é”™è¯¯ç´¯åŠ </p>
<h2 id="we-cannot-directly-use-GAN"><a href="#we-cannot-directly-use-GAN" class="headerlink" title="we cannot directly use GAN"></a>we cannot directly use GAN</h2><p>GAN çš„ç›®æ ‡æ˜¯è®©ç”Ÿæˆå™¨ $G$ å’Œåˆ¤åˆ«å™¨ $D$ äº’ç›¸ç«äº‰ï¼š</p>
<script type="math/tex; mode=display">\min_{G} \max_{D} V(D, G) = \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{\mathbf{z}}(\mathbf{z})}[\log(1 - D(G(\mathbf{z})))]</script><ul>
<li><strong>åˆ¤åˆ«å™¨ $D$ çš„ç›®æ ‡ (æœ€å¤§åŒ– $\max_{D}$):</strong><ul>
<li>æœ€å¤§åŒ– $\log D(\mathbf{x})$ (çœŸå®æ•°æ® $\mathbf{x}$ è¢«åˆ¤åˆ«ä¸ºçœŸçš„æ¦‚ç‡)ã€‚</li>
<li>æœ€å¤§åŒ– $\log(1 - D(G(\mathbf{z})))$ (ç”Ÿæˆæ•°æ® $G(\mathbf{z})$ è¢«åˆ¤åˆ«ä¸ºå‡çš„æ¦‚ç‡)ã€‚</li>
</ul>
</li>
<li><strong>ç”Ÿæˆå™¨ $G$ çš„ç›®æ ‡ (æœ€å°åŒ– $\min_{G}$):</strong><ul>
<li>æœ€å°åŒ– $\log(1 - D(G(\mathbf{z})))$ (å³è®©ç”Ÿæˆæ•°æ® $G(\mathbf{z})$ è¢«åˆ¤åˆ«ä¸ºçœŸçš„æ¦‚ç‡ $D(G(\mathbf{z}))$ å°½é‡é«˜)ã€‚</li>
</ul>
</li>
</ul>
<ul>
<li>$G$ (ç”Ÿæˆå™¨) è¾“å‡ºçš„æ˜¯è¯æ±‡è¡¨ä¸Š<strong>æ¦‚ç‡</strong>åˆ†å¸ƒã€‚<ul>
<li>ä¸ºäº†å¾—åˆ°ä¸€ä¸ªå…·ä½“çš„è¯åºåˆ—ï¼ˆæ–‡æœ¬ï¼‰ï¼Œéœ€è¦ä»è¿™ä¸ªåˆ†å¸ƒä¸­è¿›è¡Œ<strong>é‡‡æ ·</strong>ï¼ˆä¾‹å¦‚ï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯æˆ–ç”¨ Top-K é‡‡æ ·ï¼‰ã€‚<ul>
<li>è¯­è¨€æ¨¡å‹ä¸­çš„è¯æ˜¯<strong>ç¦»æ•£çš„</strong>ï¼ˆä¾‹å¦‚ï¼Œâ€çŒ«â€ã€â€ç‹—â€ã€â€è·‘â€ï¼‰ï¼Œè€Œä¸æ˜¯åƒå›¾åƒåƒç´ å€¼é‚£æ ·çš„<strong>è¿ç»­</strong>å€¼ã€‚<br>gradient cannot flow back through discrete samplingï¼</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="The-gumbel-softmax-reparameterization"><a href="#The-gumbel-softmax-reparameterization" class="headerlink" title="The gumbel-softmax reparameterization"></a>The gumbel-softmax reparameterization</h2><p>Gumbel-Maxï¼šç¦»æ•£é‡‡æ ·çš„è¿‡ç¨‹ï¼šargmaxéè¿ç»­ä¸å¯å¾®</p>
<pre><code>$$z = \text{one\_hot}(\arg \max_i [\log \pi_i + g_i])$$
</code></pre><p>Gumbel-Softmaxç”¨ <strong>Softmax å‡½æ•°</strong>æ›¿æ¢äº† $\arg \max$ æ“ä½œ</p>
<script type="math/tex; mode=display">y_i = \frac{\exp((\log \pi_i + g_i) / \tau)}{\sum_{j=1}^{k} \exp((\log \pi_j + g_j) / \tau)}</script><p>æ¸©åº¦çš„ä½œç”¨ï¼š</p>
<ul>
<li><strong>$\tau$ è¾ƒå¤§</strong>ï¼š$y$ çš„åˆ†å¸ƒä¼šæ›´<strong>å¹³æ»‘</strong>ï¼Œæ›´æ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼Œä½†<strong>è¿‘ä¼¼è¯¯å·®è¾ƒå¤§</strong>ã€‚</li>
<li><strong>$\tau$ è¾ƒå°</strong>ï¼š$y$ çš„åˆ†å¸ƒä¼šæ›´<strong>å°–é” (sharper)</strong>ï¼Œæ›´æ¥è¿‘ One-Hot å‘é‡ï¼Œä»è€Œæ›´å¥½åœ°<strong>è¿‘ä¼¼ç¦»æ•£çš„é‡‡æ ·</strong>ï¼Œå¹¶ä¸”æ›´æ¥è¿‘ Gumbel-Max çš„ç»“æœã€‚</li>
</ul>
<p>å¯¹äºæ¸©åº¦ä¹Ÿæœ‰diversity-quality trade-off</p>
<p>However, it is shown that language GANs are actually worse than the MLE baseline.</p>
<pre><code class="lang-python">def gumbel_softmax(logits, tau=1.0, hard=False, dim=-1):
    &quot;&quot;&quot;
    å®ç°äº† Gumbel-Softmax é‡å‚æ•°åŒ–æŠ€å·§ã€‚

    å‚æ•°è¯´æ˜:
        logits (Tensor): æœªæ ‡å‡†åŒ–çš„å¯¹æ•°æ¦‚ç‡ï¼ˆæ¥è‡ªç”Ÿæˆå™¨ G çš„è¾“å‡ºï¼‰ã€‚
        tau (float): æ¸©åº¦å‚æ•° (Ï„)ï¼Œæ§åˆ¶ Softmax è¿‘ä¼¼çš„å¹³æ»‘ç¨‹åº¦ã€‚
        hard (bool): å¦‚æœä¸º Trueï¼Œåˆ™åº”ç”¨ Straight-Throughï¼ˆç›´é€šï¼‰æŠ€å·§ï¼š
                     åœ¨å‰å‘ä¼ æ’­ä¸­ä½¿ç”¨ One-Hot å‘é‡ï¼Œè€Œåœ¨åå‘ä¼ æ’­ä¸­ä½¿ç”¨è¿ç»­æ¢¯åº¦ã€‚
        dim (int): åº”ç”¨ Softmax çš„ç»´åº¦ï¼ˆé€šå¸¸æ˜¯è¯æ±‡è¡¨ç»´åº¦ï¼‰ã€‚
    &quot;&quot;&quot;

    # --- 1. Gumbel å™ªå£°æ³¨å…¥ ---

    # 1a. ç”Ÿæˆ Gumbel å™ªå£° g_i ~ Gumbel(0, 1)ã€‚
    # è¿™æ˜¯é€šè¿‡é€†å˜æ¢é‡‡æ ·æ–¹æ³•å®ç°çš„ï¼šg_i = -log(-log(U))ï¼Œå…¶ä¸­ U ~ Uniform(0, 1)ã€‚
    gumbels = -torch.empty_like(logits).exponential_().log() 

    # 1b. å°† Gumbel å™ªå£°æ·»åŠ åˆ° logits å¹¶é™¤ä»¥æ¸©åº¦ (tau)ã€‚
    # è¿™å¯¹åº”äº Gumbel-Softmax å…¬å¼ä¸­çš„åˆ†å­éƒ¨åˆ†ï¼š(log(pi_i) + g_i) / tau
    gumbels = (logits + gumbels) / tau

    # --- 2. è¿ç»­ Softmax è¾“å‡º (y_soft) ---

    # è®¡ç®—è¿ç»­çš„ã€å¯å¾®åˆ†çš„ Softmax è¾“å‡ºã€‚
    # è¿™ä¸ª y_soft å€¼ç”¨äºåœ¨åå‘ä¼ æ’­ä¸­è®¡ç®—æ¢¯åº¦ã€‚
    y_soft = gumbels.softmax(dim)

    # --- 3. Straight-Throughï¼ˆç›´é€šï¼‰æŠ€å·§å®ç° ---

    if hard:
        # A. å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨ç¡¬æ€§ One-Hot å‘é‡ï¼‰

        # æ‰¾åˆ° y_soft ä¸­æœ€å¤§æ¦‚ç‡å€¼å¯¹åº”çš„ç´¢å¼•ã€‚
        # [1] ä» .max() è¿”å›çš„å…ƒç»„ä¸­æå–ç´¢å¼•ï¼ˆmax_indicesï¼‰ã€‚
        # è¿™æ¨¡æ‹Ÿäº† Gumbel-Max æŠ€å·§ä¸­çš„ arg max æ“ä½œã€‚
        index = y_soft.max(dim, keepdim=True)[1]

        # åŸºäºæœ€å¤§ç´¢å¼•ï¼Œåˆ›å»ºä¸€ä¸ªç¡¬æ€§ï¼ˆç¦»æ•£ï¼‰çš„ One-Hot å‘é‡ y_hardã€‚
        # åœ¨å‰å‘è®¡ç®—ä¸­ï¼ˆä¾‹å¦‚ä½œä¸ºåˆ¤åˆ«å™¨ D çš„è¾“å…¥ï¼‰ï¼Œä½¿ç”¨çš„æ˜¯ y_hardã€‚
        y_hard = torch.zeros_like(logits).scatter_(dim, index, 1.0)

        # B. åå‘ä¼ æ’­ï¼ˆä½¿ç”¨æŸ”æ€§è¿ç»­æ¢¯åº¦ï¼‰

        # Straight-Through è¡¨è¾¾å¼ï¼šret = y_hard - y_soft.detach() + y_soft
        # æ¢¯åº¦åˆ†æï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼š
        # 1. d(y_hard)/d(logits) â‰ˆ 0 ï¼ˆç”±äºç¦»æ•£/arg max æ“ä½œï¼Œæ¢¯åº¦è¢«å¿½ç•¥ï¼‰ã€‚
        # 2. d(y_soft.detach())/d(logits) = 0 ï¼ˆæ¢¯åº¦è¢« .detach() æ˜¾å¼åˆ‡æ–­ï¼‰ã€‚
        # 3. d(y_soft)/d(logits) æ˜¯å”¯ä¸€æœ‰æ•ˆçš„ã€å¹³æ»‘çš„æ¢¯åº¦ã€‚
        # æœ€ç»ˆæ¢¯åº¦ï¼šd(ret)/d(logits) â‰ˆ d(y_soft)/d(logits)
        ret = y_hard - y_soft.detach() + y_soft

    else:
        # æ ‡å‡†é‡å‚æ•°åŒ–æŠ€å·§ï¼ˆä¸ä½¿ç”¨ ST æŠ€å·§ï¼‰ã€‚
        # è¿ç»­çš„ y_soft å‘é‡ç”¨äºå‰å‘å’Œåå‘ä¼ æ’­ã€‚
        ret = y_soft

    return ret
</code></pre>
<h2 id="policy-gradient"><a href="#policy-gradient" class="headerlink" title="policy gradient"></a>policy gradient</h2><p><img src="nlp=rl.png" alt=""></p>
<p>é¿å…äº†é‡‡æ ·å¯¼è‡´çš„ä¸å¯å¾®åˆ†</p>
<p>å…¬å¼å³ä¾§çš„<strong>æ¢¯åº¦ $\nabla<em>{\theta}$ åªä½œç”¨äºå¯¹æ•°æ¦‚ç‡ $\log P</em>{\theta}(\mathbf{y}|\mathbf{x})$</strong>ã€‚</p>
<ul>
<li><strong>$\log P_{\theta}(\mathbf{y}|\mathbf{x})$</strong> æ˜¯ä¸€ä¸ª<strong>è¿ç»­</strong>ä¸”<strong>å¯å¾®åˆ†</strong>çš„å‡½æ•°ï¼Œå®ƒæ˜¯æ¨¡å‹è¾“å‡ºæ¦‚ç‡çš„å¯¹æ•°ï¼Œä¸æ¨¡å‹å‚æ•° $\theta$ ä¹‹é—´æœ‰æ˜ç¡®çš„ã€å¯å¾®åˆ†çš„è®¡ç®—å›¾ã€‚</li>
<li><strong>å›æŠ¥ $r(\mathbf{x}, \mathbf{y})$</strong> åªæ˜¯ä¸€ä¸ªåœ¨<strong>é‡‡æ ·å®Œæˆå</strong>è®¡ç®—å‡ºæ¥çš„<strong>æ ‡é‡æƒé‡</strong>ï¼ˆå®ƒä¸ä¾èµ–äº $\theta$ï¼Œå› æ­¤æ±‚æ¢¯åº¦æ—¶è¢«è§†ä¸ºå¸¸é‡ï¼‰ã€‚</li>
<li><strong>é‡‡æ ·è¿‡ç¨‹ $\mathbf{y} \sim P_{\theta}$</strong> è¢«ç§»åˆ°äº†<strong>æœŸæœ› $\mathbb{E}$ çš„å¤–éƒ¨</strong>ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›ï¼ˆMonte Carloï¼‰æ–¹æ³•è¿‘ä¼¼å®ç°ã€‚</li>
</ul>
<h2 id="MLE-å¸Œæœ›æ›´å¤šçš„diversity"><a href="#MLE-å¸Œæœ›æ›´å¤šçš„diversity" class="headerlink" title="MLE å¸Œæœ›æ›´å¤šçš„diversity"></a>MLE å¸Œæœ›æ›´å¤šçš„diversity</h2><p>æœ€å¤§ä¼¼ç„¶ä¼°è®¡ - MLEï¼š</p>
<script type="math/tex; mode=display">\arg \min_{\theta} \underset{W \sim P_D}{\mathbb{E}} \left[ -\frac{1}{L} \sum_{l=0}^{L-1} \log P_M(W_{l+1}|W_{1:l}) \right]</script><p>æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œ$P<em>M(W) = P_M(W_1) \cdot P_M(W_2|W_1) \cdots P_M(W_L|W</em>{1:L-1})$ï¼Œå› æ­¤è´Ÿå¯¹æ•°ä¼¼ç„¶å¯ä»¥ç®€åŒ–ä¸ºå¯¹æ•´ä¸ªåºåˆ— $W$ çš„è´Ÿå¯¹æ•°ä¼¼ç„¶çš„æœŸæœ›ã€‚</p>
<script type="math/tex; mode=display">\arg \min_{\theta} \underset{W \sim P_D}{\mathbb{E}} \left[ -\log P_M(W) \right]</script><script type="math/tex; mode=display">D_{KL}(P_D || P_M) = \underset{W \sim P_D}{\mathbb{E}} [\log P_D(W)] - \underset{W \sim P_D}{\mathbb{E}} [\log P_M(W)]</script><p>å‰ä¸€é¡¹å°±æ˜¯çœŸå®åˆ†å¸ƒçš„entropyçš„è´Ÿæ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹å‡ºMLEæ˜¯å¸Œæœ›modelçš„åˆ†å¸ƒéå¸¸æ¥è¿‘çœŸå®çš„åˆ†å¸ƒ</p>
<p><img src="nlp=kld.png" alt=""></p>
<h2 id="exposure-bias-ä¸æ˜¯å¤§é—®é¢˜â€”â€”The-Prefix-Switching-Experiment"><a href="#exposure-bias-ä¸æ˜¯å¤§é—®é¢˜â€”â€”The-Prefix-Switching-Experiment" class="headerlink" title="exposure bias ä¸æ˜¯å¤§é—®é¢˜â€”â€”The Prefix Switching Experiment"></a>exposure bias ä¸æ˜¯å¤§é—®é¢˜â€”â€”The Prefix Switching Experiment</h2><p>ç¬¬ä¸€ç»„ï¼Œç»™çœŸå®çš„å¥å­</p>
<p>ç¬¬äºŒç»„ï¼Œè‡ªå›å½’</p>
<p>ç¬¬ä¸‰ç»„ï¼Œéšæœºtokenå‰ç¼€</p>
<p>Did the error accumulate ?<br>æ¨¡å‹åœ¨å‰ç¼€ä¸­æ¥æ”¶åˆ°äº†ä¸¥é‡çš„é”™è¯¯æˆ–å¹²æ‰°ï¼ˆæ‰“ä¹±çš„æˆ–éšæœºçš„ Tokenï¼‰ï¼Œä½†å®ƒå¹¶æ²¡æœ‰å¯¼è‡´æ•´ä¸ªåç»­åºåˆ—çš„ç”Ÿæˆè´¨é‡ç¾éš¾æ€§åœ°ä¸‹é™ã€‚</p>
<p>Mysteriously, the model self-recovers from the errors in the prefix.</p>
<p>For the shuffled data prefix, the model still generates something related<br>to the air force.</p>
<p>EB-M, quantifies the<br>quality ratio between generations from data prefix and a given (imperfect)<br>prefix.</p>
<p>The combination of MLE training and sampling algorithm is very<br>strong and is the default choice.</p>
<h1 id="sampling"><a href="#sampling" class="headerlink" title="sampling"></a>sampling</h1><p>topk </p>
<script type="math/tex; mode=display">\hat{p}_i = \frac{p_i \cdot \mathbf{1}\{i \leq K\}}{Z}</script><p>topp</p>
<script type="math/tex; mode=display">\hat{p}_i = \frac{p_i \cdot \mathbf{1}\{\sum_{j=1}^{i-1} p_j < P\}}{Z}</script><p>Temperature Sampling</p>
<script type="math/tex; mode=display">\hat{p}_i = \frac{\exp(\log(p_i)/T)}{Z} = \frac{(p_i)^{1/T}}{Z}</script><h2 id="ğŸ¤”-å®ƒä»¬æœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿ"><a href="#ğŸ¤”-å®ƒä»¬æœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿ" class="headerlink" title="ğŸ¤” å®ƒä»¬æœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿ"></a>ğŸ¤” å®ƒä»¬æœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿ</h2><p>Entropy Reduction<br>Order Preservation<br>Slope Preservation</p>
<h3 id="1-å…ƒç´ é¡ºåºå¾—ä»¥ä¿ç•™"><a href="#1-å…ƒç´ é¡ºåºå¾—ä»¥ä¿ç•™" class="headerlink" title="1. å…ƒç´ é¡ºåºå¾—ä»¥ä¿ç•™"></a>1. å…ƒç´ é¡ºåºå¾—ä»¥ä¿ç•™</h3><p> $p_i \ge p_j \implies \hat{p}_i \ge \hat{p}_j$</p>
<h3 id="2-åˆ†å¸ƒçš„ç†µè¢«é™ä½"><a href="#2-åˆ†å¸ƒçš„ç†µè¢«é™ä½" class="headerlink" title="2. åˆ†å¸ƒçš„ç†µè¢«é™ä½"></a>2. åˆ†å¸ƒçš„ç†µè¢«é™ä½</h3><p> $\mathcal{H}(\hat{p}) \le \mathcal{H}(p)$</p>
<h3 id="3-åˆ†å¸ƒçš„æ–œç‡å¾—ä»¥ä¿ç•™"><a href="#3-åˆ†å¸ƒçš„æ–œç‡å¾—ä»¥ä¿ç•™" class="headerlink" title="3. åˆ†å¸ƒçš„æ–œç‡å¾—ä»¥ä¿ç•™"></a>3. åˆ†å¸ƒçš„æ–œç‡å¾—ä»¥ä¿ç•™</h3><script type="math/tex; mode=display">\frac{\log p_i - \log p_j}{\log p_l - \log p_k} = \frac{\log \hat{p}_i - \log \hat{p}_j}{\log \hat{p}_l - \log \hat{p}_k}</script><h1 id="Correcting-bad-behavior-of-NLG-models"><a href="#Correcting-bad-behavior-of-NLG-models" class="headerlink" title="Correcting bad behavior of NLG models"></a>Correcting bad behavior of NLG models</h1><h2 id="repeat"><a href="#repeat" class="headerlink" title="repeat"></a>repeat</h2><p>è¿™ç§åŸºäºç‚¹ç§¯çš„åŒ¹é…æœºåˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨<strong>è§£ç  (Decoding)</strong> é˜¶æ®µä¸<strong>è´ªå©ªæœç´¢ (Greedy Search)</strong> æˆ–<strong>æ³¢æŸæœç´¢ (Beam Search)</strong> ç»“åˆæ—¶ï¼Œç‰¹åˆ«å®¹æ˜“å¯¼è‡´é‡å¤ï¼š</p>
<ul>
<li><strong>è¯­ä¹‰æƒ¯æ€§</strong>ï¼šTransformer æœ¬è´¨ä¸Šæ˜¯æ“…é•¿æ•æ‰<strong>å±€éƒ¨ä¾èµ–</strong>å’Œ<strong>è¯­ä¹‰ä¸€è‡´æ€§</strong>çš„ã€‚å½“ä¸€ä¸ªåºåˆ—åœ¨çŸ­æ—¶é—´å†…é‡å¤å‡ºç°æ—¶ï¼Œæ¨¡å‹ä¼šè®¤ä¸ºâ€œä¿æŒè¿™ç§æ¨¡å¼â€æ˜¯<strong>æœ€å®‰å…¨ã€æœ€ä¸€è‡´</strong>çš„é€‰æ‹©ï¼Œä»è€Œåœ¨ä¼—å¤šå¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯ä¸­ï¼Œ<strong>å€¾å‘äºé€‰æ‹©ä¸å½“å‰å†å²ç›¸ä¼¼åº¦æœ€é«˜çš„è¯</strong>ï¼ˆå³ç‚¹ç§¯åˆ†æ•°æœ€é«˜çš„è¯ï¼‰ã€‚</li>
<li><strong>ç¼ºä¹å…¨å±€æŠ‘åˆ¶</strong>ï¼šæ ‡å‡†çš„ Transformer æ¶æ„åœ¨è®¾è®¡ä¸Šç¼ºä¹ä¸€ä¸ª<strong>å†…ç½®çš„æœºåˆ¶</strong>æ¥<strong>æƒ©ç½š</strong>æˆ–<strong>æŠ‘åˆ¶</strong>åˆšåˆšç”Ÿæˆè¿‡çš„å†…å®¹ã€‚å®ƒåªæ˜¯åœ¨å¯»æ‰¾å±€éƒ¨æœ€ä½³åŒ¹é…ï¼Œè€Œç‚¹ç§¯æ­£æ˜¯æ‰¾åˆ°è¿™ä¸ªå±€éƒ¨æœ€ä½³åŒ¹é…ï¼ˆç›¸ä¼¼åº¦ï¼‰çš„æœ‰æ•ˆå·¥å…·ã€‚</li>
</ul>
<h3 id="Biased-decoding"><a href="#Biased-decoding" class="headerlink" title="Biased decoding"></a>Biased decoding</h3><script type="math/tex; mode=display">p_i = \frac{\exp(x_i / (T \cdot I(i \in g)))}{\sum_{j} \exp(x_j / (T \cdot I(j \in g)))}</script><ul>
<li>$I(c)$ï¼šè¿™æ˜¯ä¸€ä¸ª<strong>æŒ‡ç¤ºå‡½æ•° (Indicator Function)</strong> æˆ–<strong>æƒ©ç½šå› å­</strong>ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š<script type="math/tex; mode=display">I(c) = \begin{cases} \theta & \text{if } c \text{ is True} \\ 1 & \text{else} \end{cases}</script></li>
<li>$\theta$ (Theta)ï¼šè®¾å®šä¸º $1.2$ï¼Œå¯¹é‡å¤ token çš„<strong>æƒ©ç½š/åå·®å› å­</strong>ã€‚</li>
</ul>
<ul>
<li>å¯¹äºæ–°çš„ã€æœªé‡å¤çš„ Token $i$ï¼š<br>  æŒ‡ç¤ºå‡½æ•° $I(i \in g)$ çš„å€¼ä¸º $1$ã€‚</li>
</ul>
<ul>
<li>å¯¹äºå·²ç”Ÿæˆçš„ã€é‡å¤çš„ Token $j$ï¼š<br>   æŒ‡ç¤ºå‡½æ•° $I(j \in g)$ çš„å€¼ä¸º $\theta = 1.2$ï¼Œå¯¼è‡´æ•´ä¸ªåˆ†æ•° $x_j / (T \cdot \theta)$ <strong>å˜å°</strong>ï¼Œé™ä½é‡å¤çš„æ¦‚ç‡</li>
</ul>
<h3 id="Unlikelihood-training-for-repetition"><a href="#Unlikelihood-training-for-repetition" class="headerlink" title="Unlikelihood training for repetition"></a>Unlikelihood training for repetition</h3><script type="math/tex; mode=display">\mathcal{L}^t_{\text{UL-token}} \left( p_\theta(\cdot|x_{<t}), c^t \right) = - \alpha \sum_{c \in c^t} \log(1 - p_\theta(c|x_{<t})) - \log p_\theta(x_t|x_{<t})</script><p>æœ€å¤§åŒ–ä¼¼ç„¶ï¼ˆåä¸€é¡¹ï¼‰ï¼Œæœ€å°åŒ–é¢„æµ‹å‡ºé‡å¤tokençš„æ¦‚ç‡ï¼ˆå‰ä¸€é¡¹ï¼‰</p>
<h2 id="Generic-Response-Problem"><a href="#Generic-Response-Problem" class="headerlink" title="Generic Response Problem"></a>Generic Response Problem</h2><p>æ¨¡å‹ç»™å‡ºçš„genericçš„å›å¤ï¼Œè€Œä¸æ˜¯æœ‰é’ˆå¯¹æ€§çš„ä¿¡æ¯é‡å¤§çš„å›å¤</p>
<p>Combined with MLE training, when the model is not sure about what to say, it<br>degrades to some simple and â€œsafeâ€ pattern in data.</p>
<h3 id="Maximum-Mutual-Information-ï¼ˆMMIï¼‰"><a href="#Maximum-Mutual-Information-ï¼ˆMMIï¼‰" class="headerlink" title="Maximum Mutual Information ï¼ˆMMIï¼‰"></a>Maximum Mutual Information ï¼ˆMMIï¼‰</h3><script type="math/tex; mode=display">\text{MMI}(S; T) = \log \frac{P(S, T)}{P(S)P(T)}</script><script type="math/tex; mode=display">\hat{T} = \arg \max_T \left\{ \log P(T|S) - \log P(T) \right\}</script><p>åœ¨æœ‰å‰æ–‡çš„æ¡ä»¶ä¸‹åæ–‡å‡ºç°çš„æ¦‚ç‡-åæ–‡å•ç‹¬å‡ºç°çš„æ¦‚ç‡ï¼Œæƒ©ç½šå’Œå‰æ–‡å…³ç³»ä¸å¤§çš„é€šç”¨å›ç­”ã€‚</p>
<h3 id="negative-training"><a href="#negative-training" class="headerlink" title="negative training"></a>negative training</h3><p>ç”¨è´Ÿæ ·æœ¬å‘Šè¯‰å¤§æ¨¡å‹ä¸åº”è¯¥å»è¯´ä»€ä¹ˆ</p>
<script type="math/tex; mode=display">\text{Loss}_{\text{new}} = \underbrace{- \log P_\theta(y_{\text{pos}}|x_{\text{pos}})}_{\text{æœ€å¤§ä¼¼ç„¶é¡¹ (æ­£æ ·æœ¬)}} + \underbrace{\log P_\theta(y_{\text{neg}}|x_{\text{neg}})}_{\text{è´Ÿæ ·æœ¬æƒ©ç½šé¡¹}}</script><h1 id="Transformer-encoder-decoder"><a href="#Transformer-encoder-decoder" class="headerlink" title="Transformer encoder-decoder"></a>Transformer encoder-decoder</h1><p>Each decoder layer is a selfattention followed by a crossattention.</p>
<p>The query vector for a transformer<br>decoderâ€™s cross-attention head<br>is from the output of the previous<br>decoder layer. However, the key<br>and value vectors are from the<br>encodersâ€™ outputs.</p>
<h2 id="Why-decoder-only-LM-is-more-convenient-less-design-choiceto-consider-or-efficient-for-both-training-or-application"><a href="#Why-decoder-only-LM-is-more-convenient-less-design-choiceto-consider-or-efficient-for-both-training-or-application" class="headerlink" title="Why decoder-only LM is more convenient (less design choiceto consider) or efficient (for both training or application)?"></a>Why decoder-only LM is more convenient (less design choiceto consider) or efficient (for both training or application)?</h2><p>Pretraining (left) and chatbot generation (right) are highly consistent.</p>
<p>â€¢ Naturally handles variable-length text generation during pretraining!</p>
<p>â€¢ During application, we just do natural concatenation (always causal<br>attention). No computation is wasted (assuming we save hidden<br>states of the history).</p>
<p><img src="nlp=dec.png" alt=""></p>
<p>In pretraining, we need to build text of variable length, and the<br>training signal is only from the decoder side.</p>
<p>â€¢ During application, we need to re-encode (especially when the<br>encoder is bi-directional) the whole history for each dialogue turn.</p>
<p><img src="nlp=encdec.png" alt=""></p>
<h2 id="rope"><a href="#rope" class="headerlink" title="rope"></a>rope</h2><p>We want the dot product between query (position $m$) and key (position $n$) to directly be a function of ($m-n$).</p>
<script type="math/tex; mode=display">
        \langle \boldsymbol{f}_{\boldsymbol{q}}(\boldsymbol{x}_m, m), \boldsymbol{f}_{\boldsymbol{k}}(\boldsymbol{x}_n, n) \rangle = \boldsymbol{g}(\boldsymbol{x}_m, \boldsymbol{x}_n, m-n)</script><h1 id="GPT3-and-in-context-learning"><a href="#GPT3-and-in-context-learning" class="headerlink" title="GPT3 and in-context learning"></a>GPT3 and in-context learning</h1><p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ˜¯æŒ‡ å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åˆ†æå…¶è¾“å…¥æç¤ºï¼ˆPromptï¼‰ä¸­åµŒå…¥çš„å°‘æ•°ç¤ºä¾‹æˆ–æ¼”ç¤ºï¼ˆDemonstrationsï¼‰ï¼Œæ¥å¿«é€Ÿç†è§£å¹¶æ‰§è¡Œç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œä¼ ç»Ÿçš„æ¨¡å‹å‚æ•°æ›´æ–°ï¼ˆå³æ— éœ€å¾®è°ƒæˆ–æ¢¯åº¦ä¸‹é™ï¼‰ã€‚</p>
<h2 id="few-shot-before-GPT3"><a href="#few-shot-before-GPT3" class="headerlink" title="few shot before GPT3"></a>few shot before GPT3</h2><p>Before GPT3, few-shot learning still refers to how a model can quickly<br>adapt to a new task demonstrated with only a few examples via<br>gradient update.</p>
<p>We have a meta-learning phase on a wide set of tasks. In effect, the<br>meta-learning problem treats entire tasks as training examples.ï¼ˆæŠŠä¸€ä¸ªä»»åŠ¡çœ‹ä½œä¸€ä¸ªæ ·æœ¬ï¼Œæƒ³è¦å­¦ä¹ åˆ°ä¸€ä¸ªæ¯”è¾ƒå¥½çš„åˆå§‹é…ç½®ï¼‰</p>
<h2 id="chain-of-thought-ï¼ˆcotï¼‰"><a href="#chain-of-thought-ï¼ˆcotï¼‰" class="headerlink" title="chain of thought ï¼ˆcotï¼‰"></a>chain of thought ï¼ˆcotï¼‰</h2><p>ideaï¼šreasoning is more consuming than computationï¼Œgive llm more time to think</p>
<p>cot çš„æ•ˆæœåœ¨æ¨¡å‹å¤§çš„æ—¶å€™æ›´åŠ æ˜æ˜¾</p>
<p>few-shot: ç»™å‡ ä¸ªä¾‹å­â€”â€”add manually written reasoning before giving answer in prompt.</p>
<p>zero-shot: chaining of 2 prompts. ç¬¬ä¸€æ­¥æ˜ç¡®è¯´è¦ä¸€æ­¥ä¸€æ­¥æ¨ç†ï¼Œè·å¾—æ¨ç†çš„è¿‡ç¨‹ã€‚ç¬¬äºŒæ­¥æ¥ç€è¿™ä¸ªå‘Šè¯‰å®ƒè¾“å‡ºçš„æ ¼å¼ï¼Œè·å¾—æ­£ç¡®çš„è¾“å‡ºã€‚</p>
<h3 id="research"><a href="#research" class="headerlink" title="research"></a>research</h3><h4 id="CoT-with-self-consistency"><a href="#CoT-with-self-consistency" class="headerlink" title="CoT with self-consistency"></a>CoT with self-consistency</h4><p>For CoT, we could sample multiple reasoning path from the LLM<br>with temperature sampling. æ¸©åº¦è¶Šé«˜è¶Šéšæœºï¼Œè¶Šæœ‰diversity</p>
<p>And then take a majority voting over the answers!</p>
<h4 id="Tree-of-Thoughts"><a href="#Tree-of-Thoughts" class="headerlink" title="Tree of Thoughts"></a>Tree of Thoughts</h4><p>Maintain and expand a thought-tree.</p>
<p>â€¢ For each existing step, we prompt the LLM to propose multiple next steps,<br>and also to judge which path (by giving a value) is more promising (pls refer<br>to paper for how the prompts are designed).</p>
<p>â€¢ The nodes that are judged to be unlikely will be discarded</p>
<p><img src="nlp=cot.png" alt=""></p>
<h4 id="bias-in-icl"><a href="#bias-in-icl" class="headerlink" title="bias in icl"></a>bias in icl</h4><p>Majority and recency bias</p>
<p>å¤šæ•°åå·®æ˜¯æŒ‡åœ¨å°‘æ ·æœ¬å­¦ä¹ çš„æç¤ºä¸­ï¼Œå¦‚æœæä¾›çš„è®­ç»ƒç¤ºä¾‹çš„ç±»åˆ«åˆ†å¸ƒæ˜¯ä¸å¹³è¡¡çš„ï¼Œæ¨¡å‹å°±ä¼šå€¾å‘äºé¢„æµ‹å‡ºç°æ¬¡æ•°æœ€å¤šçš„é‚£ä¸ªç±»åˆ«ã€‚å³ä½¿ä¸€ä¸ªæ–°çš„æµ‹è¯•æ ·æœ¬å®¢è§‚ä¸Šå±äºå°‘æ•°ç±»åˆ«ï¼Œæ¨¡å‹ä¹Ÿæ›´å€¾å‘äºè¾“å‡ºå¤šæ•°ç±»åˆ«ï¼Œä»è€Œç‰ºç‰²äº†å°‘æ•°ç±»åˆ«çš„å¬å›ç‡å’Œæ•´ä½“å‡†ç¡®æ€§ã€‚</p>
<p>è¿‘å› åå·®æ˜¯æŒ‡åœ¨å°‘æ ·æœ¬å­¦ä¹ çš„æç¤ºä¸­ï¼Œæ¨¡å‹ä¼šå€¾å‘äºé¢„æµ‹åœ¨æç¤ºæœ«å°¾ï¼ˆæˆ–æœ€è¿‘ï¼‰å‡ºç°çš„ç±»åˆ«ã€‚å³ä½¿æç¤ºä¸­çš„ç±»åˆ«åˆ†å¸ƒæ˜¯å¹³è¡¡çš„ï¼Œä»…ä»…æ”¹å˜ç¤ºä¾‹çš„é¡ºåºï¼Œä¹Ÿä¼šæ˜¾è‘—æ”¹å˜æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œå¯¼è‡´é¢„æµ‹çš„é«˜æ–¹å·®ï¼ˆä¸ç¨³å®šï¼‰ã€‚</p>
<p>Calibration ï¼ˆä¿®æ­£ï¼‰of few-shot prediction</p>
<script type="math/tex; mode=display">\mathbf{\hat{q}} = \text{softmax}(\mathbf{W}\mathbf{\hat{p}} + \mathbf{b})</script><ul>
<li>$\mathbf{\hat{p}}$ï¼šæ¨¡å‹ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ LLMï¼‰åŸå§‹é¢„æµ‹çš„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒå‘é‡ã€‚</li>
<li>$\mathbf{W}$ï¼šæƒé‡çŸ©é˜µã€‚</li>
<li>$\mathbf{b}$ï¼šåç½®å‘é‡ (bias)ã€‚</li>
<li>$\mathbf{\hat{q}}$ï¼šæ ¡å‡†åçš„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒå‘é‡ã€‚</li>
</ul>
<p>åˆ©ç”¨æ¨¡å‹å¯¹â€œç©ºè¾“å…¥â€çš„é¢„æµ‹ $\text{prediction}_{\text{null}}$ æ¥æŠµæ¶ˆå…¶å›ºæœ‰çš„åå·®ã€‚</p>
<ul>
<li><strong>è®¾ç½® $\mathbf{b}$ï¼š</strong> å°†åç½®å‘é‡ $\mathbf{b}$ è®¾ä¸º $\mathbf{0}$ã€‚</li>
<li><strong>è®¾ç½® $\mathbf{W}$ï¼š</strong> å°†æƒé‡çŸ©é˜µ $\mathbf{W}$ è®¾ä¸ºä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå…¶å¯¹è§’çº¿å…ƒç´ æ˜¯ç©ºè¾“å…¥é¢„æµ‹æ¦‚ç‡çš„å€’æ•°ã€‚<script type="math/tex; mode=display">\mathbf{W} = \text{diag}(\text{prediction}_{\text{null}})^{-1}</script>è¿™æ˜¯å¸Œæœ›æœ€åç©ºè¾“å…¥çš„è¾“å‡ºåˆ†å¸ƒæ˜¯å‡åŒ€çš„</li>
</ul>
<h4 id="Induction-attention-head-for-repetition"><a href="#Induction-attention-head-for-repetition" class="headerlink" title="Induction attention head: for repetition"></a>Induction attention head: for repetition</h4><p>â€œå½’çº³æ³¨æ„åŠ›å¤´â€ä¸æ˜¯ Transformer æ¶æ„ä¸­é¢„è®¾çš„ç»„ä»¶ï¼Œè€Œæ˜¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªå‘å­¦ä¹ åˆ°çš„ã€ç”±ä¸€ä¸ªæˆ–å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼ˆé€šå¸¸æ˜¯ä¸¤ä¸ªå¤´åœ¨ä¸åŒå±‚ä¸­åä½œï¼‰ç»„æˆçš„åŠŸèƒ½æ€§ç”µè·¯ï¼ˆCircuitï¼‰ã€‚</p>
<p>å‡è®¾è¾“å…¥åºåˆ—æ˜¯ï¼šâ€¦[A][B]â€¦[A]</p>
<p>å½“æ¨¡å‹å¤„ç†ç¬¬äºŒä¸ª [A] æ—¶ï¼Œå½’çº³å¤´ä¼šæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š</p>
<p>åŒ¹é…ï¼ˆMatchï¼‰ï¼š å®ƒä¼šå›é¡¾åºåˆ—ï¼Œæ‰¾åˆ°ä¸Šä¸€æ¬¡å‡ºç° [A] çš„ä½ç½®ã€‚</p>
<p>å¤åˆ¶/é¢„æµ‹ï¼ˆCopy/Predictï¼‰ï¼š å®ƒä¼šæŸ¥çœ‹ä¸Šä¸€æ¬¡ [A] åé¢ç´§è·Ÿç€çš„æ ‡è®° [B]ï¼Œå¹¶åˆ©ç”¨è¿™ä¸ªä¿¡æ¯æ¥é¢„æµ‹å½“å‰ç¬¬äºŒä¸ª [A] åé¢ä¹Ÿåº”è¯¥è·Ÿç€ [B]ã€‚</p>
<p>ç®€å•æ¥è¯´ï¼Œå®ƒèƒ½å‘ç°å¹¶åº”ç”¨åºåˆ—ä¸­é‡å¤å‡ºç°çš„ [A] â†’ [B] æ¨¡å¼ã€‚</p>
<h1 id="Instruction-tuning"><a href="#Instruction-tuning" class="headerlink" title="Instruction tuning"></a>Instruction tuning</h1><p>motivation: we are lazy, å¸Œæœ›zero-shot promptingï¼ˆä¸ç»™ä¾‹å­ï¼‰</p>
<h2 id="FLAN-Finetuned-Language-Net"><a href="#FLAN-Finetuned-Language-Net" class="headerlink" title="FLAN (Finetuned Language Net)"></a>FLAN (Finetuned Language Net)</h2><p> Simple idea: After pretraining, we finetune the language model on a good amount of<br>â€œinstruction followingâ€ data.</p>
<p> Each training samples contains the task description, an input, and the target output.</p>
<p>During evaluation, we hope the model can generalize to unseen task type.</p>
<p>FLAN data construction: Collected data from 62<br>existing NLP tasks. For each task, manually<br>compose ten unique templates<br>(for diversity) that use natural<br>language instructions to<br>describe the task.</p>
<p><img src="nlp=comp.png" alt=""></p>
<h1 id="Alignment-with-reinforcement-learning-human-feed-back-RLHF"><a href="#Alignment-with-reinforcement-learning-human-feed-back-RLHF" class="headerlink" title="Alignment with reinforcement learning human feed back (RLHF)"></a>Alignment with reinforcement learning human feed back (RLHF)</h1><p>We collect samples<br>from the model, and ask<br>labelers to rank them.<br>These ranks are used to<br>train the reward model</p>
<p>This reward<br>model is<br>used for RL.</p>
<h2 id="Why-is-it-practical"><a href="#Why-is-it-practical" class="headerlink" title="Why is it practical?"></a>Why is it practical?</h2><p>â€¢ Itâ€™s also easier for the human labeler to rank the responses, than coming up<br>with a better response.</p>
<p>â€¢ From pretraining, the LLM might be strong enough to give a good sample<br>when you sample enough times.</p>
<h2 id="what-could-be-its-advantage"><a href="#what-could-be-its-advantage" class="headerlink" title="what could be its advantage"></a>what could be its advantage</h2><p>(comparing to, say, more supervised finetuning on high-quality data)?</p>
<p>â€¢ Itâ€™s usually easier to train a good discriminator than a good generator<br>(especially now that we can use base the reward model on an existing LLM).</p>
<p>â€¢ By giving low reward, we are teaching the model â€œwhat not to sayâ€ by<br>sampling from it.</p>
<h2 id="è®©æœºå™¨ç”Ÿæˆæ›´ç¬¦åˆæˆ‘ä»¬éœ€æ±‚çš„å›ç­”"><a href="#è®©æœºå™¨ç”Ÿæˆæ›´ç¬¦åˆæˆ‘ä»¬éœ€æ±‚çš„å›ç­”" class="headerlink" title="è®©æœºå™¨ç”Ÿæˆæ›´ç¬¦åˆæˆ‘ä»¬éœ€æ±‚çš„å›ç­”"></a>è®©æœºå™¨ç”Ÿæˆæ›´ç¬¦åˆæˆ‘ä»¬éœ€æ±‚çš„å›ç­”</h2><p>Trivial method: Promptingï¼ˆDirectly prompt the LM to alignï¼‰</p>
<p>Pros: Training-free;</p>
<p>Cons: No guarantee that the model will precisely follow, and requires<br>careful prompt design</p>
<p>Best-of-N</p>
<p>1) Samples multiple solutions;<br>2) Chooses the one with the highest score given by a reward model.</p>
<p>Pros: Do not need to train the policy model, simple and powerful;</p>
<p>Cons: not efficient and you might need a large N</p>
<h2 id="objective"><a href="#objective" class="headerlink" title="objective"></a>objective</h2><script type="math/tex; mode=display">\max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(\cdot|x)} \left[ r_{\phi}(x, y) \right] - \beta \mathbb{D}_{\text{KL}} \left[ \pi_{\theta}(\cdot | x) \parallel \pi_{\text{ref}}(\cdot | x) \right]</script><script type="math/tex; mode=display">\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(\cdot|x)} \left[ r_{\phi}(x, y) - \beta (\log \pi_{\theta}(y|x) - \log \pi_{\text{ref}}(y|x)) \right]</script><p>å¥–åŠ±æ¨¡å‹ $\phi$ , å‰ä¸€é¡¹æ˜¯ä¸ºäº†æé«˜å¥–åŠ±</p>
<p>åä¸€é¡¹é˜²æ­¢ä¼˜åŒ–åçš„æ¨¡å‹ $\pi<em>{\theta}$ <strong>åç¦»å¤ªè¿œ</strong>ï¼ˆdeviating too farï¼‰äºåˆå§‹å‚è€ƒæ¨¡å‹ $\pi</em>{\text{ref}}$<br>ï¼ˆReward over-optimization issueï¼‰<br>The reward model is an<br>imperfect proxy, optimizing its<br>value too much can hinder<br>ground truth performance (first<br>increase, then decrease).</p>
<p>$\pi<em>{\theta}(\cdot | x)$ å’Œ $\pi</em>{\text{ref}}(\cdot | x)$å«ä¹‰ï¼š å®ƒä»¬ä»£è¡¨åœ¨ç»™å®šè¾“å…¥ $x$ çš„æ¡ä»¶ä¸‹ï¼Œæ‰€æœ‰å¯èƒ½çš„è¾“å‡º $y$ ä¸Šçš„å®Œæ•´æ¦‚ç‡åˆ†å¸ƒã€‚</p>
<p>$\pi<em>{\theta}(y|x)$ å’Œ $\pi</em>{\text{ref}}(y|x)$å«ä¹‰ï¼š å®ƒä»¬ä»£è¡¨åœ¨ç»™å®šè¾“å…¥ $x$ çš„æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹ç”Ÿæˆç‰¹å®šå›å¤ $y$ çš„æ¦‚ç‡ã€‚</p>
<p>syntheticï¼ˆäººé€ çš„ï¼‰ setting for the Gold modelï¼šå®é™…ä¸Šå¹¶æ²¡æœ‰çœŸçš„ç”¨äººç±»æ ‡è®°ï¼Œè€Œæ˜¯ä½¿ç”¨å¤§æ¨¡å‹æ ‡è®°</p>
<h2 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">ç®—æ³•</th>
<th style="text-align:left">å¯¹åº”è¡Œä¸º</th>
<th style="text-align:left">ç»“æœ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>ç­–ç•¥æ¢¯åº¦ (PG)</strong></td>
<td style="text-align:left">å¸æœºéå¸¸æ¿€è¿›ï¼Œä¸€è„šæ²¹é—¨åˆ°åº•æˆ–ä¸€è„šåˆ¹è½¦è¸©æ­»ã€‚</td>
<td style="text-align:left"><strong>æ–¹å·®å¤ªå¤§ (Variances are too high)</strong>ï¼Œå®¹æ˜“å¯¼è‡´è®­ç»ƒä¸ç¨³å®šç”šè‡³å´©æºƒã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>TRPO</strong></td>
<td style="text-align:left">å¸æœºçŸ¥é“è¦æ¸©å’Œé©¾é©¶ï¼Œä½†æ¯æ¬¡å¯åŠ¨å‰éƒ½è¦ç”¨å¤æ‚çš„æ•°å­¦å…¬å¼ç²¾ç¡®è®¡ç®—æ–¹å‘ç›˜è½¬è§’å’Œæ²¹é—¨æ·±åº¦ã€‚</td>
<td style="text-align:left"><strong>å®‰å…¨ç¨³å®š</strong>ï¼Œä½†<strong>å®ç°æå…¶å¤æ‚</strong>ï¼Œè®¡ç®—æˆæœ¬é«˜ã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>PPO-Clip</strong></td>
<td style="text-align:left">å¸æœºå­¦äº†ä¸€ä¸ªç®€å•çš„â€œå®‰å…¨è§„åˆ™â€ï¼š<strong>å¦‚æœå½“å‰æ“ä½œè¢«è®¤ä¸ºå¾ˆå¥½ï¼Œå°±é¼“åŠ±ä»–ç»§ç»­ï¼Œä½†ä¸èƒ½è¶…è¿‡ä¸€ä¸ªå›ºå®šçš„é™åº¦ã€‚å¦‚æœæ“ä½œä¸å¥½ï¼Œå°±é™åˆ¶ä»–åˆ«åšå¾—å¤ªå·®ã€‚</strong></td>
<td style="text-align:left"><strong>å®‰å…¨ä¸”é«˜æ•ˆã€‚</strong> å®ƒç”¨ä¸€ä¸ªç®€å•çš„â€œæˆªæ–­â€æœºåˆ¶ï¼Œè¾¾åˆ°äº†ä¸ TRPO ç›¸ä¼¼çš„ç¨³å®šæ•ˆæœï¼Œä½†é¿å…äº†å¤æ‚çš„è®¡ç®—ã€‚</td>
</tr>
</tbody>
</table>
</div>
<p>PPO çš„ç›´è§‰å°±ä½“ç°åœ¨å®ƒçš„ <strong>CLIP ç›®æ ‡å‡½æ•°</strong>ä¸­ï¼š</p>
<script type="math/tex; mode=display">L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left( p_t(\theta) \hat{A}_t, \text{clip}(p_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]</script><ol>
<li><strong>$p_t(\theta)$ (æ–°æ—§ç­–ç•¥çš„æ¯”ç‡)</strong>ï¼šä»£è¡¨æ–°ç­–ç•¥ $\pi<em>\theta$ ç›¸å¯¹äºæ—§ç­–ç•¥ $\pi</em>{\theta_{old}}$ <strong>â€œå˜åŠ¨äº†å¤šå°‘â€</strong>ã€‚</li>
<li><strong>ä¼˜åŠ¿å‡½æ•° $\hat{A}_t$</strong>ï¼šä»£è¡¨å½“å‰åŠ¨ä½œ $a_t$ çš„<strong>å¥½åç¨‹åº¦</strong>ã€‚</li>
<li><strong>æ ¸å¿ƒç›´è§‰ï¼ˆæœ€å°åŒ–å’Œæˆªæ–­ï¼‰ï¼š</strong><ul>
<li><strong>å½“ $\hat{A}_t$ ä¸ºæ­£ï¼ˆåŠ¨ä½œå¥½ï¼‰</strong>ï¼šPPO æƒ³è¦æé«˜è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼ˆå¢å¤§ $p_t(\theta)$ï¼‰ï¼Œä½† $\min$ å‡½æ•°ä¼šç¡®ä¿è¿™ä¸ªæ¯”ç‡<strong>ä¸ä¼šè¶…è¿‡ $1+\epsilon$</strong>ã€‚è¿™å°±å¥½åƒåœ¨è¯´ï¼šâ€œä½ åšå¾—å¾ˆå¥½ï¼Œä½†æˆ‘åªå¥–åŠ±ä½ åˆ°è¿™ä¸ªç¨‹åº¦ï¼Œé˜²æ­¢ä½ å¤ªå¾—æ„å¿˜å½¢ï¼ŒæŠŠç­–ç•¥æ”¹å¾—é¢ç›®å…¨éã€‚â€</li>
<li><strong>å½“ $\hat{A}_t$ ä¸ºè´Ÿï¼ˆåŠ¨ä½œå·®ï¼‰</strong>ï¼šPPO æƒ³è¦é™ä½è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼ˆå‡å° $p_t(\theta)$ï¼‰ï¼Œä½† $\min$ å‡½æ•°ä¼šç¡®ä¿è¿™ä¸ªæ¯”ç‡<strong>ä¸ä¼šä½äº $1-\epsilon$</strong>ã€‚è¿™å°±å¥½åƒåœ¨è¯´ï¼šâ€œä½ åšå¾—å¾ˆå·®ï¼Œæˆ‘æƒ©ç½šä½ ï¼Œä½†æƒ©ç½šä¸èƒ½å¤ªé‡ï¼Œé˜²æ­¢ä½ ä¸€æ¬¡æ€§æŠŠç­–ç•¥æ”¹é”™ã€‚â€</li>
</ul>
</li>
</ol>
<p>PPOçš„é—®é¢˜ï¼š too much hyper parameters</p>
<h2 id="DPO"><a href="#DPO" class="headerlink" title="DPO"></a>DPO</h2><p>Advantage: We no longer need a reward model or a value model.</p>
<script type="math/tex; mode=display">\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_{\theta}(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right].</script><p>$y_w$ï¼šæ¨¡å‹å¯¹ $x$ ç”Ÿæˆçš„<strong>è¢«é€‰æ‹©/åå¥½ (winner)</strong> çš„å›å¤ã€‚$y_l$ï¼šæ¨¡å‹å¯¹ $x$ ç”Ÿæˆçš„<strong>è¢«æ‹’ç»/ä¸åå¥½ (loser)</strong> çš„å›å¤ã€‚</p>
<script type="math/tex; mode=display">\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}}) = -\beta \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \sigma(\hat{r}_{\theta}(x, y_l) - \hat{r}_{\theta}(x, y_w)) \left[ \nabla_{\theta} \log \pi_{\theta}(y_w | x) - \nabla_{\theta} \log \pi_{\theta}(y_l | x) \right] \right].</script><h4 id="A-ç­–ç•¥æ›´æ–°æ–¹å‘-Action-Term"><a href="#A-ç­–ç•¥æ›´æ–°æ–¹å‘-Action-Term" class="headerlink" title="A. ç­–ç•¥æ›´æ–°æ–¹å‘ (Action Term)"></a>A. ç­–ç•¥æ›´æ–°æ–¹å‘ (Action Term)</h4><script type="math/tex; mode=display">\left[ \nabla_{\theta} \log \pi_{\theta}(y_w | x) - \nabla_{\theta} \log \pi_{\theta}(y_l | x) \right]</script><ul>
<li>è¿™æ˜¯<strong>æ¢¯åº¦ä¸Šå‡</strong>æ–¹å‘</li>
</ul>
<h4 id="B-å¥–åŠ±ä¼°è®¡-Estimated-Reward"><a href="#B-å¥–åŠ±ä¼°è®¡-Estimated-Reward" class="headerlink" title="B. å¥–åŠ±ä¼°è®¡ (Estimated Reward)"></a>B. å¥–åŠ±ä¼°è®¡ (Estimated Reward)</h4><script type="math/tex; mode=display">\hat{r}_{\theta}(x, y) = \beta \log \frac{\pi_{\theta}(y | x)}{\pi_{\text{ref}}(y | x)}</script><ul>
<li>DPO çš„å…³é”®åœ¨äºï¼Œå®ƒ<strong>éšå«åœ°</strong>å°†å¥–åŠ±æ¨¡å‹ $r(x, y)$ æ›¿æ¢æˆäº†ç­–ç•¥æ¨¡å‹ $\pi<em>{\theta}$ ç›¸å¯¹äºå‚è€ƒæ¨¡å‹ $\pi</em>{\text{ref}}$ çš„<strong>å¯¹æ•°æ¦‚ç‡æ¯”</strong>ï¼Œå†ä¹˜ä»¥ $\beta$ã€‚è¿™ä¸ª $\hat{r}<em>{\theta}(x, y)$ è¢«ç§°ä¸º<strong>ç»éªŒå¥–åŠ± (empirical reward)</strong>ï¼Œå®ƒæ˜¯ç­–ç•¥ $\pi</em>{\theta}$ åœ¨å½“å‰å‚æ•°ä¸‹å¯¹ $y$ çš„å¥–åŠ±ä¼°è®¡ã€‚</li>
</ul>
<h4 id="C-æƒé‡é¡¹-Weight-Term"><a href="#C-æƒé‡é¡¹-Weight-Term" class="headerlink" title="C. æƒé‡é¡¹ (Weight Term)"></a>C. æƒé‡é¡¹ (Weight Term)</h4><script type="math/tex; mode=display">\sigma(\hat{r}_{\theta}(x, y_l) - \hat{r}_{\theta}(x, y_w))</script><ul>
<li>å¦‚æœ<strong>å½“å‰ç­–ç•¥ $\pi_{\theta}$ å·²ç»æ­£ç¡®åœ°æ•æ‰äº†åå¥½</strong>ï¼ˆå³ $\hat{r}<em>{\theta}(x, y_w) &gt; \hat{r}</em>{\theta}(x, y<em>l)$ï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸ªå¥–åŠ±å·®å¼‚ $(\hat{r}</em>{\theta}(x, y<em>l) - \hat{r}</em>{\theta}(x, y_w))$ æ˜¯ä¸€ä¸ª<strong>è¾ƒå¤§çš„è´Ÿæ•°</strong>ã€‚Sigmoid å‡½æ•° $\sigma(\text{å¤§è´Ÿæ•°})$ æ¥è¿‘ <strong>0</strong>ï¼Œå› æ­¤<strong>æƒé‡å¾ˆå°</strong>ã€‚</li>
<li>å¦‚æœ<strong>å½“å‰ç­–ç•¥ $\pi_{\theta}$ é”™è¯¯åœ°æ•æ‰äº†åå¥½</strong>ï¼ˆå³ $\hat{r}<em>{\theta}(x, y_w) &lt; \hat{r}</em>{\theta}(x, y_l)$ï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸ªå¥–åŠ±å·®å¼‚æ˜¯ä¸€ä¸ª<strong>æ­£æ•°</strong>ã€‚Sigmoid å‡½æ•° $\sigma(\text{æ­£æ•°})$ æ¥è¿‘ <strong>1</strong> æˆ– <strong>0.5</strong> ä»¥ä¸Šï¼Œå› æ­¤<strong>æƒé‡å¾ˆå¤§</strong>ã€‚</li>
</ul>
<h3 id="âœ¨-è¿›ä¸€æ­¥çš„ç›´è§‰ï¼šæ­£åˆ™åŒ–-Regularization-Intuition"><a href="#âœ¨-è¿›ä¸€æ­¥çš„ç›´è§‰ï¼šæ­£åˆ™åŒ–-Regularization-Intuition" class="headerlink" title="âœ¨ è¿›ä¸€æ­¥çš„ç›´è§‰ï¼šæ­£åˆ™åŒ– (Regularization Intuition)"></a>âœ¨ è¿›ä¸€æ­¥çš„ç›´è§‰ï¼šæ­£åˆ™åŒ– (Regularization Intuition)</h3><p> $\beta [\log \pi<em>{\theta}(y_l|x) - \log \pi</em>{\theta}(y<em>w|x)] - \beta [\log \pi</em>{\text{ref}}(y<em>l|x) - \log \pi</em>{\text{ref}}(y_w|x)]$</p>
<p>è¿™ä¸ªè¡¨è¾¾å¼å®é™…ä¸Šæ˜¯ï¼š</p>
<script type="math/tex; mode=display">\underbrace{\beta (\log \frac{\pi_{\theta}(y_l|x)}{\pi_{\theta}(y_w|x)})}_{\text{ç­–ç•¥ } \pi_{\theta} \text{çš„ log-prob å·®å¼‚}} - \underbrace{\beta (\log \frac{\pi_{\text{ref}}(y_l|x)}{\pi_{\text{ref}}(y_w|x)})}_{\text{å‚è€ƒ } \pi_{\text{ref}} \text{çš„ log-prob å·®å¼‚}}</script><ul>
<li><strong>DPO ç›®æ ‡ï¼š</strong> DPO æŸå¤±çš„ç›®æ ‡æ˜¯è®©ç­–ç•¥ $\pi<em>{\theta}$ å¯¹ $y_w$ å’Œ $y_l$ çš„å¯¹æ•°æ¦‚ç‡<strong>å·®å¼‚</strong> ($\log \pi</em>{\theta}(y<em>w|x) - \log \pi</em>{\theta}(y_l|x)$) <strong>å¢å¤§</strong>ï¼ˆå³è®© $y_w$ æ¯” $y_l$ æ›´å¯èƒ½å‡ºç°ï¼‰ã€‚</li>
<li><strong>æ­£åˆ™åŒ–ä½œç”¨ï¼š</strong> DPO æŸå¤±è¦æ±‚ $\pi<em>{\theta}$ çš„è¿™ä¸ªå·®å¼‚<strong>ä¸ä»…è¦å¤§</strong>ï¼Œè€Œä¸”è¦ç›¸å¯¹äº<strong>å‚è€ƒæ¨¡å‹</strong> $\pi</em>{\text{ref}}$ çš„<strong>åŸå§‹å·®å¼‚</strong>è¿›è¡Œè°ƒæ•´ã€‚</li>
<li><strong>ç»“è®ºï¼š</strong> DPO å®é™…ä¸Šæ˜¯åœ¨<strong>æ­£åˆ™åŒ–</strong>ç­–ç•¥ $\pi<em>{\theta}$ çš„å¯¹æ•°æ¦‚ç‡å·®å¼‚ï¼Œä½¿å…¶åœ¨æ‹Ÿåˆäººç±»åå¥½çš„åŒæ—¶ï¼Œ<strong>ä¸ä¼šè¿‡åº¦åç¦»</strong>åŸºç¡€æ¨¡å‹ $\pi</em>{\text{ref}}$ çš„è¡Œä¸ºã€‚è¿™ä¸ RLHF-PPO ä¸­ä½¿ç”¨ KL æ•£åº¦è¿›è¡Œæ­£åˆ™åŒ–æœ‰ç›¸ä¼¼çš„ç›®çš„ï¼Œä½† DPO å°†è¿™ä¸ªæ­£åˆ™åŒ–ç›´æ¥åµŒå…¥åˆ°äº†<strong>æŸå¤±å‡½æ•°</strong>çš„å®šä¹‰ä¸­ã€‚</li>
</ul>
<p>DPO é€šè¿‡ç»•è¿‡ å¥–åŠ±æ¨¡å‹ (Reward Model) å’Œ å¼ºåŒ–å­¦ä¹  (RL) æ­¥éª¤ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´ç®€å•ã€æ›´ç¨³å®šã€‚<br>RLHF-PPO ç”±äºå¼•å…¥äº†å¤æ‚çš„å¼ºåŒ–å­¦ä¹ å’Œå¥–åŠ±æ¨¡å‹ï¼Œè¢«è®¤ä¸ºæœ‰æ›´å¤§çš„æ½œåŠ›ï¼ˆå¯èƒ½åœ¨å¤æ‚çš„å¯¹é½ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼‰ã€‚</p>
<h1 id="Mixture-of-Experts-Model"><a href="#Mixture-of-Experts-Model" class="headerlink" title="Mixture of Experts Model"></a>Mixture of Experts Model</h1><p>Idea of conditional computation:<br> We still build a super big model, but<br>we only selectively activate a relevant<br>portion for each training sample.</p>
<p> MoE is natural for model parallel</p>
<script type="math/tex; mode=display">
G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k))</script><script type="math/tex; mode=display">
\text{KeepTopK}(v, k)_i = \begin{cases} v_i & \text{if } v_i \text{ is in the top } k \text{ elements of } v \\ -\infty & \text{otherwise} \end{cases}</script><script type="math/tex; mode=display">
H(x)_i = (x \cdot W_{g})_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i)</script><ul>
<li><strong>æ ¸å¿ƒçº¿æ€§å˜æ¢ï¼š</strong> $(x \cdot W<em>{g})_i$ æ˜¯å¯¹è¾“å…¥ $x$ åº”ç”¨æƒé‡çŸ©é˜µ $W</em>{g}$ åçš„ç¬¬ $i$ ä¸ªå…ƒç´ ã€‚è¿™æ˜¯è®¡ç®—ä¸“å®¶ $i$ åˆå§‹å¾—åˆ†çš„åŸºç¡€ã€‚</li>
<li><strong>å™ªå£°é¡¹ï¼š</strong> é˜²æ­¢ä¸“å®¶è¿‡åº¦ä¸“ä¸šåŒ– (Over-Specialization) æˆ–åå¡Œ (Collapse):é—¨æ§ç½‘ç»œ $G(x)$ å€¾å‘äºå°†ç›¸ä¼¼çš„è¾“å…¥æŒç»­è·¯ç”±åˆ°å¾—åˆ†æœ€é«˜çš„å°‘æ•°ä¸“å®¶ã€‚è¿™ä¼šå¯¼è‡´è¿™äº›å°‘æ•°ä¸“å®¶è¢«è¿‡åº¦ä½¿ç”¨ (over-utilized)ï¼Œè€Œå…¶ä»–å¤§å¤šæ•°ä¸“å®¶åˆ™åˆ©ç”¨ä¸è¶³ (under-utilized)ï¼Œå‚æ•°æ›´æ–°å°‘ï¼Œå½¢åŒè™šè®¾ã€‚è¿™ç§ä¸å¹³è¡¡ä¹Ÿè¢«ç§°ä¸ºâ€œè·¯ç”±åå¡Œâ€ (Routing Collapse)ã€‚</li>
</ul>
<h2 id="Balancing-loads-between-experts"><a href="#Balancing-loads-between-experts" class="headerlink" title="Balancing loads between experts"></a>Balancing loads between experts</h2><script type="math/tex; mode=display">\text{Importance}(X) = \sum_{x \in X} G(x) \quad \text{(6)}</script><ul>
<li>$G(x)$ æ˜¯æŒ‡<strong>é—¨æ§å€¼ï¼ˆgate valueï¼‰</strong>ã€‚åœ¨MoEæ¨¡å‹ä¸­ï¼Œé—¨æ§ç½‘ç»œä¼šä¸ºæ¯ä¸ªè¾“å…¥ $x$ è¾“å‡ºä¸€ä¸ªåˆ†å¸ƒï¼Œå†³å®šå°†è¯¥è¾“å…¥åˆ†é…ç»™å“ªä¸ªä¸“å®¶ã€‚</li>
<li>$\text{Importance}(X)$ æ˜¯è¯¥ä¸“å®¶åœ¨æ•´ä¸ªæ‰¹æ¬¡ $X$ ä¸­æ‰€æœ‰æ ·æœ¬<strong>é—¨æ§å€¼ï¼ˆä½¿ç”¨æ¦‚ç‡ï¼‰çš„æ€»å’Œ</strong>ã€‚è¿™ä¸ªå€¼è¡¡é‡äº†è¯¥ä¸“å®¶åœ¨å½“å‰æ‰¹æ¬¡ä¸­è¢«æ¿€æ´»å’Œä½¿ç”¨çš„ç¨‹åº¦ã€‚</li>
</ul>
<p>è´Ÿè½½å‡è¡¡æŸå¤± $\mathcal{L}_{\text{importance}}$ æ˜¯æ ¹æ®æ‰€æœ‰ä¸“å®¶çš„é‡è¦æ€§å€¼é›†åˆè®¡ç®—å¾—å‡ºçš„ï¼š</p>
<script type="math/tex; mode=display">\mathcal{L}_{\text{importance}}(X) = w_{\text{importance}} \cdot \text{CV}(\text{Importance}(X))^2 \quad \text{(7)}</script><ul>
<li>$\text{Importance}(X)$ æ­¤æ—¶æ˜¯ä¸€ä¸ª<strong>å‘é‡</strong>ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¸“å®¶å„è‡ªçš„ $\text{Importance}$ å€¼ã€‚</li>
<li>$\text{CV}(\cdot)$ æ˜¯<strong>å˜å¼‚ç³»æ•°ï¼ˆCoefficient of Variationï¼‰</strong>ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š<script type="math/tex; mode=display">\text{CV} = \text{std}/\text{mean}</script>  å³ï¼š$\text{CV} = \text{æ ‡å‡†å·®} / \text{å‡å€¼}$ã€‚</li>
<li><p>$w_{\text{importance}}$ æ˜¯ä¸€ä¸ª<strong>æ‰‹åŠ¨è°ƒæ•´çš„ç¼©æ”¾å› å­ï¼ˆscaling factorï¼‰</strong>ï¼Œç”¨äºæ§åˆ¶æ­¤æŸå¤±åœ¨æ€»æŸå¤±ä¸­çš„æƒé‡å’Œå½±å“åŠ›ã€‚</p>
<p>æœ€å°åŒ– $\mathcal{L}_{\text{importance}}$ æ„å‘³ç€æœ€å°åŒ– $\text{CV}(\text{Importance}(X))$ã€‚ç”±äº $CV$ è¶Šå°è¡¨ç¤ºæ•°æ®è¶Šé›†ä¸­ï¼Œå› æ­¤è¿™é¼“åŠ±æ‰€æœ‰ä¸“å®¶å…·æœ‰è¿‘ä¼¼ç›¸ç­‰çš„â€œé‡è¦æ€§â€ï¼Œä»è€Œå®ç°äº†ä¸“å®¶é—´çš„å‡è¡¡è´Ÿè½½ã€‚</p>
</li>
</ul>
<h2 id="æ›´ç®€å•çš„è´Ÿè½½å‡è¡¡æŸå¤±â€è§£æ"><a href="#æ›´ç®€å•çš„è´Ÿè½½å‡è¡¡æŸå¤±â€è§£æ" class="headerlink" title="æ›´ç®€å•çš„è´Ÿè½½å‡è¡¡æŸå¤±â€è§£æ"></a>æ›´ç®€å•çš„è´Ÿè½½å‡è¡¡æŸå¤±â€è§£æ</h2><script type="math/tex; mode=display">\text{loss} = \alpha \cdot \sum_{i=1}^{N} f_i \cdot P_i \quad \text{(4)}</script><p>åœ¨è¿™é‡Œï¼Œ$p$ æŒ‡çš„æ˜¯ <strong>è·¯ç”±å™¨æ¦‚ç‡ï¼ˆRouter Probabilityï¼‰</strong>ï¼Œä¹Ÿç§°ä¸º <strong>é—¨æ§æ¦‚ç‡ï¼ˆGate Probabilityï¼‰</strong>ã€‚</p>
<ul>
<li><strong>$p_i(x)$ï¼š</strong> æ˜¯æŒ‡é—¨æ§ç½‘ç»œï¼ˆRouterï¼‰å¯¹è¾“å…¥ $x$ è®¡ç®—å¾—åˆ°çš„ã€å°†å…¶åˆ†æ´¾ç»™<strong>ç¬¬ $i$ ä¸ªä¸“å®¶</strong>çš„æ¦‚ç‡ã€‚<ul>
<li>åœ¨MoEæ¨¡å‹ä¸­ï¼Œé—¨æ§ç½‘ç»œé€šå¸¸ä¼šè¾“å‡ºä¸€ä¸ª <strong>$N$ ç»´çš„æ¦‚ç‡å‘é‡</strong> $p(x)$ï¼Œå…¶ä¸­ $N$ æ˜¯ä¸“å®¶æ•°é‡ï¼Œ$\sum_{i=1}^{N} p_i(x) = 1$ã€‚</li>
</ul>
</li>
<li><strong>$P_i$ (å…¬å¼ 6)ï¼š</strong> æ˜¯æŒ‡åœ¨æ•´ä¸ªæ‰¹æ¬¡ $B$ ä¸­ï¼Œåˆ†é…ç»™<strong>ç¬¬ $i$ ä¸ªä¸“å®¶</strong>çš„<strong>æ¦‚ç‡çš„å¹³å‡å€¼</strong>ã€‚<script type="math/tex; mode=display">P_i = \frac{1}{T} \sum_{x \in B} p_i(x) \quad</script></li>
</ul>
<p>$f_i$ è¡¨ç¤ºåœ¨å½“å‰æ‰¹æ¬¡ $B$ ä¸­<strong>å®é™…è¢«åˆ†æ´¾ï¼ˆhard-routedï¼‰ç»™ä¸“å®¶ $i$ çš„ tokens çš„æ¯”ä¾‹</strong>ï¼ˆActual Usageï¼‰ï¼š</p>
<script type="math/tex; mode=display">f_i = \frac{1}{T} \sum_{x \in B} \mathbb{I}\{\operatorname{argmax} p(x) = i\} \quad</script><ul>
<li>$\operatorname{argmax} p(x) = i$ è¡¨ç¤º $p_i(x)$ æ˜¯æ‰€æœ‰ä¸“å®¶ä¸­æ¦‚ç‡æœ€å¤§çš„ï¼Œå³æ ·æœ¬ $x$ æœ€ç»ˆè¢«ç¡®å®šåˆ†æ´¾ç»™äº†ä¸“å®¶ $i$ã€‚</li>
<li>$f_i$ è¡¡é‡çš„æ˜¯ä¸“å®¶ $i$ åœ¨å½“å‰æ‰¹æ¬¡ä¸­ <strong>â€œç¡¬æ€§â€å¤„ç†çš„å®é™…å·¥ä½œé‡</strong>ã€‚</li>
</ul>
<p>æƒ©ç½šå°‘æ•°ä¸“å®¶è¢«è¿‡åº¦ä½¿ç”¨</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">ç‰¹æ€§</th>
<th style="text-align:left"><strong>$f_i$ï¼ˆå®é™…ä½¿ç”¨ç‡ - Actual Usageï¼‰</strong></th>
<th style="text-align:left"><strong>$P_i$ï¼ˆæ¦‚ç‡åˆ†é…ç‡ - Allocated Probabilityï¼‰</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>å…¬å¼</strong></td>
<td style="text-align:left"><script type="math/tex">f_i = \frac{1}{T} \sum_{x \in B} \mathbb{I}\{\operatorname{argmax} p(x) = i\}</script></td>
<td style="text-align:left"><script type="math/tex">P_i = \frac{1}{T} \sum_{x \in B} p_i(x)</script></td>
</tr>
<tr>
<td style="text-align:left"><strong>å«ä¹‰</strong></td>
<td style="text-align:left"><strong>ç¡¬æ€§é€‰æ‹©ç»“æœ</strong>ã€‚å®é™…è¢«åˆ†æ´¾ç»™ä¸“å®¶ $i$ çš„ tokens å æ€» tokens æ•° $T$ çš„æ¯”ä¾‹ã€‚</td>
<td style="text-align:left"><strong>è½¯æ€§æ¦‚ç‡å‡å€¼</strong>ã€‚é—¨æ§ç½‘ç»œåˆ†é…ç»™ä¸“å®¶ $i$ çš„æ¦‚ç‡ $p_i(x)$ åœ¨æ•´ä¸ªæ‰¹æ¬¡ $B$ ä¸­çš„å¹³å‡å€¼ã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>è®¡ç®—åŸºå‡†</strong></td>
<td style="text-align:left">åŸºäº <strong>$\operatorname{argmax}$</strong> è¿ç®—ï¼šåªå…³å¿ƒå“ªä¸ªä¸“å®¶è·å¾—äº†æœ€é«˜çš„æ¦‚ç‡ï¼Œç»“æœæ˜¯ 0 æˆ– 1ï¼ˆæŒ‡ç¤ºå‡½æ•° $\mathbb{I}$ï¼‰ã€‚</td>
<td style="text-align:left">åŸºäº <strong>è½¯æ¦‚ç‡ $p_i(x)$</strong> çš„æ±‚å’Œï¼šè€ƒè™‘äº†é—¨æ§ç½‘ç»œå¯¹æ‰€æœ‰ä¸“å®¶çš„æ¦‚ç‡åˆ†é…å¤§å°ã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>ä»£è¡¨æ€§</strong></td>
<td style="text-align:left">è¡¡é‡ä¸“å®¶ $i$ <strong>å®é™…å¤„ç†çš„å·¥ä½œé‡</strong>ã€‚</td>
<td style="text-align:left">è¡¡é‡ä¸“å®¶ $i$ <strong>é¢„æœŸè¢«ä½¿ç”¨çš„å¹³å‡æ¦‚ç‡æƒé‡</strong>ã€‚</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Fine-Grained-Expert-Segmentation"><a href="#Fine-Grained-Expert-Segmentation" class="headerlink" title="Fine-Grained Expert Segmentation"></a>Fine-Grained Expert Segmentation</h2><p>while maintaining<br>the number of parameters<br>constant, we segment the experts<br>into a finer grain by splitting the<br>FFN intermediate hidden<br>dimension.</p>
<h2 id="Shared-Expert-Isolation"><a href="#Shared-Expert-Isolation" class="headerlink" title="Shared Expert Isolation"></a>Shared Expert Isolation</h2><p>we<br>isolate certain experts to serve as<br>shared experts that are always<br>activated, aiming at capturing and<br>consolidating common knowledge<br>across varying contexts. ï¼ˆä¸€éƒ¨åˆ†æ˜¯ä¸“å®¶foré€šç”¨çŸ¥è¯†ï¼Œä¸€ç›´è¢«æ¿€æ´»ï¼‰</p>
