<h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><h2 id="KLD"><a href="#KLD" class="headerlink" title="KLD"></a>KLD</h2><script type="math/tex; mode=display">D_{KL}(p(x) || q(x)) = \int_{-\infty}^{\infty} p(x) \ln \frac{p(x)}{q(x)} dx</script><script type="math/tex; mode=display">D_{KL}(p(x) || q(x)) = \sum_{x \in X} p(x) \ln \frac{p(x)}{q(x)}</script><p>KLD is non-symmetric</p>
<p>è¿™æ˜¯ä¸€ä¸ªå…³äº <strong>KL æ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰</strong> æ ¸å¿ƒæ¦‚å¿µçš„éå¸¸å¥½çš„é—®é¢˜ã€‚ç†è§£è¿™ä¸¤ä¸ªæ–¹å‘çš„å·®å¼‚ï¼Œæ˜¯ç†è§£å˜åˆ†æ¨æ–­ï¼ˆVIï¼‰å’Œç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ VAE å’Œ GANï¼‰çš„å…³é”®ã€‚</p>
<script type="math/tex; mode=display">D_{KL}(P_D \| P_M)$$å’Œ$$D_{KL}(P_M \| P_D)$$ éƒ½è¡¨ç¤ºæ¨¡å‹åˆ†å¸ƒ $P_M$ æ¥è¿‘çœŸå®æ•°æ®åˆ†å¸ƒ $P_D$ çš„ç¨‹åº¦ï¼Œä½†å®ƒä»¬å¯¹â€œæ¥è¿‘â€çš„å®šä¹‰æ˜¯**ä¸å¯¹ç§°**çš„ï¼Œå› æ­¤åœ¨æœ€å°åŒ–æ—¶ä¼šäº§ç”Ÿæˆªç„¶ä¸åŒçš„ç»“æœã€‚

---

## âš–ï¸ $D_{KL}(P_D \| P_M)$ çš„ç›´è§‰ï¼š**æ¨¡å¼è¦†ç›– (Mode-Covering)**

$$D_{KL}(P_D \| P_M) = \mathbb{E}_{x \sim \mathbf{P_D}} \left[ \log \frac{P_D(x)}{P_M(x)} \right]</script><h3 id="æ ¸å¿ƒç›´è§‰ï¼šå®³æ€•é”™è¿‡ï¼ˆå¯¹-P-D-çš„é«˜å¯†åº¦åŒºæ•æ„Ÿï¼‰"><a href="#æ ¸å¿ƒç›´è§‰ï¼šå®³æ€•é”™è¿‡ï¼ˆå¯¹-P-D-çš„é«˜å¯†åº¦åŒºæ•æ„Ÿï¼‰" class="headerlink" title="æ ¸å¿ƒç›´è§‰ï¼šå®³æ€•é”™è¿‡ï¼ˆå¯¹ $P_D$ çš„é«˜å¯†åº¦åŒºæ•æ„Ÿï¼‰"></a>æ ¸å¿ƒç›´è§‰ï¼š<strong>å®³æ€•é”™è¿‡ï¼ˆå¯¹ $P_D$ çš„é«˜å¯†åº¦åŒºæ•æ„Ÿï¼‰</strong></h3><p>è¿™ä¸ªè¡¨è¾¾å¼æ˜¯ä»¥<strong>çœŸå®æ•°æ®åˆ†å¸ƒ $P_D$</strong> ä¸ºæƒé‡è®¡ç®—æœŸæœ›çš„ã€‚</p>
<ul>
<li><strong>æƒ©ç½šæœºåˆ¶ï¼š</strong> å¦‚æœ $P<em>D(x)$ çš„å€¼å¾ˆé«˜ï¼ˆæ¯”å¦‚åœ¨ä¸€ä¸ªçœŸå®æ•°æ®æ¨¡å¼çš„<strong>å³°å€¼</strong>å¤„ï¼‰ï¼Œä½†æ¨¡å‹ $P_M(x)$ åœ¨æ­¤å¤„çš„å€¼å¾ˆä½ï¼ˆæ¨¡å‹<strong>é”™è¿‡</strong>äº†è¿™ä¸ªæ¨¡å¼ï¼‰ï¼Œé‚£ä¹ˆ $\log(P_D(x)/P_M(x))$ ä¼šæ˜¯ä¸€ä¸ªéå¸¸å¤§çš„æ­£æ•°ã€‚ç”±äº $P_D(x)$ æƒé‡ä¹Ÿå¾ˆå¤§ï¼Œè¿™ä¸ªé¡¹çš„è´¡çŒ®ä¼šè®© $D</em>{KL}(P_D | P_M)$ <strong>æ€¥å‰§å¢å¤§</strong>ã€‚</li>
<li><strong>ä¼˜åŒ–ç»“æœï¼ˆæœ€å°åŒ–ï¼‰ï¼š</strong> ä¸ºäº†é¿å…è¿™ç§å·¨å¤§çš„æƒ©ç½šï¼Œæ¨¡å‹ $P_M$ å¿…é¡»ç¡®ä¿åœ¨ <strong>$P_D$ æ‰€æœ‰å¯†åº¦é«˜çš„åŒºåŸŸï¼ˆæ‰€æœ‰æ¨¡å¼ï¼‰</strong> éƒ½åˆ†é…æ¦‚ç‡ã€‚</li>
<li><strong>æ‹Ÿåˆç‰¹æ€§ï¼š</strong> <strong>æ¨¡å¼è¦†ç›– (Mode-Covering)</strong>ã€‚æ¨¡å‹ä¼š<strong>æ‰©å¼ </strong>ï¼ŒåŠªåŠ›è¦†ç›– $P_D$ çš„æ‰€æœ‰æ¨¡å¼ï¼Œå³ä½¿è¿™æ„å‘³ç€å®ƒå¿…é¡»åœ¨æ¨¡å¼ä¹‹é—´çš„ä½å¯†åº¦åŒºä¹Ÿåˆ†é…ä¸€äº›æ¦‚ç‡ã€‚</li>
<li><strong>åº”ç”¨åœºæ™¯ï¼š</strong> <strong>æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)</strong> å’Œ <strong>å˜åˆ†è‡ªç¼–ç å™¨ (VAE)</strong> çš„ä¸»è¦ä¼˜åŒ–æ–¹å‘ï¼ˆä¸ ELBO ç›¸å…³ï¼‰ï¼Œå¼ºè°ƒ<strong>å¤šæ ·æ€§</strong>ã€‚</li>
</ul>
<blockquote>
<p>ğŸ’¡ <strong>ç±»æ¯”ï¼š</strong> $P_M$ å°±åƒä¸€ä¸ª<strong>å¹¿æ’’ç½‘çš„æ•é±¼äºº</strong>ã€‚ä»–æœ€å®³æ€•çš„æ˜¯é”™è¿‡ä»»ä½•ä¸€ä¸ªé±¼ç¾¤ï¼ˆ$P_D$ çš„æ¨¡å¼ï¼‰ï¼Œæ‰€ä»¥ä»–ä¼šå°½åŠ›æ‰©å¤§ä»–çš„ç½‘ï¼ˆ$P_M$ çš„åˆ†å¸ƒèŒƒå›´ï¼‰ï¼Œå³ä½¿è¿™ä¼šæ•åˆ°ä¸€äº›æ— å…³çš„æ‚ç‰©ï¼ˆåœ¨ $P_D$ ä½å¯†åº¦åŒºåˆ†é…æ¦‚ç‡ï¼Œå¯¼è‡´ç”Ÿæˆæ ·æœ¬æ¨¡ç³Šï¼‰ã€‚</p>
</blockquote>
<hr>
<h2 id="âš”ï¸-D-KL-P-M-P-D-çš„ç›´è§‰ï¼šæ¨¡å¼æœç´¢-Mode-Seeking"><a href="#âš”ï¸-D-KL-P-M-P-D-çš„ç›´è§‰ï¼šæ¨¡å¼æœç´¢-Mode-Seeking" class="headerlink" title="âš”ï¸ $D_{KL}(P_M | P_D)$ çš„ç›´è§‰ï¼šæ¨¡å¼æœç´¢ (Mode-Seeking)"></a>âš”ï¸ $D_{KL}(P_M | P_D)$ çš„ç›´è§‰ï¼š<strong>æ¨¡å¼æœç´¢ (Mode-Seeking)</strong></h2><script type="math/tex; mode=display">D_{KL}(P_M \| P_D) = \mathbb{E}_{x \sim \mathbf{P_M}} \left[ \log \frac{P_M(x)}{P_D(x)} \right]</script><h3 id="æ ¸å¿ƒç›´è§‰ï¼šå®³æ€•çŠ¯é”™ï¼ˆå¯¹-P-D-çš„ä½å¯†åº¦åŒºæ•æ„Ÿï¼‰"><a href="#æ ¸å¿ƒç›´è§‰ï¼šå®³æ€•çŠ¯é”™ï¼ˆå¯¹-P-D-çš„ä½å¯†åº¦åŒºæ•æ„Ÿï¼‰" class="headerlink" title="æ ¸å¿ƒç›´è§‰ï¼šå®³æ€•çŠ¯é”™ï¼ˆå¯¹ $P_D$ çš„ä½å¯†åº¦åŒºæ•æ„Ÿï¼‰"></a>æ ¸å¿ƒç›´è§‰ï¼š<strong>å®³æ€•çŠ¯é”™ï¼ˆå¯¹ $P_D$ çš„ä½å¯†åº¦åŒºæ•æ„Ÿï¼‰</strong></h3><p>è¿™ä¸ªè¡¨è¾¾å¼æ˜¯ä»¥<strong>æ¨¡å‹åˆ†å¸ƒ $P_M$</strong> ä¸ºæƒé‡è®¡ç®—æœŸæœ›çš„ã€‚</p>
<ul>
<li><strong>æƒ©ç½šæœºåˆ¶ï¼š</strong> å¦‚æœæ¨¡å‹ $P<em>M(x)$ çš„å€¼å¾ˆé«˜ï¼ˆæ¨¡å‹è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå¯èƒ½ç”Ÿæˆçš„æ ·æœ¬ï¼‰ï¼Œä½†çœŸå®æ•°æ® $P_D(x)$ åœ¨æ­¤å¤„çš„å€¼å¾ˆä½ï¼ˆ<strong>å±±è°·</strong>æˆ–<strong>ç©ºéš™</strong>ï¼‰ï¼Œé‚£ä¹ˆ $\log(P_M(x)/P_D(x))$ ä¼šæ˜¯ä¸€ä¸ªéå¸¸å¤§çš„æ­£æ•°ã€‚ç”±äº $P_M(x)$ æƒé‡ä¹Ÿå¾ˆå¤§ï¼Œè¿™ä¸ªé¡¹çš„è´¡çŒ®ä¼šè®© $D</em>{KL}(P_M | P_D)$ <strong>æ€¥å‰§å¢å¤§</strong>ã€‚</li>
<li><strong>ä¼˜åŒ–ç»“æœï¼ˆæœ€å°åŒ–ï¼‰ï¼š</strong> ä¸ºäº†é¿å…è¿™ç§æƒ©ç½šï¼Œæ¨¡å‹ $P_M$ ä¼šè¢«å¼ºçƒˆè¿«ä½¿å°†è‡ªå·±çš„æ¦‚ç‡è´¨é‡é›†ä¸­åˆ° <strong>$P_D$ å¯†åº¦é«˜çš„åŒºåŸŸ</strong>ã€‚å®ƒå®æ„¿å¿½ç•¥ä¸€äº›æ¨¡å¼ï¼Œä¹Ÿä¸æ„¿æ„åœ¨ $P_D$ çš„ç¨€ç–åŒºåŸŸâ€œè¯´è°â€ï¼ˆåˆ†é…æ¦‚ç‡ï¼‰ã€‚</li>
<li><strong>æ‹Ÿåˆç‰¹æ€§ï¼š</strong> <strong>æ¨¡å¼æœç´¢ (Mode-Seeking)</strong>ã€‚æ¨¡å‹ä¼š<strong>æ”¶ç¼©</strong>ï¼Œé€‰æ‹©å¹¶èšç„¦äº $P_D$ çš„ä¸€ä¸ªæˆ–å‡ ä¸ªæ¨¡å¼è¿›è¡Œæ‹Ÿåˆã€‚</li>
<li><strong>åº”ç”¨åœºæ™¯ï¼š</strong> <strong>å˜åˆ†æ¨æ–­ (VI)</strong> ä¸­å¯¹è¿‘ä¼¼åéªŒåˆ†å¸ƒ $q$ çš„è¦æ±‚ï¼Œä»¥åŠ <strong>GAN</strong> æ¨¡å‹çš„å†…åœ¨å€¾å‘ï¼Œå¼ºè°ƒ<strong>è´¨é‡</strong>ã€‚</li>
</ul>
<blockquote>
<p>ğŸ’¡ <strong>ç±»æ¯”ï¼š</strong> $P_M$ å°±åƒä¸€ä¸ª<strong>ç²¾å‡†ç„å‡†çš„ç‹™å‡»æ‰‹</strong>ã€‚ä»–æœ€å®³æ€•çš„æ˜¯å°„åï¼ˆåœ¨ $P_D$ çš„ä½å¯†åº¦åŒºåˆ†é…æ¦‚ç‡ï¼‰ï¼Œæ‰€ä»¥ä»–åªä¼šç„å‡†ä»–èƒ½ç¡®ä¿å‘½ä¸­çš„<strong>ä¸€ä¸ªæˆ–å‡ ä¸ªæœ€æ¸…æ™°çš„ç›®æ ‡</strong>ï¼ˆ$P_D$ çš„é«˜å³°ï¼‰ã€‚è¿™å¯¼è‡´ä»–å¯èƒ½ä¼šé”™è¿‡å…¶ä»–æ¨¡å¼ï¼Œä½†ç¡®ä¿äº†æé«˜çš„ç²¾åº¦ã€‚</p>
</blockquote>
<h2 id="Multi-class-classification"><a href="#Multi-class-classification" class="headerlink" title="Multi-class classification"></a>Multi-class classification</h2><p>Encode: x -&gt; d-dim vector h</p>
<p>Predict: </p>
<p>z=Wh + b (z is logits)</p>
<p>Use softmax to map z to probability P(y|x).</p>
<p>é˜²æ­¢softmax exlode: normal softmax åˆ†å­åˆ†æ¯åŒæ—¶é™¤ä»¥ $e^{max}$</p>
<p>Traning:</p>
<p>use cross entropy loss</p>
<script type="math/tex; mode=display">L_{\text{CE}} = \sum_i -\log P(y = y_i | \mathbf{x}_i)</script><p>use SGD:</p>
<p>neural nets we donâ€™t have a closed-form optimal solution </p>
<p>minibatch is much faster and effective, also have some regularization effect</p>
<h1 id="NN-Basics"><a href="#NN-Basics" class="headerlink" title="NN Basics"></a>NN Basics</h1><p>MLP<br><img src="nlp=0.png" alt=""></p>
<h2 id="Bag-of-words"><a href="#Bag-of-words" class="headerlink" title="Bag of words"></a>Bag of words</h2><p>A simple way to encode a sentence:<br> a |V|-dim vector, the i-th dimension indicates whether the i-th word in V(vocabulary)<br>exists in x</p>
<h2 id="h-Wx-embed"><a href="#h-Wx-embed" class="headerlink" title="h=Wx embed"></a>h=Wx embed</h2><p>æ³¨æ„ï¼šThe difference with LSA and word2vec is that here the word embedding<br>matrix is treated as part (the first layer) of the parameters of the NN model. But indeed, you can use word2vec/LSA (trained on larger data without label)<br>to initialize this matrix.</p>
<script type="math/tex; mode=display">\sigma'(z) = \sigma(z) \cdot (1 - \sigma(z))</script><p>A linear transform $y=Wx$, stacking linear transfroms is still a linear transform.</p>
<h2 id="Back-Propagation"><a href="#Back-Propagation" class="headerlink" title="Back-Propagation"></a>Back-Propagation</h2><p>Chain Rule</p>
<script type="math/tex; mode=display">z = Wh + b</script><script type="math/tex; mode=display">\frac{\partial z}{\partial h}=W</script><script type="math/tex; mode=display">\frac{\partial z}{\partial W}=h^\top</script><script type="math/tex; mode=display">\frac{\partial z}{\partial b}=I</script><p>one hotï¼š åªæœ‰çœŸå®çš„labelä¸º1ï¼Œå…¶ä»–çš„å…¨0</p>
<script type="math/tex; mode=display">\frac{\partial \mathcal{L}_{\text{CE}}(\mathbf{y}|\mathbf{x})}{\partial \mathbf{z}} = \frac{\partial (-\log(\text{softmax}(\mathbf{z}))[\mathbf{y}])}{\partial \mathbf{z}} = \text{softmax}(\mathbf{z}) - \mathbf{\tilde{y}}</script><p>proofï¼š $\mathcal{L}(\mathbf{z}, \mathbf{\tilde{y}}) = -\sum_{j=1}^K \tilde{y}_j \log(\hat{y}_j)$</p>
<p> $\mathbf{\hat{y}} = \text{softmax}(\mathbf{z})$ã€‚å…¶ä¸­ $\hat{y}<em>j = \frac{e^{z_j}}{\sum</em>{k=1}^K e^{z_k}}$ã€‚</p>
<ul>
<li><strong>çœŸå®æ ‡ç­¾ï¼š</strong> $\mathbf{\tilde{y}}$ï¼Œå‡è®¾çœŸå®æ ‡ç­¾æ˜¯ $c$ï¼Œåˆ™ $\tilde{y}_c = 1$ï¼Œå…¶ä½™ $\tilde{y}_j = 0$ã€‚</li>
<li><strong>äº¤å‰ç†µæŸå¤±ï¼š</strong> $\mathcal{L}(\mathbf{z}, \mathbf{\tilde{y}}) = -\sum_{j=1}^K \tilde{y}_j \log(\hat{y}_j)$ã€‚<ul>
<li>å› ä¸º $\mathbf{\tilde{y}}$ æ˜¯ one-hot å‘é‡ï¼Œæ‰€ä»¥ä¸Šå¼ç®€åŒ–ä¸ºï¼š$\mathcal{L} = -\log(\hat{y}<em>c) = -\log\left(\frac{e^{z_c}}{\sum</em>{k=1}^K e^{z_k}}\right)$ã€‚</li>
</ul>
</li>
</ul>
<h4 id="i-æ˜¯æ­£ç¡®åˆ†ç±»-c-çš„ç´¢å¼•ï¼ˆå³-i-c-ï¼‰"><a href="#i-æ˜¯æ­£ç¡®åˆ†ç±»-c-çš„ç´¢å¼•ï¼ˆå³-i-c-ï¼‰" class="headerlink" title="$i$ æ˜¯æ­£ç¡®åˆ†ç±» $c$ çš„ç´¢å¼•ï¼ˆå³ $i=c$ï¼‰"></a>$i$ æ˜¯æ­£ç¡®åˆ†ç±» $c$ çš„ç´¢å¼•ï¼ˆå³ $i=c$ï¼‰</h4><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial z_c} = \frac{\partial}{\partial z_c} \left[ -\log\left(\frac{e^{z_c}}{\sum_{k=1}^K e^{z_k}}\right) \right]</script><p>åˆ©ç”¨å¯¹æ•°æ€§è´¨ $\log(A/B) = \log A - \log B$ï¼š</p>
<script type="math/tex; mode=display">\mathcal{L} = -z_c + \log\left(\sum_{k=1}^K e^{z_k}\right)</script><p>æ±‚å¯¼ï¼š</p>
<script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial z_c} = \frac{\partial}{\partial z_c}(-z_c) + \frac{\partial}{\partial z_c}\left(\log\left(\sum_{k=1}^K e^{z_k}\right)\right)</script><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial z_c} = -1 + \underbrace{\frac{1}{\sum_{k=1}^K e^{z_k}}}_{\text{å¤–å±‚å¯¼æ•°}} \cdot \underbrace{e^{z_c}}_{\text{å†…å±‚å¯¼æ•°}}</script><p>å›ä»£ Softmax çš„å®šä¹‰ $\hat{y}<em>c = \frac{e^{z_c}}{\sum</em>{k=1}^K e^{z_k}}$ï¼š</p>
<script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial z_c} = -1 + \hat{y}_c</script><p>å› ä¸º $i=c$ï¼Œæ‰€ä»¥ $\tilde{y}_i = 1$ã€‚</p>
<script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial z_i} = \hat{y}_i - 1 = \hat{y}_i - \tilde{y}_i</script><h4 id="i-ä¸æ˜¯æ­£ç¡®åˆ†ç±»çš„ç´¢å¼•ï¼ˆå³-i-neq-c-ï¼‰"><a href="#i-ä¸æ˜¯æ­£ç¡®åˆ†ç±»çš„ç´¢å¼•ï¼ˆå³-i-neq-c-ï¼‰" class="headerlink" title="$i$ ä¸æ˜¯æ­£ç¡®åˆ†ç±»çš„ç´¢å¼•ï¼ˆå³ $i \neq c$ï¼‰"></a>$i$ ä¸æ˜¯æ­£ç¡®åˆ†ç±»çš„ç´¢å¼•ï¼ˆå³ $i \neq c$ï¼‰</h4><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial z_i} = \frac{\partial}{\partial z_i} \left[ -z_c + \log\left(\sum_{k=1}^K e^{z_k}\right) \right]</script><p>æ±‚å¯¼ï¼ˆæ³¨æ„ $-z_c$ é¡¹å¯¹ $z_i$ çš„å¯¼æ•°ä¸º 0ï¼‰ï¼š</p>
<script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial z_i} = 0 + \underbrace{\frac{1}{\sum_{k=1}^K e^{z_k}}}_{\text{å¤–å±‚å¯¼æ•°}} \cdot \underbrace{e^{z_i}}_{\text{å†…å±‚å¯¼æ•°}}</script><p>å›ä»£ Softmax çš„å®šä¹‰ $\hat{y}<em>i = \frac{e^{z_i}}{\sum</em>{k=1}^K e^{z_k}}$ï¼š</p>
<script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial z_i} = \hat{y}_i</script><p>å› ä¸º $i \neq c$ï¼Œæ‰€ä»¥ $\tilde{y}_i = 0$ã€‚</p>
<script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial z_i} = \hat{y}_i - 0 = \hat{y}_i - \tilde{y}_i</script><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><ul>
<li><p><strong>è®­ç»ƒçš„æ—¶å€™éšæœºå¤±æ´»ï¼š</strong> åœ¨å¤„ç†æ¯ä¸ª <strong>mini-batch</strong> æ•°æ®æ—¶ï¼Œç¥ç»ç½‘ç»œä¸­çš„<strong>æ¯ä¸ªç¥ç»å…ƒå•å…ƒï¼ˆUnitï¼‰</strong>ï¼ˆåŠå…¶æ‰€æœ‰ä¼ å…¥å’Œä¼ å‡ºçš„è¿æ¥ï¼‰éƒ½ä¼šä»¥ä¸€ä¸ªé¢„è®¾çš„æ¦‚ç‡ $p$ è¢«éšæœºåœ°â€œä¸¢å¼ƒâ€æˆ–<strong>å¤±æ´»</strong>ã€‚</p>
</li>
<li><p>At test time, all units are present,<br>but with weights scaled<br>by p (i.e. w becomes pw)</p>
</li>
</ul>
<p>parallel computing: For a minibatch of input, we can<br>concat them into a input matrix. Matrix-vector operation now<br>becomes matrix-matrix<br>operation.</p>
<h1 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h1><h2 id="feed-forward-neural-network"><a href="#feed-forward-neural-network" class="headerlink" title="feed forward neural network"></a>feed forward neural network</h2><p><img src="nlp=nnlm.png" alt=""></p>
<p>Note a big difference with the sentiment classifier is that the output class number is now |V|,<br>making the model slow. </p>
<h2 id="class-based-neural-network"><a href="#class-based-neural-network" class="headerlink" title="class-based neural network"></a>class-based neural network</h2><p><img src="nlp=class.png" alt=""></p>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p><img src="nlp=rnn.png" alt=""></p>
<p><img src="nlp=rnn0.png" alt=""></p>
<p>better long distance dependenceï¼š hidden statesï¼Œ shared $W_ih$ and $W_hh$</p>
<p>Back propagation through time</p>
<p>problem: gradient explode/ vanish</p>
<p>æ ‡å‡†çš„ RNN éšè—çŠ¶æ€è®¡ç®—æ˜¯ï¼š$h<em>t = \text{activation}(W</em>{hh} h<em>{t-1} + W</em>{ih} x_t)$ã€‚</p>
<p>è¿™é‡Œç®€åŒ–ä¸ºï¼š$h<em>t \approx W</em>{hh} h<em>{t-1} + W</em>{ih} x_t$ã€‚</p>
<ul>
<li>ä» $h<em>t$ åˆ° $h</em>{t-1}$ çš„æ¢¯åº¦ $\frac{\partial h<em>t}{\partial h</em>{t-1}} \approx W_{hh}$ã€‚</li>
</ul>
<script type="math/tex; mode=display">\frac{\partial L_t}{\partial h_1} \approx W_{hh}^{T^{t-1}} \frac{\partial L_t}{\partial h_t}</script><p>t-1æ¬¡è¿ä¹˜å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤± </p>
<p>Gradient exploding is more serious because it makes training impossibleï¼Œå› ä¸ºæ¢¯åº¦æ— é™å¤§äº†</p>
<p>è§£å†³æ–¹æ¡ˆï¼šgradient clippingï¼Œset the maximum norm of gradient to be $\gamma$.</p>
<p>parallel of rnnï¼šparallel across scentence</p>
<p><img src="nlp=rnnp.png" alt=""></p>
<p>parallel traning: use padding to get same scentence length</p>
<p>you can also design a cnn to deal with variable seqlen</p>
<h3 id="autoregressive-LM"><a href="#autoregressive-LM" class="headerlink" title="autoregressive LM"></a>autoregressive LM</h3><p><img src="nlp=ar.png" alt=""></p>
<h3 id="Gated-recurrent-unit"><a href="#Gated-recurrent-unit" class="headerlink" title="Gated recurrent unit"></a>Gated recurrent unit</h3><script type="math/tex; mode=display">h_t = z_t \odot \hat{h}_t + (1 - z_t) \odot h_{t-1}</script><p><img src="nlp=grnn.png" alt=""></p>
<p>resnet</p>
<p><img src="nlp=resnet.png" alt=""></p>
<h3 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h3><p>cannot be applied to AR-LM, because we cannot uitilize the information from the future. æœ€ç»ˆçš„hidden stateå¿…é¡»ç­‰åˆ°æ‰€æœ‰è®¡ç®—å…¨éƒ¨ç»“æŸä¹‹åæ‰èƒ½å®Œæˆã€‚</p>
<p><img src="nlp=birnn.png" alt=""></p>
<h3 id="Encoder-decoder"><a href="#Encoder-decoder" class="headerlink" title="Encoder-decoder"></a>Encoder-decoder</h3><p><img src="nlp=sembed.png" alt=""></p>
<p>scentence-encoding: å¥å­æœ€å¼€å§‹åŠ ä¸€ä¸ªç‰¹æ®Šçš„tokenï¼Œè¿™ä¸ªä¸»è¦åˆ©ç”¨çš„æ˜¯åå‘çš„h0</p>
<p>We can use a bi-rnn encoder for the input sequence, and use a uni-rnn decoder<br>for the output.</p>
<h3 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h3><p><img src="nlp=att.png" alt=""></p>
<p>æ­¥éª¤ä¸€ï¼šè®¡ç®—å¯¹é½åˆ†æ•° (Alignment Score)</p>
<script type="math/tex; mode=display">\tilde{a}_i = (h_i^{enc})^T W_a h_{t-1}^{dec}</script><p>æ­¥éª¤äºŒï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†å¸ƒ (Attention Distribution)</p>
<pre><code>$$a = \text{softmax}(\tilde{a})$$
</code></pre><p>æ­¥éª¤ä¸‰ï¼šè®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ (Context Vector)</p>
<pre><code>$$c_t = \sum_{i} a_i h_i^{enc}$$
</code></pre><h3 id="greedy-decoding"><a href="#greedy-decoding" class="headerlink" title="greedy decoding"></a>greedy decoding</h3><p>autoregressiveï¼Œæ¯ä¸€æ¬¡éƒ½é€‰æ‹©å±€éƒ¨æœ€ä¼˜çš„ï¼Œæ‰€ä»¥ä¸ä¸€å®šæ˜¯å…¨å±€æœ€ä¼˜çš„</p>
<h3 id="beam-search-decoding"><a href="#beam-search-decoding" class="headerlink" title="beam search decoding"></a>beam search decoding</h3><p><img src="nlp=b.png" alt=""></p>
<pre><code>// æ­¥éª¤ 1: åˆå§‹åŒ–
// åˆå§‹åŒ–é›†æŸ (beams)ã€‚é›†æŸæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå­˜å‚¨å½“å‰å¾—åˆ†æœ€é«˜çš„ B ä¸ªå€™é€‰åºåˆ—ã€‚
// æ ¼å¼ï¼š[(åºåˆ—, ç´¯ç§¯å¯¹æ•°æ¦‚ç‡å¾—åˆ†)]
beams = [[&quot;&lt;s&gt;&quot;, 0]] 
// åˆå§‹åºåˆ—ä¸ºèµ·å§‹ç¬¦å· &quot;&lt;s&gt;&quot;ï¼Œç´¯ç§¯å¾—åˆ†ä¸º 0ã€‚

// æ­¥éª¤ 2: è¿­ä»£ç”Ÿæˆåºåˆ—
// L æ˜¯ç›®æ ‡åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚å¾ªç¯ä» t=1ï¼ˆç”Ÿæˆç¬¬ä¸€ä¸ªè¯ï¼‰åˆ° Lã€‚
for t from 1 to L:
    // åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„é›†æŸåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å½“å‰æ—¶é—´æ­¥ t æ‰©å±•åçš„æ‰€æœ‰æ–°åºåˆ—ã€‚
    new_b = [] 

    // éå†å½“å‰é›†æŸä¸­çš„æ‰€æœ‰ B ä¸ªå€™é€‰åºåˆ—ã€‚
    for b in beams:
        // è§£æ„å½“å‰å€™é€‰åºåˆ—ï¼šhis æ˜¯å†å²åºåˆ—ï¼Œscore æ˜¯å®ƒçš„ç´¯ç§¯å¯¹æ•°æ¦‚ç‡å¾—åˆ†ã€‚
        his, score = b[0], b[1]

        // ä½¿ç”¨è¯­è¨€æ¨¡å‹ (LM) é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚
        // logprob å­˜å‚¨åŸºäº his å¾—åˆ°çš„è¯æ±‡è¡¨æ‰€æœ‰è¯çš„å¯¹æ•°æ¦‚ç‡ã€‚
        logprob = LM-NextPredict[his] 

        // ä» logprob ä¸­é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ k=B ä¸ªè¯æ±‡åŠå…¶å¯¹æ•°æ¦‚ç‡ã€‚
        // idx_s: è¿™ B ä¸ªè¯æ±‡åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚
        // logprob_s: è¿™ B ä¸ªè¯æ±‡å¯¹åº”çš„å¯¹æ•°æ¦‚ç‡ã€‚
        idx_s, logprob_s = top-k(logprob, k=B)

        // æ­¥éª¤ 3: æ‰©å±•åºåˆ—å¹¶è®¡ç®—æ–°å¾—åˆ†
        // éå†è¿™ B ä¸ªæœ€ä½³çš„ä¸‹ä¸€ä¸ªè¯æ±‡ã€‚
        for j from 0 to (B-1):
            // æ‰©å±•åºåˆ—ï¼šå°†æ–°çš„æœ€ä½³è¯ V[idx_s[j]] æ·»åŠ åˆ°å†å²åºåˆ— his åé¢ã€‚
            // V æ˜¯è¯æ±‡è¡¨ (Vocabulary)ã€‚
            // æ›´æ–°å¾—åˆ†ï¼šæ–°å¾—åˆ† = æ—§å¾—åˆ† + æ–°è¯æ±‡çš„å¯¹æ•°æ¦‚ç‡ (å› ä¸º log(P1*P2) = log(P1)+log(P2))ã€‚
            new_b.append((his + V[idx_s[j]], score + logprob_s[j]))

    // æ­¥éª¤ 4: ç­›é€‰å’Œæ›´æ–°é›†æŸ
    // å¯¹ new_b ä¸­çš„æ‰€æœ‰åºåˆ—ï¼ˆå½“å‰æœ‰ B*B ä¸ªï¼‰æŒ‰å¾—åˆ†ï¼ˆscoreï¼‰è¿›è¡Œé™åºæ’åºã€‚
    // åªä¿ç•™å¾—åˆ†æœ€å¤§çš„ B ä¸ªåºåˆ—ã€‚
    sort and only keep B sequences in new_b with largest score.

    // ç”¨ç­›é€‰åçš„ B ä¸ªæœ€ä½³åºåˆ—æ›´æ–° beamsï¼Œè¿›å…¥ä¸‹ä¸€è½®è¿­ä»£ã€‚
    beams = new_b

// æ­¥éª¤ 5: è¿”å›ç»“æœ
// å¾ªç¯ç»“æŸåï¼Œbeams ä¸­å­˜å‚¨äº†é•¿åº¦ä¸º L çš„ B ä¸ªæœ€ä½³åºåˆ—ã€‚
// è¿”å›å¾—åˆ†æœ€é«˜ï¼ˆæ’åœ¨ç¬¬ä¸€ä½ï¼‰çš„åºåˆ—ã€‚
return beams[0]
</code></pre><p>å…³é”®ï¼›æ¯ä¸€æ¬¡ä¸€ä¸ªnodeéƒ½ä¼šç”Ÿæˆbä¸ªåˆ†æ”¯ï¼Œä½†æˆ‘ä»¬åªä¿ç•™å¾—åˆ†æœ€å¤§çš„bä¸ª</p>
<h3 id="belu-metric-for-machine-translation"><a href="#belu-metric-for-machine-translation" class="headerlink" title="belu metric for machine translation"></a>belu metric for machine translation</h3><script type="math/tex; mode=display">
\text{precision}_n = \frac{\text{number of } n\text{-gram matches in reference}}{\text{number of } n\text{-grams in predicted}}</script><p>ç®€æ´æƒ©ç½šç”¨äº<strong>æƒ©ç½š</strong>é‚£äº›<strong>æ¯”å‚è€ƒè¯‘æ–‡çŸ­</strong>çš„æœºå™¨è¯‘æ–‡ï¼Œä»¥ç¡®ä¿ç¿»è¯‘çš„é•¿åº¦åˆç†ã€‚</p>
<script type="math/tex; mode=display">
\text{brevity-penalty} = \min \left\{ 1, \exp \left( 1 - \frac{|\text{reference}|}{|\text{predicted}|} \right) \right\}</script><p>BLEU æœ€ç»ˆå¾—åˆ†æ˜¯<strong>ç®€æ´æƒ©ç½š</strong>å’Œ<strong>å‡ ä½•å¹³å‡ $n$-gram ç²¾å‡†åº¦</strong>çš„ä¹˜ç§¯ã€‚</p>
<script type="math/tex; mode=display">
\text{BLEU} = \text{brevity-penalty} \times \left( \prod_{n=1}^{4} \text{precision}_n \right)^{\frac{1}{4}}</script><h3 id="creating-paired-data"><a href="#creating-paired-data" class="headerlink" title="creating paired data"></a>creating paired data</h3><p>Given a decent amount of bilingual data (X, Y) and a good amount<br>monolingual data in target language Y.</p>
<p>â€¢ Q: What can you do to create more paired bilingual data?</p>
<p>â€¢ You can train a backward model Y-&gt;X, and conduct generation on<br>the monolingual data. Thatâ€™s called back translation.</p>
<h2 id="GLUE-benchmark"><a href="#GLUE-benchmark" class="headerlink" title="GLUE benchmark"></a>GLUE benchmark</h2><p>The General Language Understanding<br>Evaluation , human level</p>
<p>SuperGLUE, harder</p>
<h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><p>ELMo (Embeddings from Language Models): build<br>deep contextualized word representation.</p>
<p>Model: multilayer bidirectional LSTM</p>
<p>Objective: predict the next word in both<br>directions independently; i.e., left-to-right<br>and right-to-left</p>
<h1 id="subword-tokenization"><a href="#subword-tokenization" class="headerlink" title="subword tokenization"></a>subword tokenization</h1><p>â€¢ (1) Start with a unigram vocabulary of all characters in the data.</p>
<p>â€¢ (2) Each iteration: In the data, find the most frequent pair, merge it,<br>and add to the vocabulary.</p>
<p>â€¢ (3) Stop when vocabulary is of pre-determined size (e.g., 50k).</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p><img src="nlp=tf.png" alt=""></p>
<h2 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h2><p>q, k, v å‡ä¸º Wx</p>
<p>è®¡ç®—ç¬¬ $i$ ä¸ªä½ç½®çš„æŸ¥è¯¢å‘é‡ $\mathbf{q}<em>i$ å¯¹æ‰€æœ‰é”®å‘é‡ $\mathbf{k}</em><em>$ï¼ˆå…¶ä¸­ $</em>$ ä»£è¡¨å¥å­ä¸­çš„æ‰€æœ‰ä½ç½®ï¼‰çš„æ³¨æ„åŠ›æƒé‡çš„å…¬å¼å¦‚ä¸‹ï¼š</p>
<script type="math/tex; mode=display">a_{i*} = \mathbf{softmax}\left(\frac{\mathbf{q}_i^\top \mathbf{k}_*}{\sqrt{\text{dim}_k}}\right)</script><p>divide $\sqrt{\text{dim}_k}$, normalize the variance, achieve more stable and smooth softmax output. </p>
<p>$Z = softmax(\frac{QK^\top}{\sqrt{dimk}})V$ parallel computation</p>
<h2 id="layernorm"><a href="#layernorm" class="headerlink" title="layernorm"></a>layernorm</h2><script type="math/tex; mode=display">\mathbf{LayerNorm}(\mathbf{h}) = \alpha \cdot \frac{\mathbf{h} - \text{mean}(\mathbf{h})}{\text{std}(\mathbf{h})} + \beta</script><pre><code class="lang-python">class LayerNorm(nn.Module):

    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features)) #å‚æ•°
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps #ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œé˜²æ­¢é™¤ä»¥0

    def forward(self, x):
        # x:(seqlen,feature)
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
</code></pre>
<p>conbined with resnet:<br>$h_{out}=F(layernorm(h))+h$</p>
<p>batchnormå¾€å¾€éœ€è¦batchsizeæ¯”è¾ƒå¤§ï¼Œä½†å½“æˆ‘ä»¬è®­å¤§è¯­è¨€æ¨¡å‹çš„æ—¶å€™batchsizeå¯èƒ½è¾¾ä¸åˆ°é‚£ä¹ˆå¤§ã€‚è€Œä¸”qkvçš„ç»´åº¦dæ˜¯ç¡®å®šçš„ï¼Œä½†seqlenä¸ç¡®å®šï¼Œæ‰€ä»¥layernormæ›´åŠ é€‚åˆsequence modelã€‚</p>
<h2 id="multihead"><a href="#multihead" class="headerlink" title="multihead"></a>multihead</h2><p>æ¯ä¸€ä¸ªheadä¸€å¥—qkvçš„æƒé‡çŸ©é˜µï¼Œæœ€åconcatå†linear</p>
<p><img src="nlp=head.png" alt=""></p>
<h2 id="positional-encoding"><a href="#positional-encoding" class="headerlink" title="positional encoding"></a>positional encoding</h2><p>Without RNN, attention alone does not<br>have order information!</p>
<blockquote>
<p><strong>Exercise:</strong> Given any $\text{pos}$ vector and a fixed number $k$, can you represent $\text{pos} + k$ as a linear transform of $\text{pos}$?</p>
<p><strong>Hint:</strong> $\sin(A+B) = \sin A \cos B + \sin B \cos A$</p>
</blockquote>
<p>è¿™é‡Œçš„ $\text{pos}$ æ˜¯æŒ‡ä½ç½® $pos$ çš„ä½ç½®ç¼–ç å‘é‡ï¼Œè€Œ $\text{pos}+k$ æ˜¯æŒ‡ä½ç½® $pos+k$ çš„ä½ç½®ç¼–ç å‘é‡ã€‚</p>
<p>å‡è®¾ä½ç½®ç¼–ç å‘é‡çš„ç¬¬ $i$ ä¸ªç»´åº¦åˆ†é‡å®šä¹‰å¦‚ä¸‹ï¼š</p>
<script type="math/tex; mode=display">\text{PE}_{\text{pos}, i} = \sin(\omega_i \cdot \text{pos})</script><p>å…¶ä¸­ $\omega_i$ æ˜¯ä¸ç»´åº¦ $i$ ç›¸å…³çš„å›ºå®šé¢‘ç‡é¡¹ã€‚</p>
<p>ç°åœ¨ï¼Œæˆ‘ä»¬æ¥çœ‹<strong>ä½ç½® $\text{pos}+k$</strong> çš„ç¬¬ $i$ ä¸ªåˆ†é‡ï¼š</p>
<script type="math/tex; mode=display">\text{PE}_{\text{pos}+k, i} = \sin(\omega_i \cdot (\text{pos} + k))</script><p>åˆ©ç”¨æç¤ºä¸­çš„<strong>ä¸‰è§’æ’ç­‰å¼</strong> $\sin(A+B) = \sin A \cos B + \sin B \cos A$ï¼Œå…¶ä¸­ $A = \omega_i \cdot \text{pos}$ï¼Œ$B = \omega_i \cdot k$ï¼š</p>
<script type="math/tex; mode=display">\text{PE}_{\text{pos}+k, i} = \underbrace{\sin(\omega_i \cdot \text{pos})}_{\text{PE}_{\text{pos}, i}} \cdot \underbrace{\cos(\omega_i \cdot k)}_{\text{å›ºå®šå¸¸é‡}} + \underbrace{\cos(\omega_i \cdot \text{pos})}_{\text{PE}_{\text{pos}, i}^{\text{perp}}} \cdot \underbrace{\sin(\omega_i \cdot k)}_{\text{å›ºå®šå¸¸é‡}}</script><script type="math/tex; mode=display">\mathbf{PE}_{\text{pos}+k} = \mathbf{A}_k \cdot \mathbf{PE}_{\text{pos}}</script><h2 id="lr-warm-up-and-linear-decay"><a href="#lr-warm-up-and-linear-decay" class="headerlink" title="lr warm-up and linear decay"></a>lr warm-up and linear decay</h2><p>We usually start with a large learning rate (lr),<br>and then decay over time.</p>
<p>For transformer models, it is very useful to set a<br>small warmup stage, where we first gradually<br>increase lr from zero to the starting value.</p>
<p> Without this trick, training is likely to get stuck.</p>
<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p>pretrained  with <strong>self<br>supervised training</strong>, no labels!</p>
<p>Two major objective used in BERT pretraining:</p>
<p>â€¢ Masked language modeling (MLM)</p>
<p>â€¢ Next sentence prediction (NSP)</p>
<h2 id="MLM-Masked-language-modeling"><a href="#MLM-Masked-language-modeling" class="headerlink" title="MLM Masked language modeling"></a>MLM Masked language modeling</h2><p>Randomly mask (via a [mask] token)<br>15% of the tokens in each sequence. </p>
<p>Ask the transformer model to<br>predict the masked token on the top<br>layer via standard cross-entropy loss.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">ç‰¹æ€§</th>
<th style="text-align:left">æ©ç è¯­è¨€å»ºæ¨¡ (MLM) - BERT</th>
<th style="text-align:left">è¿ç»­è¯è¢‹æ¨¡å‹ (CBOW) - Word2Vec</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>æ¨¡å‹æ¶æ„</strong></td>
<td style="text-align:left"><strong>æ·±å±‚</strong> Transformer ç¼–ç å™¨ (Deep Transformer Encoder)ã€‚</td>
<td style="text-align:left"><strong>æµ…å±‚</strong> ç¥ç»ç½‘ç»œ (Shallow Neural Network)ã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>è®­ç»ƒç›®æ ‡</strong></td>
<td style="text-align:left"><strong>é¢„æµ‹</strong> è¾“å…¥åºåˆ—ä¸­è¢« <strong><code>[MASK]</code></strong> æ ‡è®°éšæœºæ›¿æ¢çš„è¯å…ƒã€‚</td>
<td style="text-align:left"><strong>é¢„æµ‹</strong> çª—å£å†…ç¼ºå¤±çš„ <strong>ç›®æ ‡è¯è¯­</strong>ï¼Œæ ¹æ®å…¶å‘¨å›´çš„ä¸Šä¸‹æ–‡è¯è¯­ã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>ä¸Šä¸‹æ–‡åˆ©ç”¨</strong></td>
<td style="text-align:left">åˆ©ç”¨ <strong>åŒå‘ä¸Šä¸‹æ–‡</strong> (å·¦ä¾§å’Œå³ä¾§) ä»¥åŠ <strong>æ•´ä¸ªåºåˆ—</strong> çš„ä¿¡æ¯ã€‚ä¾èµ– Transformer çš„ <strong>è‡ªæ³¨æ„åŠ›æœºåˆ¶</strong> æ¥æ•æ‰è¿œè·ç¦»ä¾èµ–ã€‚</td>
<td style="text-align:left">ä»…åˆ©ç”¨<strong>å›ºå®šå¤§å°</strong>çª—å£å†…çš„ä¸Šä¸‹æ–‡è¯è¯­ã€‚å°†ä¸Šä¸‹æ–‡è¯è¯­è§†ä¸ºâ€œè¯è¢‹â€ï¼Œé€šå¸¸é€šè¿‡æ±‚å’Œæˆ–å¹³å‡å®ƒä»¬çš„å‘é‡æ¥è¡¨ç¤ºä¸Šä¸‹æ–‡ã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>è¯åºæ•æ„Ÿæ€§</strong></td>
<td style="text-align:left"><strong>æ•æ„Ÿã€‚</strong> ç”±äºä½¿ç”¨äº† Transformer æ¶æ„å’Œ<strong>ä½ç½®ç¼–ç </strong>ï¼Œæ¨¡å‹çŸ¥é“è¯è¯­çš„å…ˆåé¡ºåºã€‚</td>
<td style="text-align:left"><strong>ä¸æ•æ„Ÿã€‚</strong> å› ä¸ºå®ƒå°†ä¸Šä¸‹æ–‡è¯è¯­è§†ä¸ºä¸€ä¸ªâ€œè¢‹å­â€è¿›è¡Œå¤„ç† (æ±‚å’Œ/å¹³å‡)ï¼Œä¸¢å¤±äº†è¯è¯­çš„å…ˆåé¡ºåºä¿¡æ¯ã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>è¾“å‡ºç»“æœ</strong></td>
<td style="text-align:left"><strong>è¯­å¢ƒåŒ–/åŠ¨æ€åµŒå…¥ (Contextualized/Dynamic Embeddings)ã€‚</strong> åŒä¸€ä¸ªè¯åœ¨ä¸åŒå¥å­ä¸­çš„å«ä¹‰ä¸åŒï¼Œå…¶å‘é‡ä¹Ÿ<strong>ä¸åŒ</strong>ã€‚</td>
<td style="text-align:left"><strong>é™æ€åµŒå…¥ (Static Embeddings)ã€‚</strong> æ— è®ºå‡ºç°åœ¨ä»€ä¹ˆè¯­å¢ƒä¸­ï¼Œä¸€ä¸ªè¯è¯­éƒ½åªæœ‰ä¸€ä¸ª<strong>å›ºå®š</strong>çš„å‘é‡è¡¨ç¤ºã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>å¤„ç†å¤šä¹‰è¯</strong></td>
<td style="text-align:left"><strong>å‡ºè‰²ã€‚</strong> èƒ½åŒºåˆ†å¤šä¹‰è¯çš„ä¸åŒå«ä¹‰ï¼ˆä¾‹å¦‚ï¼Œâ€œé“¶è¡Œâ€ä½œä¸ºé‡‘èæœºæ„å’Œâ€œæ²³å²¸â€çš„å«ä¹‰ï¼‰ã€‚</td>
<td style="text-align:left"><strong>è¾ƒå¼±ã€‚</strong> æ— æ³•åŒºåˆ†å¤šä¹‰è¯çš„ä¸åŒå«ä¹‰ï¼Œä¼šä¸ºæ‰€æœ‰è¯­å¢ƒä¸‹çš„å¤šä¹‰è¯å­¦ä¹ <strong>åŒä¸€ä¸ª</strong>å‘é‡ã€‚</td>
</tr>
</tbody>
</table>
</div>
<p>Problem: If we only add loss for<br>masked tokens, then the<br>transformer would not build<br>good representations for nonmasked tokens.</p>
<p>For 10% of the time, we replace<br>[M] with a random token.</p>
<p>For another 10% of the time, we<br>do not change the original token.</p>
<p>80% remaining time, the mask token is used.</p>
<h2 id="NSP-Next-Sentence-Prediction"><a href="#NSP-Next-Sentence-Prediction" class="headerlink" title="NSP Next Sentence Prediction"></a>NSP Next Sentence Prediction</h2><p> In addition to MLM, we also add a [CLS]<br>token and ask BERT to predict whether<br>sentence2 is the next sentence of<br>sentence1. </p>
<p>[CLS] æ¥åˆ†ç±»ï¼Œ[SEP] æ¥åˆ†å‰²å¥å­</p>
<h2 id="finetune"><a href="#finetune" class="headerlink" title="finetune"></a>finetune</h2><p>BERT finetuning cont.</p>
<p>â€¢ After pretraining, we slightly modify the top layers of BERT and<br>tune it on downstream tasks</p>
<p>top layer: è¾“å‡º $T$ çš„å±‚</p>
<h2 id="efficient-approach"><a href="#efficient-approach" class="headerlink" title="efficient approach"></a>efficient approach</h2><h3 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h3><p> ç”Ÿæˆå™¨ (Generator)</p>
<ul>
<li><strong>å·¥ä½œåŸç†ï¼š</strong> å®ƒé€šå¸¸æ˜¯ä¸€ä¸ª<strong>å°å‹</strong>çš„<strong>æ©ç è¯­è¨€æ¨¡å‹ (MLM)</strong>ã€‚</li>
<li><strong>è¾“å…¥å¤„ç†ï¼š</strong> å®ƒé¦–å…ˆåƒ BERT é‚£æ ·ï¼Œå¯¹è¾“å…¥å¥å­è¿›è¡Œ<strong>æ©ç </strong>ï¼ˆå¦‚å°† â€œthe chef cooked the mealâ€ å˜ä¸º â€œthe chef [MASK] the mealâ€ï¼‰ã€‚</li>
<li><strong>è¾“å‡ºï¼š</strong> å®ƒé¢„æµ‹è¢«æ©ç çš„è¯å…ƒï¼Œå¹¶ç”¨è¿™äº›<strong>è²Œä¼¼åˆç† (plausible)</strong> çš„é¢„æµ‹æ›¿ä»£åŸå¥å­ä¸­çš„ä¸€äº›è¯å…ƒã€‚<ul>
<li><em>ç¤ºä¾‹ï¼š</em> åŸå§‹å¥å­ä¸­çš„ <code>cooked</code> è¢« <code>[MASK]</code>ï¼Œç”Ÿæˆå™¨å¯èƒ½ä¼šé¢„æµ‹å‡º <code>ate</code>ï¼Œç„¶åç”¨ <code>ate</code> æ›¿æ¢åŸè¯ã€‚</li>
</ul>
</li>
</ul>
<p>åˆ¤åˆ«å™¨ (Discriminator) - ELECTRA æ¨¡å‹æœ¬èº«</p>
<ul>
<li><strong>è¾“å…¥ï¼š</strong> æ¥æ”¶è¢«ç”Ÿæˆå™¨<strong>æ›¿æ¢ï¼ˆæŸåï¼‰</strong> è¿‡çš„è¾“å…¥åºåˆ—ã€‚</li>
<li>è¿™æ˜¯ä¸€ä¸ªäºŒå…ƒåˆ†ç±»ä»»åŠ¡ï¼šå¯¹æ¯ä¸ªè¯å…ƒ $x_i$ï¼Œé¢„æµ‹ $P(\text{IsReplaced}|x_i)$ã€‚</li>
</ul>
<p>This new pre-training<br>task is more efficient<br>than MLM because the<br>task is defined over all<br>input tokens rather than<br>just the small subset<br>that was masked out.</p>
<h3 id="Longformer-Sliding-Window-Attention"><a href="#Longformer-Sliding-Window-Attention" class="headerlink" title="Longformer Sliding Window Attention"></a>Longformer Sliding Window Attention</h3><p><strong>æ¯ä¸ª Token çš„è®¡ç®—é‡ï¼š</strong> å¯¹äºåºåˆ—ä¸­çš„<strong>ä»»æ„ä¸€ä¸ª token</strong>ï¼ˆæŸ¥è¯¢ Qï¼‰ï¼Œå®ƒåªéœ€è¦è®¡ç®—ä¸å®ƒå±€éƒ¨çª—å£ $w$ å†…çš„<strong>å…¶ä»– $w$ ä¸ª token</strong>ï¼ˆé”® Kï¼‰çš„æ³¨æ„åŠ›å¾—åˆ†, $O(w)$ã€‚  æ€»è®¡ç®—é‡ $\approx n \times O(w) = O(n \cdot w)$ã€‚</p>
<p> For an embedding<br>on layer L, whatâ€™s its receptive field?<br>(how many input tokens does it<br>cover?)</p>
<p>Lxw</p>
<p><img src="nlp=win.png" alt=""></p>
<p>Combined with dilated sliding window</p>
<p>We can add gaps in the window to make it<br>even wider with the same amount of compute(æ•æ‰é•¿è·ç¦»ä¾èµ–)</p>
<p>We can use a combination of 2 heads of<br>dilated and other heads with local sliding<br>window.ï¼ˆç”¨multiheadå®ç°çŸ­è·ç¦»é•¿è·ç¦»ç»“åˆï¼‰</p>
<h1 id="VAE-LMï¼ˆä¸è€ƒï¼‰"><a href="#VAE-LMï¼ˆä¸è€ƒï¼‰" class="headerlink" title="VAE-LMï¼ˆä¸è€ƒï¼‰"></a>VAE-LMï¼ˆä¸è€ƒï¼‰</h1><p><img src="nlp=vae.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">ç»„ä»¶</th>
<th style="text-align:left">åç§°</th>
<th style="text-align:left">åŠŸèƒ½</th>
<th style="text-align:left">ç»“æ„</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>$q_{\phi}$ (Encoder)</strong></td>
<td style="text-align:left"><strong>ç¼–ç å™¨/åéªŒæ¨¡å‹</strong></td>
<td style="text-align:left">æ¥æ”¶è¾“å…¥å¥å­ $\mathbf{x}$ï¼Œå°†å®ƒå‹ç¼©æˆä¸€ä¸ªæ½œåœ¨è¯­ä¹‰å‘é‡ $\mathbf{z}$ çš„åˆ†å¸ƒã€‚</td>
<td style="text-align:left">1. <strong>$\text{RNNs}$ (LSTM Cell):</strong> ç¼–ç æ•´ä¸ªå¥å­ $\mathbf{x}$ã€‚ 2. <strong>Linear Layers:</strong> æ ¹æ® $\text{RNN}$ çš„æœ€ç»ˆéšè—çŠ¶æ€ï¼Œè¾“å‡ºæ½œå˜é‡ $\mathbf{z}$ åˆ†å¸ƒçš„å‚æ•° $\mu$ (å‡å€¼) å’Œ $\sigma$ (æ–¹å·®)ã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>$\mathbf{z}$</strong></td>
<td style="text-align:left"><strong>æ½œåœ¨å˜é‡ (Latent Variable)</strong></td>
<td style="text-align:left">ä¸€ä¸ªè¿ç»­çš„ä½ç»´å‘é‡ï¼Œä»£è¡¨æ•´ä¸ªå¥å­çš„<strong>å…¨å±€è¯­ä¹‰</strong>ã€‚</td>
<td style="text-align:left">ä» $\mathbf{z}$ çš„åˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°ã€‚</td>
</tr>
<tr>
<td style="text-align:left"><strong>$p_{\theta}$ (Decoder)</strong></td>
<td style="text-align:left"><strong>è§£ç å™¨/ç”Ÿæˆæ¨¡å‹</strong></td>
<td style="text-align:left">æ¥æ”¶ $\mathbf{z}$ ä½œä¸ºè¾“å…¥ï¼Œå¹¶é€å­—ç”Ÿæˆï¼ˆé‡æ„ï¼‰å¥å­ $\mathbf{x}$ã€‚</td>
<td style="text-align:left">1. <strong>$\text{RNNs}$ (LSTM Cell):</strong> æ¥æ”¶ $\mathbf{z}$ ä½œä¸ºåˆå§‹çŠ¶æ€æˆ–è¾“å…¥ã€‚ 2. <strong>ç”Ÿæˆè¿‡ç¨‹:</strong> ä»èµ·å§‹ç¬¦å·ï¼ˆå¦‚ <code>&lt;EOS&gt;</code> åœ¨å›¾ä¸­ä¼¼ä¹è¢«è¯¯ç½®ï¼Œé€šå¸¸æ˜¯ <code>&lt;BOS&gt;</code> æˆ– $\mathbf{z}$ï¼‰å¼€å§‹ï¼Œé€æ­¥ç”Ÿæˆ $\text{work} \rightarrow \text{work} \rightarrow \text{<EOS>}$ã€‚</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>åéªŒ ($q_{\phi}$) å’Œå…ˆéªŒ ($p(\mathbf{z})$) åˆ†å¸ƒï¼š</strong> å›¾ä¸­æ˜ç¡®æŒ‡å‡ºï¼Œå…ˆéªŒåˆ†å¸ƒ $p(\mathbf{z})$ å’ŒåéªŒåˆ†å¸ƒ $q_{\phi}(\mathbf{z} | \mathbf{x})$ éƒ½æ˜¯<strong>é«˜æ–¯åˆ†å¸ƒ (Gaussian)</strong>ï¼Œä¸”é€šå¸¸æ˜¯å¯¹è§’åæ–¹å·®çŸ©é˜µï¼ˆdiagonalï¼‰ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åœ¨æ½œç©ºé—´ä¸­æ˜¯å¯å‚æ•°åŒ–çš„ã€‚</li>
</ul>
<hr>
<h2 id="ğŸš€-ç”Ÿæˆ-Generation-è¿‡ç¨‹"><a href="#ğŸš€-ç”Ÿæˆ-Generation-è¿‡ç¨‹" class="headerlink" title="ğŸš€ ç”Ÿæˆ (Generation) è¿‡ç¨‹"></a>ğŸš€ ç”Ÿæˆ (Generation) è¿‡ç¨‹</h2><p>ç”Ÿæˆæ–°å¥å­æ—¶ï¼Œæˆ‘ä»¬åªä½¿ç”¨<strong>è§£ç å™¨ $p_{\theta}$</strong>ï¼š</p>
<ol>
<li><strong>Sample $\mathbf{z}$ from prior $p(\mathbf{z})$:</strong> ä»ä¸€ä¸ªç®€å•çš„<strong>å…ˆéªŒåˆ†å¸ƒ</strong>ï¼ˆå¦‚æ ‡å‡†æ­£æ€åˆ†å¸ƒ $\mathcal{N}(\mathbf{0}, \mathbf{I})$ï¼‰ä¸­éšæœºé‡‡æ ·ä¸€ä¸ªæ½œåœ¨è¯­ä¹‰å‘é‡ $\mathbf{z}$ã€‚è¿™ä¸ª $\mathbf{z}$ å°±æ˜¯æˆ‘ä»¬å¸Œæœ›ç”Ÿæˆå¥å­æ‰€æ‹¥æœ‰çš„<strong>æ„å›¾æˆ–ä¸»é¢˜</strong>ã€‚</li>
<li><strong>Sample $\mathbf{x}$ from our generative model $p_{\theta}(\mathbf{x} | \mathbf{z})$:</strong> å°†è¿™ä¸ªé‡‡æ ·çš„ $\mathbf{z}$ å–‚ç»™<strong>è§£ç å™¨</strong>ï¼Œè§£ç å™¨ $\text{RNN}$ å°±ä¼šé€è¯ç”Ÿæˆå¥å­ $\mathbf{x}$ï¼Œç›´åˆ°ç”Ÿæˆç»“æŸç¬¦å· ($\text{<EOS>}$)ã€‚</li>
</ol>
<hr>
<h2 id="ğŸ“ˆ-è®­ç»ƒç›®æ ‡ï¼š-text-VAE-â€œ-text-ELBO-â€-æœ€å¤§åŒ–"><a href="#ğŸ“ˆ-è®­ç»ƒç›®æ ‡ï¼š-text-VAE-â€œ-text-ELBO-â€-æœ€å¤§åŒ–" class="headerlink" title="ğŸ“ˆ è®­ç»ƒç›®æ ‡ï¼š$\text{VAE}$ â€œ$\text{ELBO}$â€ (æœ€å¤§åŒ–)"></a>ğŸ“ˆ è®­ç»ƒç›®æ ‡ï¼š$\text{VAE}$ â€œ$\text{ELBO}$â€ (æœ€å¤§åŒ–)</h2><script type="math/tex; mode=display">
\mathcal{L}(\theta; \mathbf{x}) = \underbrace{-\text{KL}(q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}))}_{\text{I. KL Divergence (Regularization Term)}} + \underbrace{\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}[\log p_{\theta}(\mathbf{x} | \mathbf{z})]}_{\text{II. Expectation of Log-Likelihood (Reconstruction Term)}}</script><ul>
<li><h3 id="I-text-KL-æ•£åº¦é¡¹ï¼ˆ-text-KL-dots-ï¼‰"><a href="#I-text-KL-æ•£åº¦é¡¹ï¼ˆ-text-KL-dots-ï¼‰" class="headerlink" title="I. $\text{KL}$ æ•£åº¦é¡¹ï¼ˆ$-\text{KL}(\dots)$ï¼‰"></a>I. $\text{KL}$ æ•£åº¦é¡¹ï¼ˆ$-\text{KL}(\dots)$ï¼‰</h3><script type="math/tex; mode=display">-\text{KL}(q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}))</script><ul>
<li><strong>ä½œç”¨ï¼š</strong> è¿™æ˜¯ä¸€ä¸ª<strong>æ­£åˆ™åŒ–é¡¹</strong>ï¼Œå®ƒè¡¡é‡äº†<strong>ç¼–ç å™¨</strong>è¾“å‡ºçš„åéªŒåˆ†å¸ƒ $q_{\phi}(\mathbf{z} | \mathbf{x})$ ä¸ç®€å•çš„<strong>å…ˆéªŒåˆ†å¸ƒ</strong> $p(\mathbf{z})$ ä¹‹é—´çš„å·®å¼‚ã€‚</li>
</ul>
</li>
</ul>
<ul>
<li><h3 id="II-é‡æ„é¡¹-mathbb-E-log-p-theta-mathbf-x-mathbf-z"><a href="#II-é‡æ„é¡¹-mathbb-E-log-p-theta-mathbf-x-mathbf-z" class="headerlink" title="II. é‡æ„é¡¹ ($\mathbb{E}[\log p_{\theta}(\mathbf{x} | \mathbf{z})]$)"></a>II. é‡æ„é¡¹ ($\mathbb{E}[\log p_{\theta}(\mathbf{x} | \mathbf{z})]$)</h3><script type="math/tex; mode=display">\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}[\log p_{\theta}(\mathbf{x} | \mathbf{z})]</script><ul>
<li><strong>ä½œç”¨ï¼š</strong> è¿™æ˜¯ä¸€ä¸ª<strong>é‡æ„é¡¹</strong>ï¼Œå®ƒè¡¡é‡äº†<strong>è§£ç å™¨</strong>ä»æ½œåœ¨å‘é‡ $\mathbf{z}$ é‡æ„å‡ºåŸå§‹å¥å­ $\mathbf{x}$ çš„<strong>å¯¹æ•°æ¦‚ç‡</strong>ã€‚</li>
</ul>
</li>
<li><h3 id="è¯æ®ä¸‹ç•Œ"><a href="#è¯æ®ä¸‹ç•Œ" class="headerlink" title="è¯æ®ä¸‹ç•Œ"></a>è¯æ®ä¸‹ç•Œ</h3><p>  $\text{ELBO}$ çš„å€¼<strong>å°äºæˆ–ç­‰äº</strong>æ•°æ®çš„å¯¹æ•°è¾¹ç¼˜ä¼¼ç„¶ $\log p(\mathbf{x})$ï¼š</p>
<script type="math/tex; mode=display">\mathcal{L}(\theta; \mathbf{x}) \leq \log p(\mathbf{x})</script><p>  å› æ­¤ï¼Œæœ€å¤§åŒ– $\mathcal{L}(\theta; \mathbf{x})$ å°±æ˜¯åœ¨æœ€å¤§åŒ– $\log p(\mathbf{x})$ çš„ä¸€ä¸ª<strong>ä¸‹ç•Œ</strong>ï¼Œä»è€Œé—´æ¥ä¼˜åŒ–äº†æ•´ä¸ªæ¨¡å‹ã€‚</p>
</li>
</ul>
<h3 id="ğŸ“ˆ-text-ELBO-æ¨å¯¼è¿‡ç¨‹è§£é‡Š"><a href="#ğŸ“ˆ-text-ELBO-æ¨å¯¼è¿‡ç¨‹è§£é‡Š" class="headerlink" title="ğŸ“ˆ $\text{ELBO}$ æ¨å¯¼è¿‡ç¨‹è§£é‡Š"></a>ğŸ“ˆ $\text{ELBO}$ æ¨å¯¼è¿‡ç¨‹è§£é‡Š</h3><p>æ¨å¯¼çš„ç›®æ ‡æ˜¯æ‰¾åˆ° $\log p(\mathbf{x})$ çš„ä¸€ä¸ª<strong>ä¸‹ç•Œ</strong>ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º $\mathcal{L}(\theta; \mathbf{x})$ æˆ– $\text{ELBO}$ã€‚</p>
<script type="math/tex; mode=display">
\ln p(\mathbf{x}) = \ln \int_{\mathbf{z}} p(\mathbf{x}, \mathbf{z}) d\mathbf{z}</script><script type="math/tex; mode=display">
= \ln \int_{\mathbf{z}} p(\mathbf{x}, \mathbf{z}) \frac{q(\mathbf{z} | \mathbf{x})}{q(\mathbf{z} | \mathbf{x})} d\mathbf{z}</script><script type="math/tex; mode=display">
\ge \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln \frac{p(\mathbf{x}, \mathbf{z})}{q(\mathbf{z} | \mathbf{x})}\right]</script><ul>
<li><strong>è¯´æ˜ï¼š</strong> è¿™æ˜¯æ¨å¯¼çš„å…³é”®ä¸€æ­¥ã€‚ç”±äº $\ln(\cdot)$ æ˜¯ä¸€ä¸ª<strong>å‡¹å‡½æ•° (concave function)</strong>ï¼Œæ ¹æ® <strong>è©¹æ£®ä¸ç­‰å¼ (Jensenâ€™s Inequality)</strong>ï¼Œå¯¹äºä»»ä½•éšæœºå˜é‡ $Y$ï¼š<script type="math/tex; mode=display">\ln(\mathbb{E}[Y]) \ge \mathbb{E}[\ln(Y)]</script></li>
</ul>
<script type="math/tex; mode=display">
= \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln \frac{p(\mathbf{x}, \mathbf{z})}{q(\mathbf{z} | \mathbf{x})}\right]</script><script type="math/tex; mode=display">
= \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln \frac{p(\mathbf{x} | \mathbf{z}) p(\mathbf{z})}{q(\mathbf{z} | \mathbf{x})}\right]</script><script type="math/tex; mode=display">
= \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln p(\mathbf{x} | \mathbf{z})\right] + \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln \frac{p(\mathbf{z})}{q(\mathbf{z} | \mathbf{x})}\right]</script><script type="math/tex; mode=display">
= \mathbb{E}_{q(\mathbf{z} | \mathbf{x})}\left[\ln p(\mathbf{x} | \mathbf{z})\right] - \int_{\mathbf{z}} q(\mathbf{z} | \mathbf{x}) \ln \frac{q(\mathbf{z} | \mathbf{x})}{p(\mathbf{z})} d\mathbf{z}</script><script type="math/tex; mode=display">
= \text{likelihood} - \mathbb{D}_{\text{KL}}[q(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z})]</script><p>åœ¨ $\text{VAE}$ è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬é€šè¿‡<strong>æœ€å¤§åŒ–</strong>è¿™ä¸ª $\text{ELBO}$ ç›®æ ‡å‡½æ•°ï¼Œé—´æ¥å®ç°äº†å¯¹çœŸå®æ•°æ®åˆ†å¸ƒ $p(\mathbf{x})$ çš„å»ºæ¨¡ã€‚</p>
<p>ä» <strong>KL æ•£åº¦çš„å®šä¹‰</strong>å¼€å§‹ï¼š</p>
<script type="math/tex; mode=display">D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) || p_{\theta}(\mathbf{z}|\mathbf{x})) = E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log \frac{q_{\phi}(\mathbf{z}|\mathbf{x})}{p_{\theta}(\mathbf{z}|\mathbf{x})} \right]</script><p>å°†å¯¹æ•°ä¸­çš„é™¤æ³•åˆ†è§£ä¸ºå‡æ³•ï¼š</p>
<script type="math/tex; mode=display">D_{KL} = E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log q_{\phi}(\mathbf{z}|\mathbf{x}) \right] - E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log p_{\theta}(\mathbf{z}|\mathbf{x}) \right]</script><p>æ ¹æ®è´å¶æ–¯å®šç† $p(\mathbf{z}|\mathbf{x}) = \frac{p(\mathbf{x}, \mathbf{z})}{p(\mathbf{x})}$ï¼Œæˆ‘ä»¬æœ‰ $\log p(\mathbf{z}|\mathbf{x}) = \log p(\mathbf{x}, \mathbf{z}) - \log p(\mathbf{x})$ã€‚ä»£å…¥ä¸Šå¼ï¼š</p>
<script type="math/tex; mode=display">D_{KL} = E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log q_{\phi}(\mathbf{z}|\mathbf{x}) \right] - E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log p_{\theta}(\mathbf{x}, \mathbf{z}) - \log p_{\theta}(\mathbf{x}) \right]</script><p>å°†æœŸæœ›ä¸­çš„ $\log p_{\theta}(\mathbf{x})$ ç§»å‡ºï¼ˆå› ä¸ºå®ƒä¸ $\mathbf{z}$ æ— å…³ï¼ŒæœŸæœ›å€¼å°±æ˜¯å®ƒæœ¬èº«ï¼‰ï¼š</p>
<script type="math/tex; mode=display">D_{KL} = E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log q_{\phi}(\mathbf{z}|\mathbf{x}) \right] - E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log p_{\theta}(\mathbf{x}, \mathbf{z}) \right] + \log p_{\theta}(\mathbf{x})</script><p>é‡æ–°æ’åˆ—å„é¡¹ï¼š</p>
<script type="math/tex; mode=display">\log p_{\theta}(\mathbf{x}) = E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log p_{\theta}(\mathbf{x}, \mathbf{z}) \right] - E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log q_{\phi}(\mathbf{z}|\mathbf{x}) \right] + D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) || p_{\theta}(\mathbf{z}|\mathbf{x}))</script><p>æœ€åï¼Œæˆ‘ä»¬å°†å‰ä¸¤é¡¹åˆå¹¶ï¼Œå¾—åˆ° ELBO çš„å®šä¹‰ï¼š</p>
<script type="math/tex; mode=display">\mathcal{L}(\theta, \phi) = E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_{\theta}(\mathbf{x}, \mathbf{z})}{q_{\phi}(\mathbf{z}|\mathbf{x})} \right]</script><p>æ‰€ä»¥ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°è¿™ä¸ªå…³é”®ç­‰å¼ï¼š</p>
<script type="math/tex; mode=display">\log p(\mathbf{x}) = \mathcal{L}(\theta, \phi) + D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) || p_{\theta}(\mathbf{z}|\mathbf{x}))</script><h2 id="é‡å‚æ•°"><a href="#é‡å‚æ•°" class="headerlink" title="é‡å‚æ•°"></a>é‡å‚æ•°</h2><script type="math/tex; mode=display">
\mathcal{L}(\theta; \mathbf{x}) = \underbrace{-\text{KL}(q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}))}_{\text{I. KL Divergence (Regularization Term)}} + \underbrace{\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}[\log p_{\theta}(\mathbf{x} | \mathbf{z})]}_{\text{II. Expectation of Log-Likelihood (Reconstruction Term)}}</script><p>the second term involves a sampling operation from the<br>parameterized q, which we can not directly back-prop</p>
<p>åœ¨å¼•å…¥é‡æ–°å‚æ•°åŒ–æŠ€å·§ä¹‹å‰ï¼Œæˆ‘ä»¬è®¡ç®—é‡æ„é¡¹çš„æ¢¯åº¦æ˜¯è¿™æ ·çš„ï¼š</p>
<script type="math/tex; mode=display">\nabla_{\phi} E_{q_{\phi}(\mathbf{z}|\mathbf{x})} [\log p_{\theta}(\mathbf{x}|\mathbf{z})]</script><p>å½“æˆ‘ä»¬ä½¿ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·æ¥è¿‘ä¼¼è¿™ä¸ªæœŸæœ›æ—¶ï¼š</p>
<script type="math/tex; mode=display">\nabla_{\phi} \left( \frac{1}{M} \sum_{m=1}^{M} \log p_{\theta}(\mathbf{x}|\mathbf{z}^{(m)}) \right) \quad \text{å…¶ä¸­ } \mathbf{z}^{(m)} \sim q_{\phi}(\mathbf{z}|\mathbf{x})</script><p><strong>é—®é¢˜æ ¸å¿ƒåœ¨äºï¼š</strong></p>
<ul>
<li><strong>é‡‡æ ·è¿‡ç¨‹ ($\mathbf{z} \sim q_{\phi}(\mathbf{z}|\mathbf{x})$) æ˜¯ä¸€ä¸ªç¦»æ•£çš„ã€ä¸å¯é€†çš„éšæœºæ“ä½œã€‚</strong></li>
<li>å®ƒå°±åƒä¸€ä¸ª<strong>é»‘ç®±</strong>ï¼Œè¾“å…¥æ˜¯å‚æ•° $\phi$ï¼ˆå†³å®šäº† $q$ çš„ $\mu$ å’Œ $\sigma$ï¼‰ï¼Œè¾“å‡ºæ˜¯æ ·æœ¬ $\mathbf{z}$ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œ<strong>è¿™ä¸ªä» $\phi$ åˆ° $\mathbf{z}$ çš„æ˜ å°„æ˜¯ä¸å¯å¯¼çš„</strong>ã€‚</li>
<li>å› æ­¤ï¼Œæˆ‘ä»¬æ— æ³•åˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®— $\mathbf{z}$ å¯¹ $\phi$ çš„æ¢¯åº¦ $\frac{\partial \mathbf{z}}{\partial \phi}$ï¼Œä¹Ÿå°±æ— æ³•å°†æŸå¤±å‡½æ•°çš„æ¢¯åº¦ $\frac{\partial \mathcal{L}}{\partial \mathbf{z}}$ ä¼ é€’å› $\phi$ã€‚</li>
</ul>
<p>é‡æ–°å‚æ•°åŒ–æŠ€å·§å°±æ˜¯é€šè¿‡<strong>é‡å†™éšæœºå˜é‡çš„ç”Ÿæˆè¿‡ç¨‹</strong>ï¼Œå°†éšæœºæ€§ï¼ˆ$\epsilon$ï¼‰å’Œå‚æ•°ä¾èµ–æ€§ ($\phi$ é€šè¿‡ $\mu, \sigma$) <strong>åˆ†ç¦»</strong>ï¼Œä»è€Œåˆ›å»ºä¸€æ¡<strong>å¯å¯¼çš„è·¯å¾„ (pathwise gradient)</strong>ï¼š</p>
<script type="math/tex; mode=display">\mathbf{z} = \mu(\mathbf{x}, \phi) + \sigma(\mathbf{x}, \phi) \odot \epsilon</script><p><strong>ç°åœ¨çš„è®¡ç®—å›¾æ˜¯ï¼š</strong></p>
<ol>
<li>$\phi \to (\mu, \sigma)$ (ç¡®å®šæ€§ï¼Œå¯å¯¼)</li>
<li>$(\mu, \sigma)$ å’Œ $\epsilon$ (éšæœºä½†<strong>ç‹¬ç«‹äº</strong> $\phi$) $\to \mathbf{z}$ (ç¡®å®šæ€§å‡½æ•° $g$ï¼Œå¯å¯¼)</li>
<li>$\mathbf{z} \to \log p(\mathbf{x}|\mathbf{z})$ (ç¡®å®šæ€§ï¼Œå¯å¯¼)</li>
<li>$\log p(\mathbf{x}|\mathbf{z}) \to \mathcal{L}$</li>
</ol>
<p>ç”±äº $\mathbf{z}$ ç°åœ¨æ˜¯ $\phi$ çš„ä¸€ä¸ª<strong>ç¡®å®šæ€§å‡½æ•°</strong>ï¼ˆé€šè¿‡ $\mu$ å’Œ $\sigma$ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨é“¾å¼æ³•åˆ™æ±‚å¯¼ï¼š</p>
<script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial \phi} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}} \frac{\partial \mathbf{z}}{\partial \mu} \frac{\partial \mu}{\partial \phi} + \frac{\partial \mathcal{L}}{\partial \mathbf{z}} \frac{\partial \mathbf{z}}{\partial \sigma} \frac{\partial \sigma}{\partial \phi}</script><p>æ³¨ï¼š$\frac{\partial \mathbf{z}}{\partial \sigma} =\epsilon$</p>
<p><img src="nlp=re.png" alt=""></p>
<h2 id="optimization-challenge"><a href="#optimization-challenge" class="headerlink" title="optimization challenge"></a>optimization challenge</h2><p>KL term quickly decrease to zero<br>(throwing away latent information)</p>
<p>Comparing to the second term, the KL-prior term is easier to optimize<br>(why?).</p>
<p>â€¢ The RNN decoder is strong and has ground-truth history in its input.</p>
<h3 id="1-KL-ä»£ä»·é€€ç«-KL-Cost-Annealing-ğŸŒ¡ï¸"><a href="#1-KL-ä»£ä»·é€€ç«-KL-Cost-Annealing-ğŸŒ¡ï¸" class="headerlink" title="1. KL ä»£ä»·é€€ç« (KL Cost Annealing) ğŸŒ¡ï¸"></a>1. KL ä»£ä»·é€€ç« (KL Cost Annealing) ğŸŒ¡ï¸</h3><ul>
<li><strong>æ–¹æ³•ï¼š</strong> åœ¨ç›®æ ‡å‡½æ•°ä¸­ï¼Œä¸º KL æ•£åº¦é¡¹æ·»åŠ ä¸€ä¸ª<strong>å¯å˜æƒé‡ $\beta$</strong>ã€‚<ul>
<li>åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œè®¾ç½® $\beta$ <strong>æ¥è¿‘æˆ–ç­‰äºé›¶</strong>ã€‚</li>
<li>éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œ$\beta$ <strong>é€æ¸å¢åŠ </strong>ï¼Œç›´åˆ°è¾¾åˆ° 1ã€‚</li>
</ul>
</li>
<li><strong>æ•°å­¦å½¢å¼ï¼š</strong><script type="math/tex; mode=display">\mathcal{L}(\theta; \vec{x}) = \mathbb{E}[\log p_\theta(\vec{x}|\vec{z})] - \beta \cdot \text{KL}(q_\phi(\vec{z}|\vec{x})||p(\vec{z}))</script></li>
<li><strong>ç›´è§‰è§£é‡Šï¼š</strong><ol>
<li><strong>æ—©æœŸ ( $\beta \approx 0$ ):</strong> ç›®æ ‡å‡½æ•°å‡ ä¹åªå‰©ä¸‹<strong>é‡æ„é¡¹</strong>ã€‚è¿™ç»™äº†ç¼–ç å™¨å’Œè§£ç å™¨å……è¶³çš„æœºä¼šï¼Œå»å­¦ä¹ ä¸€ä¸ª<strong>æœ€å¤§åŒ–ä¿¡æ¯é‡</strong>çš„æ½œåœ¨è¡¨ç¤º $\vec{z}$ï¼Œè€Œä¸å¿…æ‹…å¿ƒ $\vec{z}$ æ˜¯å¦æ¥è¿‘å…ˆéªŒåˆ†å¸ƒ $p(\vec{z})$ã€‚</li>
<li><strong>åæœŸ ( $\beta \to 1$ ):</strong> KL é¡¹çš„æƒé‡é€æ¸å¢å¤§ï¼Œå¼€å§‹å‘æŒ¥å…¶<strong>æ­£åˆ™åŒ–</strong>ä½œç”¨ï¼Œè¿«ä½¿å­¦ä¹ åˆ°çš„æ½œåœ¨åˆ†å¸ƒ $q_\phi(\vec{z}|\vec{x})$ æ¥è¿‘å…ˆéªŒ $p(\vec{z})$ã€‚</li>
</ol>
</li>
<li><strong>æ•ˆæœï¼š</strong> è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°è®©æ¨¡å‹å…ˆâ€œå­¦ä¼šè¯´è¯â€ï¼ˆå­¦ä¹ æœ‰ç”¨çš„ $\vec{z}$ï¼‰ï¼Œç„¶åå†â€œè§„èŒƒè¯­æ³•â€ï¼ˆæ»¡è¶³ KL çº¦æŸï¼‰ï¼Œä»è€Œé¿å…äº† $\vec{z}$ åœ¨è®­ç»ƒåˆæœŸå°±è¢«ä¼˜åŒ–å™¨è½»æ˜“æŠ›å¼ƒã€‚</li>
</ul>
<hr>
<h3 id="2-è¾“å…¥è¯ä¸¢å¼ƒ-Input-Word-Dropping-âœ‚ï¸"><a href="#2-è¾“å…¥è¯ä¸¢å¼ƒ-Input-Word-Dropping-âœ‚ï¸" class="headerlink" title="2. è¾“å…¥è¯ä¸¢å¼ƒ (Input Word Dropping) âœ‚ï¸"></a>2. è¾“å…¥è¯ä¸¢å¼ƒ (Input Word Dropping) âœ‚ï¸</h3><ul>
<li><strong>æ–¹æ³•ï¼š</strong> è¿™æ˜¯ä¸€ç§ç›´æ¥<strong>å‰Šå¼±è§£ç å™¨</strong>çš„æ–¹æ³•ã€‚<ul>
<li>åœ¨è®­ç»ƒæ—¶ï¼Œéšæœºåœ°<strong>ç§»é™¤éƒ¨åˆ†æˆ–å…¨éƒ¨</strong>ç”¨äº<strong>æ•™å¸ˆå¼ºåˆ¶</strong>çš„ <strong>Ground-Truth å†å²è¯</strong>ï¼ˆå³ $x_{t-1}$ï¼‰ã€‚forcing the model to<br>rely on the latent vectorã€‚</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-è¯è¢‹æŸå¤±-Bag-of-Words-Loss-BoW-Loss-ğŸ›ï¸"><a href="#3-è¯è¢‹æŸå¤±-Bag-of-Words-Loss-BoW-Loss-ğŸ›ï¸" class="headerlink" title="3. è¯è¢‹æŸå¤± (Bag-of-Words Loss, BoW Loss) ğŸ›ï¸"></a>3. è¯è¢‹æŸå¤± (Bag-of-Words Loss, BoW Loss) ğŸ›ï¸</h3><ul>
<li><strong>æ–¹æ³•ï¼š</strong> é™¤äº†æ ‡å‡†çš„åºåˆ—é‡æ„æŸå¤±å¤–ï¼Œ<strong>å¹¶è¡Œåœ°</strong>è®­ç»ƒä¸€ä¸ªè¾…åŠ©è§£ç å™¨ï¼ˆæˆ–è€…åœ¨ä¸»è§£ç å™¨ä¸­å¢åŠ ä¸€ä¸ªåˆ†æ”¯ï¼‰æ¥é¢„æµ‹è¾“å…¥ $\vec{x}$ çš„<strong>è¯è¢‹è¡¨ç¤º (Bag-of-Words, BoW)</strong>ã€‚<ul>
<li><strong>è¯è¢‹è¡¨ç¤ºï¼š</strong> æ˜¯ä¸€ä¸ªå‘é‡ï¼ŒåªåŒ…å« $\vec{x}$ ä¸­<strong>æ¯ä¸ªè¯å‡ºç°çš„æ¬¡æ•°</strong>ï¼Œè€Œå¿½ç•¥è¯çš„é¡ºåºï¼ˆå³<strong>éåºåˆ—ä¿¡æ¯</strong>ï¼‰ã€‚</li>
<li>è¿™ä¸ª BoW æŸå¤±æ—¨åœ¨ç¡®ä¿ $\vec{z}$ è‡³å°‘ç¼–ç äº† $\vec{x}$ çš„<strong>å†…å®¹ä¿¡æ¯</strong>ï¼Œå³ä½¿æ²¡æœ‰ç¼–ç é¡ºåºä¿¡æ¯ã€‚</li>
</ul>
</li>
</ul>
